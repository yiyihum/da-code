{"id": "plot-bar-004", "source": "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "instruction": "This is the Titanic dataset, with relevant descriptions in the README.md file. Your task is to plot a bar chart showing the number of male and female survivors in each passenger class. The partially completed code is provided in analysis.py. Save the plotted image as 'result.jpg'. The image should have dimensions of 14 by 12, with the title 'Sex vs Pclass Survival Comparison'. The x-axis should be labeled 'Sex' with categories 'Male' and 'Female', and the y-axis should be labeled 'Survived'. Additionally, the bars representing each class (1st, 2nd, and 3rd) should be colored as follows: \"#e9d4d0\" for 1st class, \"a16f8d\" for 2nd class, and \"#2d223a\" for 3rd class.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-005", "source": "https://www.kaggle.com/code/aakashnain/is-it-better-than-2017", "instruction": "This is the 2018 Kaggle Machine Learning & Data Science Survey dataset, with relevant descriptions in the README.md file. Your task is to draw a bar chart showing the number of people in each age group who filled out the survey. The method for dividing age groups is outlined in AgeGroup.md. Save the plotted bar chart as 'result.png'. The title of the chart should be 'Age Group Distribution', with the x-axis labeled as 'Age Group' and the y-axis labeled as 'Count'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-006", "source": "https://www.kaggle.com/code/youssefismail20/international-football-matches-1872-to-2023#Host-Countries-and-Tournaments-\ud83c\udfdf%EF%B8%8F", "instruction": "This dataset contains international football results from 1872 to 2024, with relevant descriptions in the README.md file. Based on this dataset, you need to compile the top 10 teams by overall scores from the years 2000 to 2023 and their scores. Then, create a bar chart sorted by total score in descending order. The method to calculate the total score can be found in the 'overall_score.txt' file. The generated image should be saved as 'team.png' with a height and width of 12 and 8, respectively. The bar chart should have the title 'Best Teams from 2000 to 2023', the x-axis title 'Football Team', the x-axis labels should be the names of the teams you filtered, and the y-axis title 'Overall Score'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-007", "source": "https://www.kaggle.com/code/tahaahmedt/seattle-airbnb-listings-eda", "instruction": "This is a Seattle Airbnb Open Data dataset, with relevant descriptions in the README.md file. Based on this dataset and the calculation method recorded in 'NeighborhoodCalculate.txt', create a bar chart showing the distribution of the top 10 neighborhoods by the number of listings. Save the bar chart as 'result.png' with a height and width of 16 and 8, respectively. The chart should have the title \"Top 10 Neighborhoods by Number of Listings\", the x-axis title \"Neighborhood\", with labels for the identified neighborhoods, and the y-axis title \"Number of Listings\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-007"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-011", "source": "https://www.kaggle.com/code/hamdallak/who-is-killed-by-us-police-why-how-and-where", "instruction": "This is a Fatal Police Shootings in the US dataset, with relevant descriptions in the README.md file. Following the instructions in region.txt, count the number of people killed by police gunfire in the Western, Central, Southern, and Northeastern regions of the United States who showed no signs of mental illness. Then, plot these counts in a bar chart and save the resulting image as 'result.png'. The colors of the bars should be ['#0000FF', '#008000', '#FFA500', '#FF0000'], the title should be \"Number of Victims Killed by Gunshot without Mental Illness by Region\", the x-axis title should be \"Region\", and the y-axis title should be \"Number of Victims\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-011"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-015", "source": "https://www.kaggle.com/code/csanskriti/amazon-sales-data-analysis", "instruction": "This is an E-Commerce Sales Dataset and Amazon Sales Dataset, with relevant descriptions provided in the README.md file. Your task is to compile the sales data for products of each size sold on Amazon, sort the sizes in ascending order, and plot these figures in a bar chart. The sales should be measured in units of $10,000. Save the resulting image as result.png. The image should have a size of (12, 6), with the title 'Sales by Product Size', x-axis labeled as 'Product Size', and y-axis labeled as 'Net Revenue in 10,000 dollars'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-006", "source": "https://www.kaggle.com/code/muzammilbaloch/global-temperature-analysis", "instruction": "This is a Climate Change: Earth Surface Temperature Data dataset, with relevant descriptions in the README.md file. Following the instructions in tips.txt, you need to plot the global seasonal average temperature changes in a line graph and save the image as result.png. The graph should have a width and height of (16, 6), with the title \"Average temperature in each season\", the x-axis titled \"Year\", and the y-axis titled \"Average temperature\". The legend title should be \"Seasons Average Temperature\", with legend labels as \"Spring\", \"Summer\", \"Autumn\", and \"Winter\". The line colors should be set to \"#ffa500\" for Spring, \"#0000ff\" for Summer, \"#008000\" for Autumn, and \"#ff0000\" for Winter.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-014", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of three different portfolio strategies (default portfolio, equal-weight portfolio, and market value-weighted portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg. The line colors should be distinct, the plot title should be \u201cCumulative Returns Over Time\u201d, the x-axis title should be \u201cDate\u201d, and the y-axis title should be \u201cCumulative Returns\u201d.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-014"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-015", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of four different portfolio strategies (equal-weight portfolio, market value-weighted portfolio, the portfolio with the highest Sharpe ratio, and the global minimum volatility portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg with a size of (12,6). The plot title should be \u201cCumulative Returns Over Time\u201d, the legend labels should be \u201cCumulative EW\u201d, \u201cCumulative MCap\u201d, \u201cCumulative MSR\u201d, and \u201cCumulative GMV\u201d, the x-axis title should be \u201cDate\u201d, and the y-axis title should be \u201cCumulative Returns\u201d.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-005", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, complete the analysis.py script to count the number of games for the four major genres and plot their proportions in a pie chart. Save the chart as result.png. The chart should have a size of (12, 8), with the pie colors set to Green, Orange, Blue, and Red. Include the genre names in the legend.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-008", "source": "https://www.kaggle.com/code/brunoguedesf/projeto-eda-delivery-center", "instruction": "This is a Delivery Center: Food & Goods orders in Brazil dataset with descriptions in the README.md file. You need to find the hub city with the largest Biker Average Delivery Distance, and plot the percentage of orders delivered by driver modal for that city as a pie chart. Save the resulting image as result.jpg. The pie chart colors should be [\u201d#ffdb00\u201d, \u201c#ffb600\u201d], the image size should be (8,8), and the image title should be \u201cPercentage of orders delivered by type of driver in CURITIBA\u201d", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-008"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-002", "source": "https://www.kaggle.com/code/fahadrehman07/data-science-job-salary-prediction-glassdoor", "instruction": "This is a Data Science Jobs Salaries Dataset, with relevant descriptions provided in the README.md file. You are tasked with creating a scatter plot to visualize the relationship between observed probabilities and predicted probabilities of football match outcomes, based on parsing a CSV file compressed in gzip format following the instructions in 'guidance.txt'. Please generate the plot and save the result as 'result.png'. The x-axis should be labeled as \"estimated prob\", the y-axis as \"observed prob\", and the scatter plot labels should be \"home victory\", \"draw\", and \"away victory\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-004", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, create a scatter plot showing the relationship between the time since the release date and the user rating count. Save the plot as result.png. The plot should have a size of (16, 6), with the scatter points colored \"blue\". Label the x-axis as \"Updated version date since release (days)\" and the y-axis as \"User Rating count\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-004"]}}], "post_process": ["plot_process"]}
{"id": "dm-csv-007", "source": "https://drive.google.com/drive/folders/1wVs85eVx4KYhN0qa-eLjlNEBSLEoJGQc?usp=share_link", "instruction": "This is a TMDB 5000 Movie Dataset with a description in README.md. According to the guidance in \u2018wrFormula.tex\u2019, you should identify the top 10 most popular movies and record their name in result.csv following the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-007"]}}], "post_process": []}
{"id": "dm-csv-009", "source": "https://drive.google.com/drive/folders/14793kyfMJ5GRbe5djEr0uzvQwXkwr-JM?usp=share_link", "instruction": "This is an Amazon Books Dataset with a description in README.md. Following the guidance provided in guidance.txt, identify the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors. Record the results in author.csv following the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-009"]}}], "post_process": []}
{"id": "dm-csv-010", "source": "https://drive.google.com/drive/folders/1V339AXpqR-f1DZK9wJNd0CN_svQf-I7o?usp=sharing", "instruction": "This is a Bike Store Relational Database, with related descriptions provided in the README.md file. You need to calculate the monthly average sales volume for each bike category and write the results into avg_units_sold.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-010"]}}], "post_process": []}
{"id": "dm-csv-011", "source": "https://drive.google.com/drive/folders/1tO4GVH2DYJJwF2-HmSf3zTEzOUopthwH?usp=sharing", "instruction": "This is a Bike Store Relational Database, with relevant descriptions provided in the README.md file. You need to calculate and aggregate the total quantity sold and total sales revenue for each bike type from 2016 to 2018. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-011"]}}], "post_process": []}
{"id": "dm-csv-015", "source": "https://drive.google.com/drive/folders/1yAz4jIJ4nuBf8qKEvzPYHrzja9RBMdek?usp=share_link", "instruction": "This is a dataset of baseball games, with relevant descriptions provided in the README.md file. You need to help me calculate the player with the most games played, the player with the most runs scored, the player with the most hits, and the player with the most home runs. Each statistical data should include the player\u2019s name and their corresponding data; for example, for the player with the most games played, record their number of games played, and for the player with the most runs scored, record their runs scored. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-015"]}}], "post_process": []}
{"id": "dm-csv-016", "source": "https://drive.google.com/drive/folders/1Nfx9mAWRZzmY6e3GPrIb2oAo2JSXfsIs?usp=share_link", "instruction": "This is a dataset for football matches, with relevant descriptions provided in the README.md file. I would like you to help me compile statistics for the seasons and the corresponding winning clubs hosted by `IT1` and `BESC`. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-016"]}}], "post_process": []}
{"id": "dm-csv-025", "source": "https://drive.google.com/drive/folders/1_ZmzR5S8YNzsl9HhK1YX06y91zoX-xLR?usp=share_link", "instruction": "This is an IPL Complete Dataset (2008-2024) with relevant descriptions provided in the README.md file. Your task is to continue the development of the existing code in analys.py to calculate the winning and losing probabilities for each team according to the instructions in tips.txt. Afterwards, you need to identify the team with the highest probability of winning and the team with the highest probability of losing. Finally, write their names and corresponding (winning or losing) probabilities into team_probabilities.csv following the template provided in sample_result.csv.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-025"]}}], "post_process": []}
{"id": "dm-csv-026", "source": "https://drive.google.com/drive/folders/1MNmqFKeUQR7hpgVxc7pPJOcymQa35K9p?usp=share_link", "instruction": "This is a dataset containing information on FIFA 22 players, with detailed descriptions provided in the README.md file. Your task is to complete the analysis.py script to rank the players based on the specified score at the position level from 2015 to 2022. Relevant hints can be found in some text (txt) and Markdown (md) files. Ensure that the results are written to the result.csv file in the format demonstrated in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-026"]}}], "post_process": []}
{"id": "dm-csv-027", "source": "https://drive.google.com/drive/folders/1bzqJXG2xjnXjbzZdW8UYyH7MYVF0ZHsw?usp=share_link", "instruction": "This is a Netflix Data dataset, with detailed descriptions provided in the README.md file. Your task is to identify the top 10 countries by the number of produced shows and their corresponding counts, as well as the top 10 movie genres by the number of occurrences and their corresponding counts. Record the results in the provided templates: Top_10_countries.csv and Top_10_Movies.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-027"]}}], "post_process": []}
{"id": "dm-csv-029", "source": "https://drive.google.com/drive/folders/1ibfAui8NvdVlZTQGW5MQab57BKPQeAH_?usp=share_link", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. Your task is to identify the top 3 users with the most pull requests and record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-029"]}}], "post_process": []}
{"id": "dm-csv-030", "source": "https://drive.google.com/drive/folders/1Wyg0lPtEKqh3F_sZOnR7wNZFb2iW7yt8?usp=share_link", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. Your task is to identify the latest 10 pull requests, merge the related file data, and find the unique files involved in these pull requests. Record the results in the unique_files.csv file using the sample_result.csv template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-030"]}}], "post_process": []}
{"id": "dm-csv-031", "source": "https://drive.google.com/drive/folders/1evC6AsX-2sevVmpbE5nlpqrfO1LZgmUJ", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. Your task is to identify the users who made the last 6 pull requests for the specific file \"src/compiler/scala/reflect/reify/phases/Calculate.scala\". Record the results in the users_last_6.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-031"]}}], "post_process": []}
{"id": "dm-csv-033", "source": "https://drive.google.com/drive/folders/13jdtj7v3Ie4Hj9aasZiNjQUc46PE9eH3?usp=sharing", "instruction": "This is a dataset titled \u201cMobile-Games-Testing-with-Cookie-Cats,\u201d with detailed descriptions provided in the README.md file. Your task is to compare the 1-day retention rates between two versions of a game (gate_30 and gate_40) according to the instructions in guidance.txt. Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 and record the result in the probability_greater_retention.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-033"]}}], "post_process": []}
{"id": "dm-csv-034", "source": "https://drive.google.com/drive/folders/19EUef4q8aewkLlRDndOMM55OPCCBsFmo?usp=share_link", "instruction": "This is a dataset titled \u201cMobile-Games-Testing-with-Cookie-Cats,\u201d with detailed descriptions provided in the README.md file. Your task is to calculate the proportion of 7-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_7_by_version.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-034"]}}], "post_process": []}
{"id": "dm-csv-036", "source": "https://drive.google.com/drive/folders/1o7QlFnyB-_RcHE9Kp7Jfa3VPu1Yf72-S?usp=share_link", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. Your task is to identify the last six users who made pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala`. and fill in their usernames in the template file users_last_6.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-036"]}}], "post_process": []}
{"id": "dm-csv-037", "source": "https://drive.google.com/drive/folders/1G-zP7Vu07sIpX4mw598XqS2v8muQXRtf?usp=share_link", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. Your task is to identify the top three users who made the most pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala` and fill in their usernames in the template file top_3_developers.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-037"]}}], "post_process": []}
{"id": "dm-csv-038", "source": "https://drive.google.com/drive/folders/1X9N6aLhTfXbic4dgXAt0e8G2x8TaTHir?usp=share_link", "instruction": "This is a dataset titled \u201cThe-GitHub-History-of-the-Scala-Language,\u201d with detailed descriptions provided in the README.md file. I want to know the number of pull requests made by two users with the nicknames \u201csoc\u201d and \u201cxeno-by\u201d for each year between 2011 and 2016. Please fill in your statistical results in the provided template file pull_requests_by_year_and_author.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-038"]}}], "post_process": []}
{"id": "dm-csv-039", "source": "https://drive.google.com/drive/folders/16Hk8i_1SqBwYWI3sp-hp4rMAgyynqsjk?usp=share_link", "instruction": "This is an Avocado Toast dataset, with detailed descriptions provided in the README.md file. You need to conduct a supply chain analysis of three ingredients used in an avocado toast according to the instructions in tips.md, utilizing the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-039"]}}], "post_process": []}
{"id": "dm-csv-043", "source": "https://drive.google.com/drive/folders/1izUKLoSM66ofPI6CV_l4_gzhM3uIZI5N?usp=share_link", "instruction": "You are provided with a Customer Segmation dataset, and detailed descriptions can be found in the README.md file. Your task is to calculate the retention data for each cohort according to the instructions provided in the tips.txt file. Save the calculated results to a file named retention.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-043"]}}], "post_process": []}
{"id": "dm-csv-044", "source": "https://drive.google.com/drive/folders/1mgjfhA3CF-Ksq7U2WHLCXspVV-RJNWvP?usp=share_link", "instruction": "This is a Customer Segmation dataset, and detailed descriptions are provided in the README.md file. Your task is to calculate the average unit price paid for items by groups of customers over time. Each group should be determined by the month of their first purchase. Save the calculated results in a file named average_price.csv, utilizing the provided template file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-044"]}}], "post_process": []}
{"id": "dm-csv-050", "source": "https://drive.google.com/drive/folders/1DQdtNaN-K38D6s5WMpqkGUUW1n9mdkTX?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, you need to calculate the cumulative returns of three different portfolio strategies: the default portfolio, the equal-weight portfolio, and the market value-weighted portfolio. Write the results (rounded to three significant figures) into result.csv according to the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-050"]}}], "post_process": []}
{"id": "dm-csv-052", "source": "https://drive.google.com/drive/folders/1fLUJMT9WE5_NbHjnlMG_gvgejo8TrhlL?usp=share_link", "instruction": "This is a customer segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, use a 1-4 scale to calculate the RFM Score and complete the segmentation of users (dividing users into three groups based on their RFM Score). Write the segmentation results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-052"]}}], "post_process": []}
{"id": "dm-csv-059", "source": "https://drive.google.com/drive/folders/1t1cWSI9zD7hKexuPQtIBxd2fNsXYAQlb?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Based on the registration dates in result.csv, you need to analyze the purchasing behavior of users within the first week of their registration date. Calculate the average number of purchases made in the first week on a daily basis, and then fill the results into result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-059"]}}], "post_process": []}
{"id": "data-sa-001", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2000, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-001"]}}], "post_process": []}
{"id": "data-sa-004", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, complete the hypothesis test for \"if the number of bike rentals is similar or different in different weather conditions\" and write the corresponding results into weather.csv according to the required format.\n\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-004"]}}], "post_process": []}
{"id": "data-sa-026", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to compute the 99% Conditional Value at Risk (CVaR) for a portfolio based on 2008-2009 data. This involves loading and preprocessing the data, calculating daily returns, and computing portfolio losses. You will then fit a Student's t-distribution to these losses using t.fit, determine the 99% Value at Risk (VaR) with t.ppf, and calculate the 99% CVaR using t.expect to integrate tail losses beyond the 99% VaR threshold. Finally, save the results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-026"]}}], "post_process": []}
{"id": "data-sa-028", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=14", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate the Black-Scholes option prices for an IBM stock using historical data. This involves computing the annualized volatility based on the stock's daily returns and then calculating the option price using the Black-Scholes model for the computed volatility and for twice the volatility. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-028"]}}], "post_process": []}
{"id": "data-sa-029", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=16", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform delta hedging for an investment portfolio with IBM stock using a European put option following the instructions in step.md. After performing the delta hedging, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-029"]}}], "post_process": []}
{"id": "data-sa-031", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=10", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform a Chow test to determine if the 2008 financial crisis caused a significant change in the relationship between mortgage delinquency rates and portfolio returns. After conducting the Chow test, write the results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-031"]}}], "post_process": []}
{"id": "data-sa-039", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2002, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-039"]}}], "post_process": []}
{"id": "data-sa-051", "source": null, "instruction": "You need to identify the region and source with the highest CO2 emissions and perform ANOVA tests to compare emissions across different fuel sources within each geographical region. Please fill in the results into the anova_results.csv file following the provided format.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-051"]}}], "post_process": []}
{"id": "data-sa-054", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Calculate 10,000 bootstrap replicates of the variance in annual rainfall at the Sheffield Weather Station. Divide the data into 50 bins, compute the bin center and corresponding probability density function (PDF) for each bin. For convenience, convert the variance to units of square centimeters. Save the results to a file named result.csv, following the template provided in sample_result.csv. (Set the random seed to 42)", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-054"]}}], "post_process": []}
{"id": "ml-binary-002", "source": "https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction", "instruction": "This is a Health Insurance Cross Sell Prediction dataset, and the relevant description is in README.md. You need to predict whether the users in test.csv will respond, and write the results into submission.csv in the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-002"]}}], "post_process": []}
{"id": "ml-binary-005", "source": "https://www.kaggle.com/code/bhatnagardaksh/pca-and-lda-implementation", "instruction": "This is a Breast Cancer Wisconsin (Diagnostic) Data Set, with the dataset description available in README.md. You need to predict the tumor diagnosis result (B or M) based on the given information. The data to be predicted is in test.csv. You need to write the predicted results into label.csv with the column name 'result'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-005"]}}], "post_process": []}
{"id": "ml-binary-013", "source": "https://www.kaggle.com/code/lilyhyseni/renewind-energy-tuning-pipelining-ml-models", "instruction": "I have a dataset on wind turbine failures, with the description available in README.md. You need to predict whether the turbine will fail based on the data in Test.csv and write the prediction results into target.csv, with the column name \"Target\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-013"]}}], "post_process": []}
{"id": "ml-binary-016", "source": "https://www.kaggle.com/code/bansodesandeep/credit-card-default-prediction", "instruction": "This is a Credit Card Default dataset, with relevant descriptions provided in the README.md file. Based on the credit card client information in test.csv, you need to predict whether they will default and write the predicted results into a file named defaulter.csv, with the column name \"IsDefaulter\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-016"]}}], "post_process": []}
{"id": "ml-binary-017", "source": "https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial", "instruction": "This is a Rain in Australia dataset, with relevant descriptions provided in the README.md file. Based on the data in test.csv, you need to predict whether it will rain the next day for each row and write the predicted results into a file named tomorrow.csv, with the column name \"RainTomorrow\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-017"]}}], "post_process": []}
{"id": "ml-cluster-006", "source": "https://www.kaggle.com/code/katherineoktaviani/patient-clustering", "instruction": "This is a Patient Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-006"]}}], "post_process": []}
{"id": "ml-cluster-009", "source": "https://www.kaggle.com/code/dipansujoshi/global-country-information-clustering", "instruction": "This is a global countries dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into 3 clusters. Write the results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label)", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-009"]}}], "post_process": []}
{"id": "ml-cluster-010", "source": "https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters", "instruction": "This is a Forest Cover Type Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into an appropriate number of clusters. Write the clustering results into cluster.csv, with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-010"]}}], "post_process": []}
{"id": "ml-cluster-013", "source": "https://www.kaggle.com/code/vipulgohel/clustering-pca", "instruction": "This is a dataset of socioeconomic and health factors for various countries, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-013"]}}], "post_process": []}
{"id": "ml-cluster-016", "source": "https://www.kaggle.com/code/khusheekapoor/clustering-using-rfm-analysis-in-python", "instruction": "This is an Online Retail II UCI dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment the customers into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-016"]}}], "post_process": []}
{"id": "ml-cluster-019", "source": "https://www.kaggle.com/code/pinardogan/rfm-customer-segmentation-using-k-means", "instruction": "This is an Online Retail II Data Set dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment different types of customer groups into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-019"]}}], "post_process": []}
{"id": "ml-competition-001", "source": "https://www.kaggle.com/code/abdmental01/bank-churn-lightgbm-and-catboost-0-8945", "instruction": "This is a dataset for a Bank customer data for churn prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-001"]}}], "post_process": []}
{"id": "ml-competition-002", "source": "https://www.kaggle.com/code/arunklenin/ps3e24-smoking-cessation-prediction-binary", "instruction": "This is a dataset for a health indicators and smoking status prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-002"]}}], "post_process": []}
{"id": "ml-competition-003", "source": "https://www.kaggle.com/code/tetsutani/icr-iarc-eda-ensemble-and-stacking-baseline", "instruction": "This is a dataset for a health features and medical condition prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-003"]}}], "post_process": []}
{"id": "ml-competition-005", "source": "https://www.kaggle.com/code/markuslill/s3e26-xgbclassifer", "instruction": "This is a dataset for a Patient data for cirrhosis outcomes prediction competition, along with an additional dataset. The descriptions are available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-005"]}}], "post_process": []}
{"id": "ml-competition-006", "source": "https://www.kaggle.com/code/abhi011097/s3-e5-eda-multi-approach-models-w-o-smote", "instruction": "This is a dataset for a Wine Quality Prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-006"]}}], "post_process": []}
{"id": "ml-competition-008", "source": "https://www.kaggle.com/code/aritra100/notebook-linear-regression-vs-regressor-r2score85", "instruction": "This is a flood prediction competition dataset, with the description available in README.md. As a participant, you need to design a method to predict the probability of flood occurrence in test.csv and write the prediction results into submission.csv, following the format of the sample_submission.csv template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-008"]}}], "post_process": []}
{"id": "ml-competition-009", "source": "https://www.kaggle.com/code/rohangulati14/leading-sol-regression-with-an-abalone-dataset", "instruction": "This is a competition based on an abalone dataset, with the description available in README.md. As a participant, you need to design a solution to complete this competition. Write your prediction results into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-009"]}}], "post_process": []}
{"id": "ml-competition-010", "source": "https://www.kaggle.com/code/iqmansingh/crab-age-voting-regression-synthetic-data", "instruction": "This is a crab age prediction competition, with the description available in README.md. To achieve a higher ranking, an additional dataset of crab ages is provided. As a participant, you need to design and optimize a method to complete this competition. Write the prediction results into submission.csv, following the format of 'sample_submission.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-010"]}}], "post_process": []}
{"id": "ml-competition-013", "source": "https://www.kaggle.com/code/tumpanjawat/s3e19-course-eda-fe-lightgbm", "instruction": "This is a dataset for a sales volume prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-013"]}}], "post_process": []}
{"id": "ml-competition-017", "source": "https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799", "instruction": "You are participating in an Automated Essay Scoring competition. The dataset and relevant descriptions are provided in the README.md file. As a participant, your task is to generate predictions of data in test.csv and write the results into a submission file named submission.csv in the format specified by sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-017"]}}], "post_process": []}
{"id": "ml-competition-020", "source": "https://www.kaggle.com/code/hardikgarg03/horse-health-prediction-using-stacked-models", "instruction": "This is a dataset for the Predict Health Outcomes of Horses competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-020"]}}], "post_process": []}
{"id": "ml-multi-003", "source": "https://www.kaggle.com/code/benvarghese/linkedin-job-posting-machine-learning", "instruction": "This is a job information dataset, with the description available in README.md. You need to predict the experience level of job positions in test.csv based on this dataset and write the results into result.csv, with the column name \"formatted_experience_level\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-003"]}}], "post_process": []}
{"id": "ml-multi-008", "source": "https://www.kaggle.com/code/steremma/news-exploration-using-gensim-and-sklearn", "instruction": "This is a News Aggregator Dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named category.csv, with the column name \"CATEGORY\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-008"]}}], "post_process": []}
{"id": "ml-multi-009", "source": "https://www.kaggle.com/code/thisishusseinali/malicious-url-detection", "instruction": "This is a Malicious URLs dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named type.csv, with the column name \"type\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-009"]}}], "post_process": []}
{"id": "ml-multi-011", "source": "https://www.kaggle.com/code/datatattle/battle-of-ml-classification-models", "instruction": "This is a Coronavirus Tweets NLP - Text Classification dataset, with relevant descriptions provided in the README.md file. Based on the text information in Corona_NLP_test.csv, you need to predict their sentiment (classifying them as \"Positive,\" \"Negative,\" or \"Neutral\") and write the predicted results into a file named sentiment.csv, with the column name \"Sentiment.\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-011"]}}], "post_process": []}
{"id": "ml-regression-002", "source": "https://www.kaggle.com/code/nigelclinton/energy-price-prediction-ml", "instruction": "I have a dataset on Spanish electricity and weather, with the description available in README.md. You need to predict the electricity prices in test.csv based on this dataset. Write your prediction results into result.csv, with the column name \"price actual\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-002"]}}], "post_process": []}
{"id": "ml-regression-004", "source": "https://www.kaggle.com/code/dima806/spotify-song-ratings-predict-catboost-shap", "instruction": "I currently have a pop music dataset, with the description available in README.md. You need to predict the popularity of the songs in test.csv based on this dataset. Write the prediction results into popularity.csv, with the column name \"Poplarity\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-004"]}}], "post_process": []}
{"id": "ml-regression-006", "source": "https://www.kaggle.com/code/hknaralasetty/lego-regression-55-23-randomforest-mae-e-5", "instruction": "This is a Lego sets and price [1955 - 2023] dataset, with the description available in README.md. You need to predict the star rating for the data in test.csv. Write your prediction results into result.csv, with the column name \"Star rating\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-006"]}}], "post_process": []}
{"id": "ml-regression-008", "source": "https://www.kaggle.com/code/michaelminhpham/predict-unit-sold-mens-women-shoes-on-tiki", "instruction": "This is a dataset of product information from the TIKI e-commerce platform, with the description available in README.md. You need to predict the product sales based on the product information in test.csv. Please note that all items in test.csv are shoes. Write the prediction results into quantity.csv, with the column name \"quantity_sold\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-008"]}}], "post_process": []}
{"id": "ml-regression-010", "source": "https://www.kaggle.com/code/renjiabarai/drug-reviews-regression", "instruction": "This is a dataset of comprehensive drug ratings, with the description available in README.md. You need to predict the usefulness of drugs in drugsComTest_raw.csv based on this dataset and write the prediction results into \"Usefulness.csv\", with the column name \"usefulness\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-010"]}}], "post_process": []}
{"id": "ml-regression-011", "source": "https://www.kaggle.com/code/mehmetisik/u-s-farms-biogas-ml-prediction-livestock", "instruction": "Here is a biogas dataset, with the description available in README.md. You need to predict the Biogas Generation Estimate (cu-ft/day) in test.csv based on this dataset, and write the prediction results into result.csv, with the column name 'biogas_generation_estimate_cuftday'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-011"]}}], "post_process": []}
{"id": "ml-regression-012", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of vehicle information in New York, with the description available in README.md. You need to predict the Mileage of vehicles in test.csv based on this dataset and write the results into result.csv, with the column name \"Mileage\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-012"]}}], "post_process": []}
{"id": "ml-regression-014", "source": "https://www.kaggle.com/code/qusaybtoush1990/car-prices-poland", "instruction": "This is a Car Prices Poland dataset, with relevant descriptions provided in the README.md file. Based on the vehicle information in test.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"price\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-014"]}}], "post_process": []}
{"id": "ml-regression-015", "source": "https://www.kaggle.com/code/msand1984/appliance-energy-prediction", "instruction": "This is an Appliances Energy Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding appliance energy consumption and write the predicted results into a file named appliance.csv, with the column name \"Appliances\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-015"]}}], "post_process": []}
