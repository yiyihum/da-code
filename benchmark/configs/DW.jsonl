{"id": "dw-clean-001", "instruction": "NYC_Open_Data_Parking_Violations is stored in the local. We need to clean the noisy data. There is a local document that stores data standards. Please follow the data standards for data cleaning.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-001"]}}], "post_process": []}
{"id": "dw-clean-002", "instruction": "NYC_Open_Data_Parking_Violations is stored in the database. We need to clean the noisy data. There is a local document that stores data standards. Please follow the data standards for data cleaning.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-002"]}}], "post_process": []}
{"id": "dw-clean-003", "instruction": "There have been a number of complaints indicating that some New York residents have been receiving multiple parking tickets for a single violation. This is resulting in the affected residents having to incur additional legal fees for a single incident. You have been tasked with identifying records that reflect this duplication of violations.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-003"]}}], "post_process": []}
{"id": "dw-clean-004", "instruction": "There are two ways to handle missing data: Drop data: drop the whole row, drop the whole column Replace data: replace it by mean, replace it by frequency, replace it based on other functions.Whole columns should be dropped only if most entries in the column are empty.  ", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-004"]}}], "post_process": []}
{"id": "dw-clean-005", "instruction": "Please process the data, we are developing an application in a country that accepts L/100km standard fuel consumption, and the length, width and height information of the car must be normalized. Save the result in "result.csv"", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-005"]}}], "post_process": []}
{"id": "dw-clean-006", "instruction": "Follow the data standard to clean the data, fill missing values using the most frequent value (mode) within each group defined by Street Name and Block. Save the result in "Building_Permits.csv"", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-006"]}}], "post_process": []}
{"id": "dw-clean-007", "instruction": "Only keep the airports and aircrafts information in the database in english. Dont create new db or csv files.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-007"]}}], "post_process": []}
{"id": "dw-clean-008", "instruction": "We will perform data preprocessing, including removing useless columns, replacing the "Other" activity type with "Unicycling", and handling missing values. Next, we will perform mean imputation on the heart rate data to fill in the missing values ​​based on the mean of different activity types. The columns to be removed as they are not useful for the analysis, which include: 'Friend's Tagged', 'Route Name', 'GPX File', 'Activity Id', 'Calories Burned', and 'Notes'. result should be the same file.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-008"]}}], "post_process": []}
{"id": "dw-clean-009", "instruction": "The Head Data Scientist at Training Data Ltd. has asked you to create a DataFrame that stores the data in customer_train.csv much more efficiently. Follow the requirements in README.md and store result in result.pkl.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-009"]}}], "post_process": []}
{"id": "dw-clean-010", "instruction": "Delete records with null values ​​or results of 0, and only keep genre, release_year, total_gross, inflation_adjusted_gross. Put the same genre together and sort by release_year in ascending order. Save the result in 'disney_movies_total_gross_cleaned.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-010"]}}], "post_process": []}
{"id": "dw-clean-011", "instruction": "Merge and organize data, save source and Net Promoter Score group in predefined format.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-011"]}}], "post_process": []}
{"id": "dw-clean-012", "instruction": "extract specific job-related information, including job titles, technical skills, and educational degrees. save the result in 'result.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-012"]}}], "post_process": []}
{"id": "dw-clean-013", "instruction": "Complete a data cleanup according to the given python file, including missing value processing and data normalization. The final result should be saved in result.csv.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-013"]}}], "post_process": []}
{"id": "dw-clean-014", "instruction": "Here are some records of past earthquakes. However, there may be some inconsistencies in the recording format of time. Please clean the data and try to figure out the days with the most earthquakes in history. Save the days and earthquakes in result.csv.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-014"]}}], "post_process": []}
{"id": "dw-clean-015", "instruction": "According to the requirements of the standard document, only these columns are retained and duplicate rows are removed", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-015"]}}], "post_process": []}
{"id": "dw-clean-016", "instruction": "According to the requirements in the README file, generate a new data information table and save it in result.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-016"]}}], "post_process": []}
{"id": "dw-clean-017", "instruction": "According to the requirements in the README file, clean the data. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-017"]}}], "post_process": []}
{"id": "dw-clean-018", "instruction": "The task involves standardizing terms and descriptions in the 'Gear Box' and 'Drive Type' fields, which may have different terminologies referring to the same types of gearboxes and drive systems.Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-018"]}}], "post_process": []}
{"id": "dw-clean-019", "instruction": "The task involves standardizing terms and descriptions in the 'brake type' and 'tyre type' fields. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-019"]}}], "post_process": []}
{"id": "dw-clean-020", "instruction": "The task involves standardizing terms and descriptions in the 'Fuel Suppy System' fields. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-020"]}}], "post_process": []}
{"id": "dw-clean-021", "instruction": "Follow the data schema to add new columns. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-021"]}}], "post_process": []}
{"id": "dw-clean-022", "instruction": "Follow the data schema to add or delete some columns, and refine some columns. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-022"]}}], "post_process": []}
{"id": "dw-clean-023", "instruction": "Follow the data schema to change the type of columns. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-023"]}}], "post_process": []}
{"id": "dw-clean-024", "instruction": "Please refer to the schema document to modify the data. Save the result in 'cars_details_merges.csv'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-024"]}}], "post_process": []}
{"id": "dw-clean-025", "instruction": "Transform the airquality DataFrame from wide to long format, and then generate a pivot table using 'Month' and 'Day' as indexes, with different air quality measurements as columns and their respective readings as values.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-025"]}}], "post_process": []}
{"id": "dw-clean-026", "instruction": "do data transformation according to the schema. result should be the same file", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-026"]}}], "post_process": []}
{"id": "dw-clean-027", "instruction": "I need to conduct further analysis to understand the weather data. Please convert the data according to the data schema.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-027"]}}], "post_process": []}
{"id": "dw-clean-028", "instruction": "Calculate the values ​​of is_arrested for different ratings and save them in result.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-028"]}}], "post_process": []}
{"id": "dw-clean-029", "instruction": "Arrest rates for various violation types at different weather ratings, save result in result.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-029"]}}], "post_process": []}
{"id": "dw-clean-030", "instruction": "Clean the data according to the schema. result should be in the same file", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-030"]}}], "post_process": []}
{"id": "dw-clean-031", "instruction": "merge the data, save the result in RI-clean.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-031"]}}], "post_process": []}
{"id": "dw-load-001", "instruction": "Complete data wrangling according to predefined data schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-001"]}}], "post_process": []}
{"id": "dw-load-002", "instruction": "Load these csv into the wwe.db sqlite database according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-002"]}}], "post_process": []}
{"id": "dw-load-003", "instruction": "Load these csv into the sport.db sqlite database according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-003"]}}], "post_process": []}
{"id": "dw-load-004", "instruction": "A batch of data about players has arrived, but it may already exist in the database. Please organize and add it to the sport.db database.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-004"]}}], "post_process": []}
{"id": "dw-load-005", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-005"]}}], "post_process": []}
{"id": "dw-load-006", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-006"]}}], "post_process": []}
{"id": "dw-load-007", "instruction": "Calculate profits for each sector from `fortune500`. Columns: `sector`, `pct80`. Find the first occurrence date for each tag from `stackoverflow`. Columns: `tag`, `mindate`. Save result in profit.csv and startdates.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-007"]}}], "post_process": []}
{"id": "dw-load-008", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-008"]}}], "post_process": []}
