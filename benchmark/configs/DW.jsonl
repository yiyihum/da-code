{"id": "dw-clean-001", "instruction": "NYC_Open_Data_Parking_Violations is stored in the local. We need to clean the noisy data. There is a local document that stores data standards. Please follow the data standards for data cleaning.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-001"]}}], "post_process": []}
{"id": "dw-clean-002", "instruction": "NYC_Open_Data_Parking_Violations is stored in the database. We need to clean the noisy data. There is a local document that stores data standards. Please follow the data standards for data cleaning.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-002"]}}], "post_process": []}
{"id": "dw-clean-003", "instruction": "There have been a number of complaints indicating that some New York residents have been receiving multiple parking tickets for a single violation. This is resulting in the affected residents having to incur additional legal fees for a single incident. You have been tasked with identifying records that reflect this duplication of violations.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-003"]}}], "post_process": []}
{"id": "dw-clean-004", "instruction": "There is some raw data here that needs to do data wrangling. Please process the missing data. ", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-004"]}}], "post_process": []}
{"id": "dw-clean-005", "instruction": "Please process the data, we are developing an application in a country that accepts L/100km standard fuel consumption, and the length, width and height information of the car must be normalized", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-005"]}}], "post_process": []}
{"id": "dw-clean-006", "instruction": "Follow the data standard to clean the data, fill missing values using the most frequent value (mode) within each group defined by Street Name and Block", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-006"]}}], "post_process": []}
{"id": "dw-clean-007", "instruction": "Only keep the airports and aircrafts information in the database in english", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-007"]}}], "post_process": []}
{"id": "dw-clean-008", "instruction": "We will perform data preprocessing, including removing useless columns, replacing the \"Other\" activity type with \"Unicycling\", and handling missing values. Next, we will perform mean imputation on the heart rate data to fill in the missing values \u200b\u200bbased on the mean of different activity types. The columns to be removed as they are not useful for the analysis, which include: 'Friend's Tagged', 'Route Name', 'GPX File', 'Activity Id', 'Calories Burned', and 'Notes'.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-008"]}}], "post_process": []}
{"id": "dw-clean-009", "instruction": "The Head Data Scientist at Training Data Ltd. has asked you to create a DataFrame that stores the data in customer_train.csv much more efficiently. Follow the requirements in README.md and store result in result.pkl.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-009"]}}], "post_process": []}
{"id": "dw-clean-010", "instruction": "Delete records with null values \u200b\u200bor results of 0, and only keep genre, release_year, total_gross, inflation_adjusted_gross. Put the same genre together and sort by release_year in ascending order.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-010"]}}], "post_process": []}
{"id": "dw-clean-011", "instruction": "Merge and organize data, save source and Net Promoter Score group in predefined format", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-011"]}}], "post_process": []}
{"id": "dw-clean-012", "instruction": "extract specific job-related information, including job titles, technical skills, and educational degrees", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-012"]}}], "post_process": []}
{"id": "dw-clean-013", "instruction": "Complete a data cleanup according to the given python file, including missing value processing and data normalization. The final result should be saved in result.csv.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-013"]}}], "post_process": []}
{"id": "dw-clean-014", "instruction": "Here are some records of past earthquakes. However, there may be some inconsistencies in the recording format of time. Please clean the data and try to figure out the days with the most earthquakes in history. Save the days and earthquakes in result.csv.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-014"]}}], "post_process": []}
{"id": "dw-clean-015", "instruction": "According to the requirements of the standard document, only these columns are retained and duplicate rows are removed", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-015"]}}], "post_process": []}
{"id": "dw-clean-016", "instruction": "According to the requirements in the README file, generate a new data information table and save it in result.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-016"]}}], "post_process": []}
{"id": "dw-clean-017", "instruction": "According to the requirements in the README file, clean the data", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-017"]}}], "post_process": []}
{"id": "dw-clean-018", "instruction": "The task involves standardizing terms and descriptions in the 'Gear Box' and 'Drive Type' fields, which may have different terminologies referring to the same types of gearboxes and drive systems.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-018"]}}], "post_process": []}
{"id": "dw-clean-019", "instruction": "The task involves standardizing terms and descriptions in the 'brake type' and 'tyre type' fields.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-019"]}}], "post_process": []}
{"id": "dw-clean-020", "instruction": "The task involves standardizing terms and descriptions in the 'Fuel Suppy System' fields", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-020"]}}], "post_process": []}
{"id": "dw-clean-021", "instruction": "Follow the data schema to add new columns", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-021"]}}], "post_process": []}
{"id": "dw-clean-022", "instruction": "Follow the data schema to add or delete some columns, and refine some columns", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-022"]}}], "post_process": []}
{"id": "dw-clean-023", "instruction": "Follow the data schema to change the type of columns", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-023"]}}], "post_process": []}
{"id": "dw-clean-024", "instruction": "Please refer to the schema document to modify the data", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-024"]}}], "post_process": []}
{"id": "dw-clean-025", "instruction": "Transform the airquality DataFrame from wide to long format, and then generate a pivot table using 'Month' and 'Day' as indexes, with different air quality measurements as columns and their respective readings as values.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-025"]}}], "post_process": []}
{"id": "dw-clean-026", "instruction": "do data transformation according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-026"]}}], "post_process": []}
{"id": "dw-clean-027", "instruction": "I need to conduct further analysis to understand the weather data. Please convert the data according to the data schema.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-027"]}}], "post_process": []}
{"id": "dw-clean-028", "instruction": "Calculate the values \u200b\u200bof is_arrested for different ratings and save them in result.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-028"]}}], "post_process": []}
{"id": "dw-clean-029", "instruction": "Arrest rates for various violation types at different weather ratings", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-029"]}}], "post_process": []}
{"id": "dw-clean-030", "instruction": "Clean the data according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-030"]}}], "post_process": []}
{"id": "dw-clean-031", "instruction": "merge the data", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-clean-031"]}}], "post_process": []}
{"id": "dw-load-001", "instruction": "Complete data wrangling according to predefined data schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-001"]}}], "post_process": []}
{"id": "dw-load-002", "instruction": "Load these csv into the wwe.db sqlite database according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-002"]}}], "post_process": []}
{"id": "dw-load-003", "instruction": "Load these csv into the wwe.db sqlite database according to the schema", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-003"]}}], "post_process": []}
{"id": "dw-load-004", "instruction": "A batch of data about players has arrived, but it may already exist in the database. Please organize and add it to the database.", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-004"]}}], "post_process": []}
{"id": "dw-load-005", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-005"]}}], "post_process": []}
{"id": "dw-load-006", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-006"]}}], "post_process": []}
{"id": "dw-load-007", "instruction": "Calculate profits for each sector from `fortune500`. Columns: `sector`, `pct80`. Find the first occurrence date for each tag from `stackoverflow`. Columns: `tag`, `mindate`. Save result in profit.csv and startdates.csv", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-007"]}}], "post_process": []}
{"id": "dw-load-008", "instruction": "load the data into the sqlite database 'database.db'", "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dw-load-008"]}}], "post_process": []}
