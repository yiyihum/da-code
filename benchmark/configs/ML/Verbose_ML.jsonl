{"id": "ml-binary-002", "source": "https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction", "instruction": "This is a Health Insurance Cross Sell Prediction dataset, and the relevant description is in README.md. You need to predict whether the users in test.csv will respond, and write the results into submission.csv in the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Loading:**\n   - Load the training and testing datasets from CSV files.\n\n2. **Feature Identification:**\n   - Identify numerical and categorical features within the dataset.\n\n3. **Categorical Variable Handling:**\n   - Encode categorical variables using appropriate techniques like mapping or one-hot encoding to represent them numerically.\n\n4. **Data Type Consistency:**\n   - Ensure all categorical variables are consistently represented as strings or integers, depending on the encoding method chosen.\n\n5. **Numerical Feature Standardization:**\n   - Standardize numerical features so they have a mean of 0 and standard deviation of 1. This ensures features are on a similar scale and prevents any one feature from dominating others.\n\n6. **Normalization:**\n   - Normalize numerical features to a specific range, such as scaling them between 0 and 1. This can be achieved using techniques like Min-Max scaling to handle different ranges of numerical data appropriately.\n\n7. **Feature Engineering:**\n   - Perform necessary transformations or create new features based on domain knowledge or statistical analysis. This step can improve the model's ability to capture relevant patterns in the data.\n\n8. **Column Dropping:**\n   - Remove columns that do not provide useful information for model training or evaluation. This could include columns with constant values or those highly correlated with other features.\n\n9. **Test Data Transformation:**\n   - Apply the same preprocessing steps used for training data to the test dataset. This ensures that the model sees data in the same format during training and evaluation, preventing data leakage and ensuring consistency in predictions.\n\n10. **Hyperparameter Definition:**\n    - Define a range of hyperparameters to optimize during model training. This includes parameters like learning rate, regularization strength, or tree depth in decision trees. The goal is to find the combination that maximizes model performance without overfitting to the training data.\n11. **Output submission**\n    - make prediction on test data and write the results into submission.csv in the format of sample_submission.csv.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-002"]}}], "post_process": []}
{"id": "ml-binary-005", "source": "https://www.kaggle.com/code/bhatnagardaksh/pca-and-lda-implementation", "instruction": "This is a Breast Cancer Wisconsin (Diagnostic) Data Set, with the dataset description available in README.md. You need to predict the tumor diagnosis result (B or M) based on the given information. The data to be predicted is in test.csv. You need to write the predicted results into label.csv with the column name 'result'.\n\n[Verbose]\n1. **Load Data**:\n   - Load training data and test data from \"test.csv\" \n2. **Drop Unnecessary Columns**:\n   - Remove columns \"id\" and \"Unnamed: 32\" from the training data.\n   - Remove the \"id\" column from the test data.\n3. **Handle Categorical Data**:\n   - Identify the categorical column \"diagnosis\" which contains the labels.\n4. **Select Numerical Columns**:\n   - Identify numerical columns (type 'float64') from the training data.\n5. **Feature Selection Using Kruskal-Wallis Test**:\n   - Perform Kruskal-Wallis test on each numerical column to assess statistical significance with respect to the categorical column \"diagnosis\".\n   - Drop numerical columns that are not statistically significant (p-value >= alpha).\n6. **Prepare Input and Target Data**:\n   - Separate the remaining columns into input features and target column for both training and test data.\n   - Map categorical target labels ('B' and 'M') to numerical values (0 and 1).\n7. **Data Scaling**:\n   - Scale the input features of both training and test data using MinMaxScaler to ensure they are within a consistent range.\n8. **Handle Class Imbalance**:\n   - Calculate class weights based on the frequency of each class in the training data to address class imbalance effectively.\n9. **Train the Model**:\n    - Initialize a RandomForestClassifier with the computed class weights and train it on the prepared training data.\n10. **Predict on Test Data**:\n    - Use the trained model to predict outcomes on the test data.\n    - Convert numerical predictions back to categorical labels ('B' and 'M') for interpretation.\n11. **Save Predictions**:\n    - Save the predicted labels to a CSV file named \"label.csv\" with a column named \"result\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-005"]}}], "post_process": []}
{"id": "ml-binary-013", "source": "https://www.kaggle.com/code/lilyhyseni/renewind-energy-tuning-pipelining-ml-models", "instruction": "I have a dataset on wind turbine failures, with the description available in README.md. You need to predict whether the turbine will fail based on the data in Test.csv and write the prediction results into target.csv, with the column name \"Target\".\n\n[Verbose]\n2. **Load Data**:\n   - Load training data from \"Train.csv\" and test data from \"Test.csv\".\n   \n3. **Initial Data Exploration**:\n   - View the first few rows of both datasets to understand their structure.\n   - Check the dimensions (rows and columns) of each dataset.\n   - Identify and count null values in each column.\n   \n4. **Prepare Input and Target Data**:\n   - Create copies of the training and test datasets.\n   - Separate input features (X) from target labels (y) in the training data.\n\n5. **Handle Missing Values**:\n   - Replace missing values in numeric columns using the median value from the training data.\n   - Apply the same replacement to the test data to maintain consistency.\n\n6. **Data Scaling**:\n   - Standardize the numeric features to have zero mean and unit variance.\n   - Scale the test data using parameters learned from the training data.\n\n7. **Handle Class Imbalance**:\n   - Address class imbalance in the training data using SMOTE, generating synthetic samples for minority class.\n\n8. **Define Pipeline**:\n   - Create a processing pipeline with steps for imputation, scaling, and model training.\n   - Configure AdaBoostClassifier with specific parameters including the base estimator, number of estimators, and learning rate.\n\n9. **Train the Model**:\n   - Fit the pipeline on the training data to train the AdaBoostClassifier model.\n\n10. **Predict on Test Data**:\n    - Use the trained pipeline to make predictions on the test data.\n\n11. **Save Predictions**:\n    - Save the predictions as a CSV file named \"target.csv\" with a column header \"Target\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-013"]}}], "post_process": []}
{"id": "ml-binary-016", "source": "https://www.kaggle.com/code/bansodesandeep/credit-card-default-prediction", "instruction": "This is a Credit Card Default dataset, with relevant descriptions provided in the README.md file. Based on the credit card client information in test.csv, you need to predict whether they will default and write the predicted results into a file named defaulter.csv, with the column name \"IsDefaulter\".\n\n[Verbose]\n1. **Load Data**:\n   - Load the train dataset and test dataset\n2. **Rename Columns for Convenience**:\n   - Rename the target column 'default.payment.next.month' to 'IsDefaulter'.\n3. **Clean and Recode Data**:\n   - Recode values in 'EDUCATION' column to merge categories 0, 5, and 6 into category 4.\n   - Recode values in 'MARRIAGE' column to merge category 0 into category 3.\n4. **Categorical Feature Encoding**:\n   - Convert categorical features ['SEX', 'EDUCATION', 'MARRIAGE'] to human-readable strings.\n   - Convert 'AGE' column to integer type.\n5. **Feature Engineering**:\n   - Create new features 'Payement_Value' and 'Dues' based on specified calculations.\n6. **One-Hot Encoding**:\n   - Apply one-hot encoding to 'EDUCATION' and 'MARRIAGE' categorical features.\n7. **Label Encoding**:\n   - Encode 'SEX' feature numerically.\n8. **Prepare Input and Target Data**:\n   - Separate input features (X) and target labels (y), excluding specified columns.\n9. **Data Scaling**:\n    - Standardize the input features (X) using `StandardScaler`.\n10. **Hyperparameter Tuning with Grid Search**:\n    - Define a parameter grid for model hyperparameters.\n    - Use `GridSearchCV` to find the best parameters for the model.\n11. **Model Training**:\n    - Train the optimal model obtained from grid search on the training data.\n12. **Predict on Test Data**:\n    - Generate predictions using the trained model on the test data.\n13. **Save Predictions**:\n    - Export the predictions to a CSV file named \"defaulter.csv\" with a column \"IsDefaulter\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-016"]}}], "post_process": []}
{"id": "ml-binary-017", "source": "https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial", "instruction": "This is a Rain in Australia dataset, with relevant descriptions provided in the README.md file. Based on the data in test.csv, you need to predict whether it will rain the next day for each row and write the predicted results into a file named tomorrow.csv, with the column name \"RainTomorrow\".\n\n[Verbose]\n1 **Load Data**:\n   - Read train dataset and test dataset\n2. **Initial Data Cleaning**:\n   - Remove the 'RISK_MM' column from the dataset.\n3. **Identify and Handle Missing Values in Categorical Variables**:\n   - Identify categorical columns with missing values and count the number of missing entries for each.\n4. **Check Cardinality in Categorical Variables**:\n   - Determine the number of unique categories in each categorical column to assess cardinality.\n5. **Date Parsing and Feature Extraction**:\n   - Convert the 'Date' column to datetime format.\n   - Extract 'Year', 'Month', and 'Day' from the 'Date' column and add them as separate features.\n6. **Identify Categorical Variables**:\n   - List and identify all categorical columns in the dataset.\n7. **One-Hot Encoding of Categorical Variables**:\n   - Encode categorical columns 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', and 'RainToday' using one-hot encoding, handling missing values appropriately.\n8. **Identify and Handle Outliers**:\n   - Calculate outliers using Interquartile Range (IQR) for specified numerical columns like 'Rainfall' and 'Evaporation'.\n9. **Impute Missing Values**:\n    - Fill missing values in numerical columns with the median of each respective column from the training set.\n    - Fill missing values in categorical columns with the most frequent value from the training set.\n10. **Cap Maximum Values in Certain Numerical Features**:\n    - Set upper caps on values for 'Rainfall', 'Evaporation', 'WindSpeed9am', and 'WindSpeed3pm' to manage outliers effectively.\n11. **Feature Scaling**:\n    - Normalize or scale numerical features using MinMaxScaler across both training and testing datasets.\n12. **Train Logistic Regression Model**:\n    - Initialize and train a logistic regression model using the preprocessed training data.\n13. **Save Predictions**:\n    - Generate predictions on the test set using the trained logistic regression model.\n    - Save the predictions to a CSV file named \"tomorrow.csv\" with a column header \"RainTomorrow\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-017"]}}], "post_process": []}
{"id": "ml-cluster-006", "source": "https://www.kaggle.com/code/katherineoktaviani/patient-clustering", "instruction": "This is a Patient Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).\n\n[Verbose]\n\n1. **Load Data**:\n   - Read the dataset \"patient_dataset.csv\" \n2. **Data Preprocessing**:\n   - Encode categorical variables 'residence_type' and 'smoking_status' using `LabelEncoder` for numerical representation.\n\n3. **Chi-Square Test**:\n   - Perform chi-square tests to analyze the association between 'residence_type' and other attributes in the dataset.\n\n4. **Data Cleaning**:\n   - Drop the 'residence_type' column following chi-square tests as it's deemed irrelevant or redundant.\n   - Convert the 'gender' column to integer type for consistency.\n\n5. **Standardization**:\n   - Create a standardized version of the dataset by scaling numerical features to have zero mean and unit variance using `StandardScaler`.\n\n6. **Min-Max Scaling**:\n   - Create a Min-Max scaled version where numerical features are scaled to a specified range [-1, 1] using `MinMaxScaler`.\n\n7. **Clustering**:\n   - Define the number of clusters (`k = 3`) for clustering analysis.\n   - Apply KMeans clustering with different initialization methods (`'random'` and `'k-means++'`) on both `df_std` and `df_mm`.\n\n8. **Save Results**:\n    - Incorporate clustered data and original feature columns, with an additional 'Cluster' column indicating cluster labels, and sace it to a CSV file named \"cluster.csv\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-006"]}}], "post_process": []}
{"id": "ml-cluster-009", "source": "https://www.kaggle.com/code/dipansujoshi/global-country-information-clustering", "instruction": "This is a global countries dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into 3 clusters. Write the results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label)\n\n[Verbose]\n1. **Reading Data**:\n   - Load the dataset from the CSV file named `world-data-2023.csv` \n2. **Selecting Important Columns**:\n   - Extract specific columns: 'Country', 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'CPI', 'GDP', 'Tax revenue (%)', 'Unemployment rate' necessary for analysis.\n3. **Data Cleaning and Preparation**:\n   - Convert 'Agricultural Land( %)' to a float after stripping '%' symbols.\n   - Convert 'Land Area(Km2)' and 'Armed Forces size' to float after removing commas for numerical consistency.\n   - Convert 'CPI' and 'GDP' by stripping '$' signs and commas, then converting to float.\n   - Convert 'Tax revenue (%)' and 'Unemployment rate' to float after stripping '%' symbols.\n   - Impute missing values using the mean for columns: 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'GDP', 'Unemployment rate'.\n4. **Principal Component Analysis (PCA)**:\n   - Perform PCA with 3 components on the selected numerical columns: 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'GDP', 'Unemployment rate'.\n5. **Clustering**:\n   - Apply KMeans clustering with `n_clusters=3` on the PCA-transformed data to group countries based on similar features.\n7. **Saving Results**:\n   - Save the clustered data to a CSV file named `cluster.csv`, including the transformed PCA features and a 'Cluster' label", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-009"]}}], "post_process": []}
{"id": "ml-cluster-010", "source": "https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters", "instruction": "This is a Forest Cover Type Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into an appropriate number of clusters. Write the clustering results into cluster.csv, with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from clustering).\n\n[Verbose]\n1. **Reading the Dataset:**\n   - Load the data from the CSV file `covtype.csv`.\n   - Identify and select numerical columns from the dataset.\n2. **Calculating New Feature:**\n   - Compute `Distance_To_Hydrology` using Euclidean distance based on `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology`.\n   - Drop the original distance columns\n3. **Mapping Categorical Variable:**\n   - Convert numerical  values to their corresponding names for better readability.\n4. **Encoding Categorical Variables:**\n   - Transform categorical variables into dummy variables to prepare them for further analysis.\n5. **Scaling Numerical Data:**\n   - Standardize numerical columns like elevation, aspect, etc., using `StandardScaler` to normalize them.\n6. **TSNE Initialization:**\n   - Initialize TSNE with different dimensionalities (`n_components=1`, `2`, and `3`) and a specific perplexity value (e.g., `perplexity=30`) for dimensionality reduction.\n7. **Performing TSNE Transformation:**\n   - Apply TSNE transformation separately to generate 1D, 2D, and 3D representations of the data.\n8. **Concatenating TSNE Components:**\n   - Combine the TSNE components with the original dataset to create, facilitating visualization and further analysis.\n9. **Saving Results:**\n    - Save the combined data including TSNE components, and 'Cluster' label to a new CSV file named `cluster.csv`, without including the index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-010"]}}], "post_process": []}
{"id": "ml-cluster-013", "source": "https://www.kaggle.com/code/vipulgohel/clustering-pca", "instruction": "This is a dataset of socioeconomic and health factors for various countries, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\n[Verbose]\n\n1. **Read and Review Data:**\n   - Load the dataset `Country-data.csv` from the specified path into a dataframe named `country_data`.\n2. **Identify Feature Types:**\n   - Identify categorical and numerical features in the dataset using appropriate method\n3. **Correcting Data:**\n   - Convert percentage values of 'Exports', 'Imports', and 'Health_Spending' to absolute values based on 'GDP_per_capita' for consistency and accuracy.\n4. **Data Scaling:**\n   - Standardize numerical features using `StandardScaler` to ensure all features contribute equally to the analysis.\n5. **Principal Component Analysis (PCA):**\n   - Apply PCA to reduce the dimensionality of the scaled data to 5 principal components using `IncrementalPCA` for efficiency.\n6. **Hierarchical Clustering:**\n   - Perform hierarchical clustering (`linkage`) on the PCA-transformed data using single linkage and the Euclidean distance metric.\n7. **Cluster Labeling:**\n   - Determine clusters using a specified distance threshold (`2.4`) and assign cluster labels using `fcluster` based on the hierarchical clustering results.\n8. **Save Results:**\n    - Save the DataFrame with cluster labels \"Cluster\" and PCA-transformed features  to a CSV file named `cluster.csv`, ensuring to exclude the index for clarity and ease of use.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-013"]}}], "post_process": []}
{"id": "ml-cluster-016", "source": "https://www.kaggle.com/code/khusheekapoor/clustering-using-rfm-analysis-in-python", "instruction": "This is an Online Retail II UCI dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment the customers into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\n[Verbose]\n1. **Reading and Preparing Data:**\n   - Load the dataset `online_retail_II.csv` into a DataFrame `df`, ensuring `InvoiceDate` is parsed as dates and set as the index for time-based analysis.\n   - Extract transactions occurring between April 1, 2011, and December 9, 2011.\n   - Handle missing values by dropping rows where `Customer ID` is null to maintain data integrity.\n   - Filter out credit transactions indicated by 'C' in the `Invoice` column \n   - Remove duplicate transactions, retaining only the first occurrence to maintain transaction accuracy.\n2. **Calculating RFM Metrics:**\n   - Compute Recency (R) based on the month of each transaction to capture how recently customers made purchases.\n   - Initialize Frequency (F) to zero and calculate the number of transactions per customer to gauge customer engagement.\n   - Compute Monetary Value (M) by multiplying `Quantity` and `Price`, then aggregate to find the total revenue per customer.\n3. **Clustering with Agglomerative Clustering:**\n   - Apply Agglomerative Clustering with 3 clusters (`n_clusters=3`) on `df_rfm`, focusing on the features R, F, and M to group customers based on their transaction behavior.\n   - Define a custom function `plot_dendrogram` to visualize the dendrogram of the hierarchical clustering process for better understanding of cluster formation.\n4. **Post-Clustering Actions:**\n   - Assign cluster labels (`Cluster`) obtained from Agglomerative Clustering to each customer.\n   - Create a new DataFramecontaining features R, F, M, and corresponding Cluster labels.\n   - Save `cluster_results` to a CSV file named `cluster.csv`, excluding the index to maintain clarity and simplicity.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-016"]}}], "post_process": []}
{"id": "ml-cluster-019", "source": "https://www.kaggle.com/code/pinardogan/rfm-customer-segmentation-using-k-means", "instruction": "This is an Online Retail II Data Set dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment different types of customer groups into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\n[Verbose]\n1. **Reading and Preparing Data:**\n   - Read the Excel file `online_retail_II.xlsx` from sheet \"Year 2009-2010\" using Pandas `read_excel` function.\n   - Identify columns in `df` such as `Customer ID`, `InvoiceDate`, `Invoice`, `Quantity`, `Price`, and `TotalPrice`.\n\n2. **Handling Missing Values:**\n   - Identify and display records where `Customer ID` matches specific missing values\n\n3. **Invoice Analysis:**\n   - Analyze invoices starting with the letter \"C\" to understand quantities and prices\n\n4. **Feature Engineering:**\n   - Create a new feature `TotalPrice` by multiplying `Quantity` with `Price`.\n\n5. **Date Manipulation:**\n   - Define `today_date` as `InvoiceDate` maximum plus 2 days to standardize recency calculation across timezones.\n\n6. **RFM Analysis:**\n   - Group by `Customer ID` to compute RFM metrics: Recency (`InvoiceDate` difference from `today_date`), Frequency (`Invoice` count), and Monetary (`TotalPrice` sum).\n\n7. **Normalization:**\n   - Normalize RFM metrics using `MinMaxScaler` from `sklearn.preprocessing`.\n\n8. **Clustering with KMeans:**\n   - Apply KMeans clustering (`KMeans`) with 6 clusters (`n_clusters=6`) on normalized RFM data (`df`).\n\n9. **Post-Clustering Actions:**\n   - Assign cluster labels (`Cluster`) to `df_clustered` DataFrame.\n   - Save `df_clustered` to CSV file named `cluster.csv` with columns derived from `df`.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-019"]}}], "post_process": []}
{"id": "ml-competition-001", "source": "https://www.kaggle.com/code/abdmental01/bank-churn-lightgbm-and-catboost-0-8945", "instruction": "This is a dataset for a Bank customer data for churn prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Loading and Preparation:**\n   - Load the training (`train.csv`), testing (`test.csv`), and submission (`sample_submission.csv`) datasets from CSV files.\n   - Check the dimensions and basic statistics of each dataset, including the number of rows, columns, and presence of null values.\n2. **Data Cleaning and Preprocessing:**\n   - Handle missing values in the datasets using appropriate methods such as imputation or removal.\n   - Scale numeric columns using `MinMaxScaler` to ensure consistent ranges.\n   - Encode categorical text features using TF-IDF vectorization to transform them into numerical representations.\n3. **Feature Engineering:**\n   - Create new meaningful features that capture insights from existing data.\n   - One-hot encode categorical features to convert them into a format suitable for machine learning models.\n4. **Modeling:**\n   - Define feature columns for training the model, excluding non-predictive columns \n   - Utilize CatBoostClassifier within a StratifiedKFold cross-validation framework to train and validate the model, ensuring robustness and performance assessment.\n5. **Prediction and Submission Preparation:**\n   - Generate predictions for the test data using the trained CatBoost model, predicting probabilities of customer churn.\n   - Prepare the submission file (`submission_cat3.csv`) by mapping predicted probabilities to the `Exited` column for submission.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-001"]}}], "post_process": []}
{"id": "ml-competition-002", "source": "https://www.kaggle.com/code/arunklenin/ps3e24-smoking-cessation-prediction-binary", "instruction": "This is a dataset for a health indicators and smoking status prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Reading and Exploration:**\n   - Load data from CSV files (`train.csv`, `test.csv`, `sample_submission.csv`) to examine the dimensions and structure of each dataset \n2. **Data Preprocessing:**\n   - **Duplicate Removal:** Remove duplicate entries from the original training dataset\n   - **Column Transformations:**normalize numerical features such as age, height, weight, etc. \n3. **Model Training and Evaluation:**\n   - Evaluate several models using cross-validation with 10 folds:\n     - **k-Nearest Neighbors (kNN):** \n     - **Linear Discriminant Analysis (LDA):**\n     - **Logistic Regression:** Train regularized logistic regression with specified parameters.\n     - **Random Forest (RF), Extra Trees (ET), Gradient Boosting (HistGB, LGBM, XGB):** Configure each with respective settings for boosting, trees, and ensemble methods.\n4. **Ensemble Model Training:**\n   - **Construct Ensembles:** Train several ensemble models using pipelines and the defined column transformer configurations.\n   - **Weighted Ensemble:** Test various weights (`w1`, `w2`, `w3`, `w4`) using `np.random.random_sample` to combine predictions`.\n5. **Final Prediction:**\n   - **Fit Models:** Refit all models  on the entire training dataset.\n   - **Test Predictions:** Generate predictions for the test set (`test`) using these models.\n   - **Ensemble Prediction:** Use the best weights to compute an ensemble prediction.\n6. **Output:**\n   - Save the final submission file (`submission.csv`) containing predictions based on the ensemble model.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-002"]}}], "post_process": []}
{"id": "ml-competition-003", "source": "https://www.kaggle.com/code/tetsutani/icr-iarc-eda-ensemble-and-stacking-baseline", "instruction": "This is a dataset for a health features and medical condition prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Loading and Preprocessing**:\n   - Load datasets (`train.csv`, `test.csv`, `greeks.csv`) setting `Id` column as index.\n2. **Handling Missing Values**:\n   - Identify columns with missing values\n   - Print missing value summary for training dataset features.\n3. **Descriptive Statistics**:\n   - Compute descriptive statistics for numeric features\n\n4. **Kernel Density Estimation**:\n   - Define `get_kde_estimation` function for kernel density estimation.\n   - Compute KDE for numeric data.\n\n5. **Data Visualization Setup**:\n   - Prepare subplots and configurations for data visualization.\n\n6. **Correlation Analysis**:\n   - Compute Pearson correlation coefficients between features.\n   - Identify highly correlated feature pairs\n\n7. **Data Deduplication**:\n   - Check and handle duplicated rows based on specified columns \n\n8. **Data Transformation Pipelines**:\n   - **Casual Preprocessing**:\n     - Implement standard scaling, log, reciprocal, and power transformations\n     - Handle categorical features with imputation and ordinal encoding.\n     - Utilize K-nearest neighbors imputation for missing values.\n\n   - **Preliminary Preprocessing**:\n     - Apply similar transformations as Casual Preprocessing.\n     - Address semi-constant columns with binarization.\n\n9. **Dimensionality Reduction**:\n   - **t-SNE Visualization**:\n     - Generate 2D and 3D t-SNE embeddings for processed data \n10. **Time Series Data Handling**:\n    - Convert datetime values to ordinal values.\n    - Normalize ordinal values and handle NaN values.\n11. **Feature Selection**:\n    - **Mutual Information** and **ANOVA F-test**:\n      - Compute mutual information and F-statistic for feature selection.\n12. **Advanced Data Preprocessing**:\n    - Refine transformations based on feature characteristics (\n13. **Machine Learning Model Training**:\n    - **Ensemble Training**:\n      - Train Voting Classifier ensemble (`VotingClassifier` with `LGBMClassifier`, `XGBClassifier`, `SVC`).\n      - Implement cross-validation strategy (`StratifiedKFold`) with undersampling.\n14. **Model Evaluation and Post-processing**:\n    - Aggregate predictions across bags and folds for averaging.\n    - Apply post-processing to probability predictions \n15. **Submission Preparation**:\n    - Generate submission file (`submission.csv`) with formatted predictions.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-003"]}}], "post_process": []}
{"id": "ml-competition-005", "source": "https://www.kaggle.com/code/markuslill/s3e26-xgbclassifer", "instruction": "This is a dataset for a Patient data for cirrhosis outcomes prediction competition, along with an additional dataset. The descriptions are available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Reading and Preparation:**\n   - Read `train.csv` and `test.csv` into `train` and `test` DataFrames.\n   - Drop `N_Days` column from both `train` and `test`.\n2. **Data Encoding and Scaling:**\n   - **Label Encoding:** Encode 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Drug' columns in `train` and `test` using `LabelEncoder`.\n   - **Standard Scaling:** Scale numerical columns with `StandardScaler`.\n   - **Power Transformation:** Apply power transformation using `PowerTransformer` with 'yeo-johnson' method to the same numerical columns.\n3. **Model Training and Optimization:**\n   - **Random Forest (RF) Classifier:** \n     - Optimize hyperparameters using `GridSearchCV` with parameters for `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n     - Select best model based on grid search results.\n4. **Model Evaluation:**\n   - **XGBoost Classifier:**\n     - Train `XGBClassifier`\n     - Make predictions) on `X_test`.\n\n5. **Final Prediction and Submission:**\n     - Generate predictionsand class probabilities for `test` using trained XGBoost model.\n     - Create `submission` DataFrame with 'Status_C', 'Status_D', 'Status_CL' columns and `id` from `test`.\n     - Save `submission` as `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-005"]}}], "post_process": []}
{"id": "ml-competition-006", "source": "https://www.kaggle.com/code/abhi011097/s3-e5-eda-multi-approach-models-w-o-smote", "instruction": "This is a dataset for a Wine Quality Prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n\n1. **Data Reading and Outlier Detection:**\n   - Read `WineQT.csv`, `train.csv`, and `test.csv`\n   - Detect outliers using quantiles and the 1.5 IQR rule for each dataset \n\n2. **Model Training and Cross-validation:**\n   - Define classifiers (`RandomForestClassifier`, `LGBMClassifier`, `GradientBoostingClassifier`) with default parameters.\n   - Prepare data for `StratifiedKFold` cross-validation (`kf`) with 5 folds.\n   - Conduct cross-validation using Cohen's kappa score  as the metric.\n\n3. **Model Execution and Prediction:**\n   - For each fold in `StratifiedKFold`:\n     - Prepare training and validation datasets.\n     - Train the model \n     - Compute and store Cohen's kappa score for each fold.\n\n4. **Ensemble Prediction and Submission:**\n   - Aggregate test predictions across all folds using mode to create final predictions for each model.\n   - Read `sample_submission.csv` into submission DataFrame\n   - Adjust predicted quality values and save to `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-006"]}}], "post_process": []}
{"id": "ml-competition-008", "source": "https://www.kaggle.com/code/aritra100/notebook-linear-regression-vs-regressor-r2score85", "instruction": "This is a flood prediction competition dataset, with the description available in README.md. As a participant, you need to design a method to predict the probability of flood occurrence in test.csv and write the prediction results into submission.csv, following the format of the sample_submission.csv template.\n\n[Verbose]\n### Detailed Steps from Data Reading to Final Answer Generation\n\n1. **Data Reading and Preparation:**\n   - CSV files `train.csv` and `test.csv` are read into DataFrames `flood_dataset` and `flood_dataset_test_df` respectively.\n   - The column `id` is dropped from `flood_dataset` as it is not needed for modeling.\n\n2. **Data Preprocessing:**\n   - `StandardScaler` is used to standardize the training data (`flood_dataset_train`) and test data (`flood_dataset_test_df`).\n   - Categorical target variable `FloodProbability` in `flood_dataset` is encoded using `LabelEncoder`.\n\n3. **Model Training:**\n   - Logistic Regression (`LogisticRegression`) is trained on the standardized training data (`flood_dataset_train_partially`) and encoded target variable (`dependent_var_transformed_partially`).\n   - Feature importance is extracted from the trained Logistic Regression model.\n\n4. **Stacked Regression Model:**\n   - Multiple regressors (`SGDRegressor`, `BayesianRidge`, `LinearRegression`) are stacked together using `StackingRegressor`, with a final estimator of `RandomForestRegressor`.\n   - The stacked model (`stacked`) is trained on the entire standardized training data (`X_train` and `y_train`).\n\n5. **Test Data Prediction:**\n   - Test data (`flood_dataset_test_df`) is standardized using the previously fitted `StandardScaler`.\n   - Predictions (`testdata_y_pred`) are made using the trained stacked model (`stacked`).\n\n6. **Submission Preparation:**\n   - Predicted values (`testdata_y_pred`) are adjusted (`testdata_y_pred/100`) and saved alongside the corresponding `id` from `flood_dataset_test_df` into a DataFrame `result`.\n   - The final DataFrame `result` is saved to `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-008"]}}], "post_process": []}
{"id": "ml-competition-009", "source": "https://www.kaggle.com/code/rohangulati14/leading-sol-regression-with-an-abalone-dataset", "instruction": "This is a competition based on an abalone dataset, with the description available in README.md. As a participant, you need to design a solution to complete this competition. Write your prediction results into submission.csv following the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Reading and Preparation:**\n   - Read CSV files `abalone.data`, `train.csv`, `test.csv`, and `sample_submission.csv`\n2. **Duplicate Handling:**\n   - Identify and count duplicates\n   - Remove duplicates from train data\n4. **Feature Engineering:**\n   - Encode categorical features using one-hot encoding\n   - Separate target variable from features\n5. **Model Training:**\n   - Initialize `RandomForestRegressor` (`best_forest`) with `random_state=27` and train it \n   - Extract and store feature importances\n6. **Feature Selection:**\n   - Identify highly correlated numeric columns  using Pearson correlation.\n   - Drop columns  based on correlation analysis.\n7. **Train-Validation Split:**\n   - Split data ` into training and validation sets\n8. **Model Stacking:**\n   - Initialize base models (`XGBRegressor`, `LGBMRegressor`, `CatBoostRegressor`, `RandomForestRegressor`) with `random_state=27`.\n   - Define meta-model (`CatBoostRegressor`) with specific hyperparameters.\n   - Train `StackingRegressor` (`stacking_model`) using base models and meta-model \n9. **Prediction and Submission:**\n   - Generate predictions using `stacking_model` \n   - Format results into a DataFrame `submission` with columns `id` and `Rings`.\n   - Save the submission DataFrame as `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-009"]}}], "post_process": []}
{"id": "ml-competition-010", "source": "https://www.kaggle.com/code/iqmansingh/crab-age-voting-regression-synthetic-data", "instruction": "This is a crab age prediction competition, with the description available in README.md. To achieve a higher ranking, an additional dataset of crab ages is provided. As a participant, you need to design and optimize a method to complete this competition. Write the prediction results into submission.csv, following the format of 'sample_submission.csv'.\n\n[Verbose]\n1. **Data Reading and Preparation:**\n   - Read CSV files (`train.csv`, `test.csv`) into DataFrames.\n   - Concatenate additional datasetst to train data\n2. **Data Cleaning and Preprocessing:**\n   - Identify and handle duplicates in `crab_train`.\n   - Replace non-standard values in `Sex` column.\n3. **Feature Engineering:**\n   - Encode categorical feature `Sex` using one-hot encoding.\n   - Handle zero values in numeric columns\n4. **Feature Transformation and Creation:**\n   - Calculate new features for both train and test data\n5. **Ensemble Model Building:**\n   - Create a VotingRegressor with models (`GradientBoostingRegressor`, `XGBRegressor`, `HistGradientBoostingRegressor`, `LGBMRegressor`, `CatBoostRegressor`) weighted for blending predictions.\n6. **Prediction and Submission:**\n   - Generate final predictions using `vrmodel.predict(crab_test)`.\n   - Format predictions into submission DataFrame (`crab_submission` with `id` and `Age`).\n   - Save submission as `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-010"]}}], "post_process": []}
{"id": "ml-competition-013", "source": "https://www.kaggle.com/code/tumpanjawat/s3e19-course-eda-fe-lightgbm", "instruction": "This is a dataset for a sales volume prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.\n\n[Verbose]\n2. **Data Preparation:**\n   - Read `train.csv` and `test.csv` into DataFrames.\n   - Convert `date` column to datetime format.\n   - Extract `Year`, `Month`, `Day`, and `WeekDay` from `date`.\n3. **Holiday Handling:**\n   - Retrieve holidays for specific countries (`AR`, `CA`, `EE`, `JP`, `ES`).\n   - Create `is_holiday` indicator based on holidays.\n4. **Feature Engineering:**\n   - Create additional features such as cyclical encoding for months and days.\n   - Apply one-hot encoding to categorical features for modeling.\n5. **Encoding Holiday Names:**\n   - Encode holiday names using `OrdinalEncoder`.\n   - Handle missing holiday names in the test set.\n6. **Model Training and Prediction:**\n   - Perform k-fold cross-validation (k=5) using `GroupKFold` on `year`.\n   - Train models (`LGBMRegressor`) with scaled features.\n\n7. **Prediction Aggregation and Submission:**\n   - Calculate mean ratios  weighted by specified years.\n   - Adjust predictionsusing computed weights.\n   - Format and save final submission as `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-013"]}}], "post_process": []}
{"id": "ml-competition-017", "source": "https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799", "instruction": "You are participating in an Automated Essay Scoring competition. The dataset and relevant descriptions are provided in the README.md file. As a participant, your task is to generate predictions of data in test.csv and write the results into a submission file named submission.csv in the format specified by sample_submission.csv.\n\n[Verbose]\n1. **Data Loading and Preprocessing:**\n   - Load datasets from CSV files.\n   - Split `full_text` into paragraphs using `\\n\\n` delimiter.\n2. **Paragraph Feature Engineering:**\n   - Explode paragraphs into individual rows.\n   - Preprocess text (lowercase, remove HTML, URLs, digits, special characters).\n   - Compute features: `paragraph_len`, `paragraph_sentence_cnt`, `paragraph_word_cnt`.\n3. **Paragraph Statistical Features:**\n   - Calculate counts of paragraphs based on specified length thresholds.\n   - Aggregate statistics (`max`, `mean`, `min`, `first`, `last`) for paragraph features.\n4. **Sentence Feature Engineering:**\n   - Split `full_text` into sentences based on periods (`.`).\n   - Filter sentences by minimum length.\n   - Compute features: `sentence_len`, `sentence_word_cnt`.\n5. **Sentence Statistical Features:**\n   - Calculate counts of sentences based on specified length thresholds.\n   - Aggregate statistics (`max`, `mean`, `min`, `first`, `last`) for sentence features.\n6. **Word Feature Engineering:**\n   - Split `full_text` into words based on spaces.\n   - Compute features: `word_len`.\n   - Remove zero-length words.\n7. **Word Statistical Features:**\n   - Calculate counts of words based on specified length thresholds.\n   - Compute statistics: `word_len_max`, `word_len_mean`, `word_len_std`, `word_len_q1`, `word_len_q2`, `word_len_q3`.\n8. **TF-IDF Vectorization:**\n   - Use `TfidfVectorizer` to transform `full_text` into TF-IDF features.\n   - Fit on training data and transform both training and test data.\n9. **Model Training:**\n   - Define `qwk_obj` as a custom objective function for LightGBM.\n   - Train LightGBM regressor with 5-fold cross-validation on engineered features.\n10. **Model Prediction:**\n    - Predict scores on test data using trained models.\n    - Average predictions from 5 models and round to integers within [1, 6].\n    - Output predictions to `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-017"]}}], "post_process": []}
{"id": "ml-competition-020", "source": "https://www.kaggle.com/code/hardikgarg03/horse-health-prediction-using-stacked-models", "instruction": "This is a dataset for the Predict Health Outcomes of Horses competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.\n\n[Verbose]\n1. **Data Loading**:\n   - Read training and test data from CSV files (`train.csv`, `test.csv`).\n2. **Handling Missing Values**:\n   - Identify and replace missing values with the most common value per column in both datasets.\n3. **Data Preprocessing**:\n   - **Categorical Encoding**: Convert categorical variables to dummy variables for physical examination columns.\n   - **Binary Encoding**: Convert binary categorical variables to numeric format (`yes`/`no` to `1`/`0`).\n4. **Feature Selection and Preparation**:\n   - **Feature Scaling**: Standardize numerical features using RobustScaler.\n   - **Feature Engineering**: Align and add specific features to ensure consistency between training and test datasets.\n   - **Class Weight Computation**: Calculate class weights for classification balance.\n5. **Model Training**:\n   - **Model Selection**: Train classifiers (`LGBMClassifier`, `XGBClassifier`, `CatBoostClassifier`) with tuned hyperparameters.\n   - **Stacking Classifier**: Combine models using `StackingClassifier` to predict class probabilities.\n6. **Model Evaluation and Submission**:\n   - **Submission Generation**: Predict test dataset outcomes using the stacked model.\n7. **Final Submission**:\n   - Generate predictions in `submission.csv` based on the stacked model.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-020"]}}], "post_process": []}
{"id": "ml-multi-003", "source": "https://www.kaggle.com/code/benvarghese/linkedin-job-posting-machine-learning", "instruction": "This is a job information dataset, with the description available in README.md. You need to predict the experience level of job positions in test.csv based on this dataset and write the results into result.csv, with the column name \"formatted_experience_level\".\n\n[Verbose]\n1. **Data Reading and Preprocessing:**\n   - Read train data and test data\n   - Remove rows with missing `formatted_experience_level`.\n2. **Feature Engineering:**\n   - Calculate text-based features\n3. **Feature Addition:**\n   - Create binary features based on keyword presence in descriptions.\n4. **Data Resampling:**\n   - Use `RandomOverSampler` to handle class imbalance in the training set.\n5. **Feature Scaling:**\n   - Normalize features using `MinMaxScaler` for logistic regression.\n8. **Text Vectorization (Bag of Words and TF-IDF):**\n   - Preprocess text data and vectorize using `CountVectorizer` and `TfidfVectorizer`.\n   - Split vectorized data and handle class imbalance with `RandomOverSampler`.\n9. **Model Training on Text Data:**\n   - Train logistic regression and gradient boosting models on Bag of Words and TF-IDF vectorized data.\n   - Evaluate using 10-fold cross-validation and metrics.\n10. **Output**\n - Make prediction on the test data\n - Write the results into result.csv, with the column name \"formatted_experience_level\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-003"]}}], "post_process": []}
{"id": "ml-multi-008", "source": "https://www.kaggle.com/code/steremma/news-exploration-using-gensim-and-sklearn", "instruction": "This is a News Aggregator Dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named category.csv, with the column name \"CATEGORY\".\n\n[Verbose]\n1. **Reading Data and Dataset Information:**\n   - Read train data and test\n2. **Date Processing:**\n   - Extract minimum and maximum dates from `TIMESTAMP` to determine dataset timeline.\n   - Generate `MONTH` and `DAY` columns based on `TIMESTAMP`.\n 3. **Category Analysis:**\n   - Map category codes (`'b'`, `'t'`, `'e'`, `'m'`) to category names (`'Business'`, `'Science'`, `'Entertainment'`, `'Health'`).\n4. **Publisher Analysis:**\n   - Identify unique publishers and count their contributions.\n \n5. **Text Tokenization and Preprocessing:**\n   - Tokenize titles using NLTK's `word_tokenize`.\n   - Remove stopwords and punctuation, optionally lemmatize tokens.\n   - Unit test the tokenizer with sample text.\n6. **Text Vectorization and Model Training:**\n   - Use `CountVectorizer` with custom tokenizer.\n   - Encode `CATEGORY` labels using `LabelEncoder`.\n   - Train a `RandomForestClassifier` on the training set.\n7. **Save Results:**\n   - Make predictions on the test data\n    - Save results (e.g., `CATEGORY` predictions) to a new CSV file (`category.csv`).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-008"]}}], "post_process": []}
{"id": "ml-multi-009", "source": "https://www.kaggle.com/code/thisishusseinali/malicious-url-detection", "instruction": "This is a Malicious URLs dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named type.csv, with the column name \"type\".\n\n[Verbose]\n\n1. **Data Reading and Initial Processing:**\n   - Read data from CSV file and inspect initial structure.\n   \n2. **Data Preprocessing and Feature Engineering:**\n   - **URL Cleaning:** Remove 'www.' from URLs and calculate URL length.\n   - **URL Classification Label Encoding:** Convert categorical labels to numerical values.\n   - **URL Feature Extraction:** Extract primary domain, count characters, detect URL shortening, abnormal URLs, secure HTTPS usage, and presence of IP addresses.\n   - **Text Analysis of URLs:** Count letters, digits, and special characters.\n 3. **Calculate Class Weight:**\n   - Determine class weights to handle class imbalance.\n4. **Model Training and Evaluation:**\n   - **Feature Selection and Training Data Preparation:** Define features and target for model training.\n   - **Model Selection and Cross-validation:** Train ExtraTreesClassifier using pipelines and cross-validation.\n\n5. **Final Model Application and Output:**\n   - **Prediction and Output:** Use the model to predict on test data.\n   - **Output Results:** Export predictions to a CSV file for final submission.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-009"]}}], "post_process": []}
{"id": "ml-multi-011", "source": "https://www.kaggle.com/code/datatattle/battle-of-ml-classification-models", "instruction": "This is a Coronavirus Tweets NLP - Text Classification dataset, with relevant descriptions provided in the README.md file. Based on the text information in Corona_NLP_test.csv, you need to predict their sentiment (classifying them as \"Positive,\" \"Negative,\" or \"Neutral\") and write the predicted results into a file named sentiment.csv, with the column name \"Sentiment.\"\n\n[Verbose]\n1. **Data Loading**\n   - **Data Files:** Read train data and test data\n     - Define and apply function to map sentiment classes to \"Positive\", \"Negative\", and \"Neutral\" in a new `label` column.\n2. **Text Preprocessing:**\n     - Apply functions to clean text: remove URLs, HTML, lowercase, remove numbers, punctuation, stopwords, mentions, hashtags, and extra spaces.\n3. **Feature Extraction:**\n     - Initialize `TfidfVectorizer` with specific parameters.\n     - Fit and transform the `OriginalTweet` text to create TF-IDF features.\n4. **Model Training and Evaluation:**\n     - Train LinearSVC and Perform 5-fold cross-validation on TF-IDF features and labels.\n5. **Final Model Training and Prediction:**\n     - Predict sentiment labels on the test set.\n6. **Output:***\n     - Save the DataFrame to `sentiment.csv` with only the `Sentiment` column.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-011"]}}], "post_process": []}
{"id": "ml-regression-002", "source": "https://www.kaggle.com/code/nigelclinton/energy-price-prediction-ml", "instruction": "I have a dataset on Spanish electricity and weather, with the description available in README.md. You need to predict the electricity prices in test.csv based on this dataset. Write your prediction results into result.csv, with the column name \"price actual\".\n\n[Verbose]\n1. **Data Loading and Preprocessing:**\n     - Read train data and test data \n     - Compute feature correlations.\n     - Identify and drop columns with zero values.\n      - Drop columns with data leakage or significant null values.\n2. **Modeling Preparation:**\n     - Split DataFrame into target vector (`price_actual`) and feature matrix.\n     - Transform categorical `season` column into numerical values.\n     - Handle missing values with mean imputation.\n3. **Modeling and Prediction:**\n     - Initialize and fit `RandomForestRegressor` model on training data.\n     - Predict `price_actual` for test data.\n     - Create DataFrame with predictions.\n     - Save predictions to `../result.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-002"]}}], "post_process": []}
{"id": "ml-regression-004", "source": "https://www.kaggle.com/code/dima806/spotify-song-ratings-predict-catboost-shap", "instruction": "I currently have a pop music dataset, with the description available in README.md. You need to predict the popularity of the songs in test.csv based on this dataset. Write the prediction results into popularity.csv, with the column name \"Poplarity\".\n\n[Verbose]\n1. **Data Loading and Preprocessing:**\n- Read and clean the training and test datasets by removing duplicates.\n- Filter out records with zero popularity in the training set.\n- Convert track duration from milliseconds to minutes for both datasets.\n- Extract the release year from the album release date for both datasets.\n- Group numerical features into larger bins for simplification.\n- Round loudness and tempo values for the test set.\n- Encode rare categorical labels to handle infrequent categories in specific columns.\n\n2. **Modeling:**\n- Use CatBoostRegressor for prediction.\n- Drop irrelevant columns from both datasets.\n- Define the target variable and features from the training set.\n- Identify categorical features and their indices.\n- Create a training pool with the training data and categorical features.\n- Initialize the CatBoostRegressor with specified parameters.\n- Train the model using the training pool.\n- Predict popularity for the test set.\n- Create a DataFrame with the prediction results.\n- Save the predictions to a CSV file.\n\n3. **Output:**\n- Save the prediction results to a CSV file named 'popularity.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-004"]}}], "post_process": []}
{"id": "ml-regression-006", "source": "https://www.kaggle.com/code/hknaralasetty/lego-regression-55-23-randomforest-mae-e-5", "instruction": "This is a Lego sets and price [1955 - 2023] dataset, with the description available in README.md. You need to predict the star rating for the data in test.csv. Write your prediction results into result.csv, with the column name \"Star rating\".\n\n[Verbose]\n1. **Data Loading and Preprocessing:**\n- Read and load data from training and test datasets.\n- Remove specific columns ('Sets URL' and 'Part URL') from both datasets.\n- Remove rows with missing values in the training dataset.\n- Standardize the 'Star rating' column to a consistent numeric format.\n- Define numeric features and convert them to numeric types.\n- Encode categorical text features using label encoding.\n- Calculate the correlation matrix and filter features with low correlation.\n- Drop duplicates from the filtered training dataset.\n\n2. **Modeling:**\n- Use RandomForestRegressor for prediction.\n- Split the data into training and testing sets.\n- Create a pipeline including feature scaling and the regressor.\n- Train the model using the training data.\n- Predict the target variable for the test data.\n- Create a DataFrame with prediction results and save it.\n\n3. **Output:**\n- Save the prediction results to a CSV file named 'result.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-006"]}}], "post_process": []}
{"id": "ml-regression-008", "source": "https://www.kaggle.com/code/michaelminhpham/predict-unit-sold-mens-women-shoes-on-tiki", "instruction": "This is a dataset of product information from the TIKI e-commerce platform, with the description available in README.md. You need to predict the product sales based on the product information in test.csv. Please note that all items in test.csv are shoes. Write the prediction results into quantity.csv, with the column name \"quantity_sold\"\n\n[Verbose]\n1. **Data Loading and Preprocessing:**\n- Load data from men and women shoes CSV files.\n- Remove the 'Unnamed: 0' column from both datasets.\n- Add a 'category' column to each dataset indicating 'men' or 'women'.\n- Concatenate the datasets into a single DataFrame and reset its index.\n- Remove duplicate rows based on the 'id' column.\n- Clean the 'brand' column by replacing specific text.\n- Create new columns for estimated revenue and discount rate based on price and quantity sold.\n\n2. **Keyword Filtering:**\n- Define a list of keywords for filtering.\n- Convert keywords and relevant columns to ASCII format.\n- Filter rows based on the presence of keywords in the name or description.\n- Remove temporary columns used for filtering.\n\n3. **Data Preparation for Modeling:**\n- Create dummy variables for categorical features.\n- Separate the dataset into features and target variable.\n- Split the data into training and testing sets.\n- Scale the feature values.\n\n4. **Model Training and Evaluation:**\n- Use LazyPredict to initialize and evaluate multiple regression models.\n- Fit the models on the training data and store performance results and predictions.\n\n5. **Output:**\n- Save the model predictions to a CSV file named 'quantity.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-008"]}}], "post_process": []}
{"id": "ml-regression-010", "source": "https://www.kaggle.com/code/renjiabarai/drug-reviews-regression", "instruction": "This is a dataset of comprehensive drug ratings, with the description available in README.md. You need to predict the usefulness of drugs in drugsComTest_raw.csv based on this dataset and write the prediction results into \"Usefulness.csv\", with the column name \"usefulness\".\n\n[Verbose]\n1.**Data Loading:**\n- Load the training data from the specified CSV file.\n2. **Data Cleaning:**\n- Replace missing values in the 'condition' column with 'NOT POINTED'.\n- Determine column types using a specified function.\n- Convert 'drugName' and 'condition' columns to numeric values.\n3. **Feature Engineering:**\n- Replace 'review' column values with the length of each review.\n- Create a 'usefulness' column as the product of 'rating' and 'usefulCount'.\n- Drop 'rating' and 'usefulCount' columns.\n4. **Prepare Training Data:**\n- Extract 'usefulness' as the target variable.\n6. **Test Data Cleaning:**\n- Load the test data from the specified CSV file.\n- Drop unnecessary columns.\n- Replace 'drugName' and 'condition' with corresponding numeric values from the training data.\n- Assign new numeric values to any new categorical values in 'drugName' and 'condition'.\n- Replace 'review' column values with the length of each review.\n- Create a 'usefulness' column as the product of 'rating' and 'usefulCount'.\n7. **Model Training and Prediction:**\n- Train a CatBoostRegressor model on the training data.\n- Predict the target variable on the test data using the trained model.\n- Store the predictions into \"Usefulness.csv\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-010"]}}], "post_process": []}
{"id": "ml-regression-011", "source": "https://www.kaggle.com/code/mehmetisik/u-s-farms-biogas-ml-prediction-livestock", "instruction": "Here is a biogas dataset, with the description available in README.md. You need to predict the Biogas Generation Estimate (cu-ft/day) in test.csv based on this dataset, and write the prediction results into result.csv, with the column name 'biogas_generation_estimate_cuftday'.\n\n[Verbose]\n1.**Data Loading**:\n- Load the dataset from the CSV file into a DataFrame.\n2. **Data Preprocessing**:\n- Identify and categorize columns as categorical, numeric, and cardinal.\n- Handle missing values by identifying columns with missing data and their ratios.\n- Apply one-hot encoding to categorical columns.\n- Exclude the target variable from numeric columns and apply robust scaling to standardize them.\n- Clean and format column names by replacing spaces with underscores, removing non-alphanumeric characters, and converting to lowercase.\n3. **Feature Selection and Target Variable**:\n- Define the target variable.\n- Create the feature set by removing the target variable from the DataFrame.\n4. **Model Training and Evaluation**:\n- Choose LightGBMRegressor\n**Hyperparameter Tuning**:\n- Define hyperparameter grids for the model.\n- Use grid search for hyperparameter tuning.\n- Train the best model from the grid search \n**Final Model**:\n- Make final predictions on the test set using the best model.\n**Results**:\n- Save the prediction results to a CSV file named `result.csv` with the column name 'biogas_generation_estimate_cuftday'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-011"]}}], "post_process": []}
{"id": "ml-regression-012", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of vehicle information in New York, with the description available in README.md. You need to predict the Mileage of vehicles in test.csv based on this dataset and write the results into result.csv, with the column name \"Mileage\".\n\n[Verbose]\n1.**Data Loading**:\n- Load the dataset from the CSV file into a DataFrame.\n2. **Data Cleaning**:\n- Replace missing values\n3. **Feature Encoding**:\n- Encode categorical variables using `LabelEncoder`.\n4. **Regression Models (Regression Task)**:\n- Train GradientBoostingRegressor on train data\n- Predict on the test set for the model\n5. **Writing Predictions to CSV**:\n- Write the predictions to a CSV file named `result.csv` with the column name \"Mileage\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-012"]}}], "post_process": []}
{"id": "ml-regression-014", "source": "https://www.kaggle.com/code/qusaybtoush1990/car-prices-poland", "instruction": "This is a Car Prices Poland dataset, with relevant descriptions provided in the README.md file. Based on the vehicle information in test.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"price\".\n\n[Verbose]\n1.**Load Dataset**:\n- Read the CSV file named \"Car_Prices_Poland_Kaggle.csv\" into a DataFrame.\n\n2. **Convert Categorical Features to Numerical**:\n-  encode the 'mark' and '  'fuel' column to numerical values and store it in a new column 'Mark'.\n3. **Drop Unnecessary Columns**:\n4. **Model Training**:\n- Initialize a `RandomForestRegressor` model.\n- Train the model using the training data\n5. **Write Results**:\n- Write the predicted results into a file named `price.csv` with the column name \"price\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-014"]}}], "post_process": []}
{"id": "ml-regression-015", "source": "https://www.kaggle.com/code/msand1984/appliance-energy-prediction", "instruction": "This is an Appliances Energy Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding appliance energy consumption and write the predicted results into a file named appliance.csv, with the column name \"Appliances\".\n\n[Verbose]\n1. **Load Dataset**:\n- Read train data and test data\n\n2. **Standardization**:\n- Scale the training and testing sets using `StandardScaler` to normalize the numerical features.\n\n2. *Hyperparameter Tuning with GridSearch**:\n- Train and Perform hyperparameter tuning for `ExtraTreesRegressor` using `GridSearchCV` to find the best combination of parameters (`max_depth`, `n_estimators`, `max_features`).\n\n3. **Feature Importance**:\n- Get the sorted list of features based on their importance from the best estimator found in GridSearchCV.\n\n4. **Model Refinement**:\n- Clone the best estimator from GridSearchCV with the optimized parameters.\n- Fit the model on a reduced dataset based on important features.\n\n5. **Output Prediction**:\n- Write the predicted results into a file named \"appliance.csv\", containing predictions for the column \"Appliances\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-015"]}}], "post_process": []}
