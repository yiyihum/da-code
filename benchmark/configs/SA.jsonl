{"id": "data-sa-001", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2000, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided in sample_result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-001"]}}], "post_process": []}
{"id": "data-sa-002", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset. Following the instructions provided in tips.md, complete the hypothesis test for \"The weekday has an impact on the number of electric bicycle rentals\" and write the results into ab_test_results.csv as required.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-002"]}}], "post_process": []}
{"id": "data-sa-003", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset. Following the instructions provided in tips.md, complete the hypothesis test for \"the number of rental bikes in different seasons are similar or different\" and write the corresponding results into kruskal_wallis_results.csv according to the format required.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-003"]}}], "post_process": []}
{"id": "data-sa-004", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset. Following the instructions in tips.md, complete the hypothesis test for \"if the number of bike rentals is similar or different in different weather conditions\" and write the corresponding results into weather.csv according to the required format.\n\n", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-004"]}}], "post_process": []}
{"id": "data-sa-005", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"229ee8\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-005"]}}], "post_process": []}
{"id": "data-sa-006", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"430b63\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-006"]}}], "post_process": []}
{"id": "data-sa-007", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"739bc9\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-007"]}}], "post_process": []}
{"id": "data-sa-008", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"82e2a0\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-008"]}}], "post_process": []}
{"id": "data-sa-009", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"8ee6f3\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-009"]}}], "post_process": []}
{"id": "data-sa-010", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"bedda4\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-010"]}}], "post_process": []}
{"id": "data-sa-011", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"246d26\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-011"]}}], "post_process": []}
{"id": "data-sa-012", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"2fc4ad\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-012"]}}], "post_process": []}
{"id": "data-sa-013", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"5277ed\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-013"]}}], "post_process": []}
{"id": "data-sa-014", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset. You need to retrieve the problem with ID \"d7e9c9\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-014"]}}], "post_process": []}
{"id": "data-sa-015", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, etc.), with the corresponding template already provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-015"]}}], "post_process": []}
{"id": "data-sa-016", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, polynomial_degree_20.csv), with the corresponding template already provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-016"]}}], "post_process": []}
{"id": "data-sa-017", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv), with the corresponding template already provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-017"]}}], "post_process": []}
{"id": "data-sa-018", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, polynomial_degree_20.csv), with the corresponding template already provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-018"]}}], "post_process": []}
{"id": "data-sa-019", "source": "https://www.kaggle.com/code/kanchana1990/calculus-2-maximum-minimum", "instruction": "You have a Calculus - Maximum/Minimum task. You need to analyze a cubic function, which is x^3-6x^2+9x+15. Find the turning points of the function and determine their nature (maximum, minimum, or points of inflection), and write the results in the specified format into turning_points.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-019"]}}], "post_process": []}
{"id": "data-sa-020", "source": "https://www.kaggle.com/code/kanchana1990/calculus-2-maximum-minimum", "instruction": "You have an Optimizing Cylinder Dimensions task. In this task, you are given a fixed surface area S=100 square units of a cylinder, and you need to determine the dimensions of the cylinder (radius r and height h) to maximize its volume.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-020"]}}], "post_process": []}
{"id": "data-sa-021", "source": "https://www.kaggle.com/code/yaserrahmati/inverting-amplifier-circuit", "instruction": "You have an Inverting Amplifier Circuit task. In this task, you need to follow the instructions in tips.md to perform calculations. Given a specific input voltage (vin​=5sin(3t) mV), resistance (R1​=4.7 kΩ and Rf​=47 kΩ), further calculate the input voltage and the output voltage at times 0, 1, 2, 3, 4, and 5 seconds. Write the calculation results into inverting_amplifier_output.csv following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-021"]}}], "post_process": []}
{"id": "data-sa-022", "source": "https://www.kaggle.com/code/yaserrahmati/probability-density-functions", "instruction": "You have a probability calculation task. In this task, you need to, given a specific probability density function  \\( \\frac{1}{\\sqrt{2\\pi }}e^{-\\frac{(x-10)^2}{2}} \\)  calculate the integral \\( \\int_{11}^{12}p(x)dx \\) to determine the probability of a grade between 11 and 12. Write the calculation results into the probability_density_function_result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-022"]}}], "post_process": []}
{"id": "data-sa-023", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/risk-and-return-recap?ex=11", "instruction": "You have a Quantitative Risk Management dataset. Based on the descriptions in epochs_description.txt, you need to calculate and save efficient covariance matrices of a portfolio over different time periods. Please save the results in the format provided by template.csv to \"{epoch}_covariance.csv\", where the epochs are \"before\", \"during\", and \"after\".", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-023"]}}], "post_process": []}
{"id": "data-sa-024", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/risk-and-return-recap?ex=10", "instruction": "You have a Quantitative Risk Management dataset. You need to calculate and save two covariance matrices: the sample covariance matrix and the efficient covariance matrix obtained using the Ledoit-Wolf method. Please save the results in the format provided by template.csv to efficient_covariance_matrix.csv and sample_covariance_matrix.csv, respectively.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-024"]}}], "post_process": []}
{"id": "data-sa-025", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=3", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to calculate key risk metrics for the portfolio losses, which include the mean and standard deviation of the losses, the 95% Value at Risk (VaR), and the Conditional Value at Risk (CVaR) for the worst 5% of cases. It is important to note that daily portfolio losses are calculated by equally weighted. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-025"]}}], "post_process": []}
{"id": "data-sa-026", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to compute the 99% Conditional Value at Risk (CVaR) for a portfolio based on 2008-2009 data. This involves loading and preprocessing the data, calculating daily returns, and computing portfolio losses. You will then fit a Student's t-distribution to these losses using t.fit, determine the 99% Value at Risk (VaR) with t.ppf, and calculate the 99% CVaR using t.expect to integrate tail losses beyond the 99% VaR threshold. Finally, save the results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-026"]}}], "post_process": []}
{"id": "data-sa-027", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=11", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to calculate the optimal portfolio weights that minimize Conditional Value at Risk (CVaR) at a 95% confidence level using historical stock price data. After determining the optimal weights, write the results in the format specified in result.csv", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-027"]}}], "post_process": []}
{"id": "data-sa-028", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=14", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to calculate the Black-Scholes option prices for an IBM stock using historical data. This involves computing the annualized volatility based on the stock's daily returns and then calculating the option price using the Black-Scholes model for the computed volatility and for twice the volatility. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-028"]}}], "post_process": []}
{"id": "data-sa-029", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=16", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to perform delta hedging for an investment portfolio with IBM stock using a European put option following the instructions in step.md. After performing the delta hedging, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-029"]}}], "post_process": []}
{"id": "data-sa-030", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=5", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to estimate the 95% Value at Risk (VaR) for a stock portfolio over two periods (2005-2006 and 2007-2009) by calculating daily returns, computing portfolio returns with equal weights, and determining the 95th percentile of the losses. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-030"]}}], "post_process": []}
{"id": "data-sa-031", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=10", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to perform a Chow test to determine if the 2008 financial crisis caused a significant change in the relationship between mortgage delinquency rates and portfolio returns. After conducting the Chow test, write the results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-031"]}}], "post_process": []}
{"id": "data-sa-032", "source": NaN, "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to calculate and compare the 30-day rolling window volatility of an equally-weighted investment bank portfolio with and without Citibank. This involves computing the daily returns for both portfolios using equal weights for the remaining banks, calculating their volatilities, and combining these volatilities into a single CSV file named result.csv, with the template already provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-032"]}}], "post_process": []}
{"id": "data-sa-033", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/advanced-risk-management?ex=4", "instruction": "You have a GEV risk estimation dataset task. In this task, you need to estimate the Conditional Value at Risk (CVaR) at the 99% confidence level for GE stock. This involves calculating daily losses from historical stock data, determining the maximum weekly losses, fitting a Generalized Extreme Value (GEV) distribution to these maxima, and using it to compute the amount needed in reserve to cover expected maximum weekly losses for a €1,000,000 GE stock holding over January 2010. Finally, save the results in the format specified in result.csv template.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-033"]}}], "post_process": []}
{"id": "data-sa-034", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical Thinking dataset task. In this task, you need to compute the Pearson correlation coefficient between female illiteracy and fertility based on the tips provided in tips.md. After calculating the correlation coefficient, fill in the result in the provided result.csv template.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-034"]}}], "post_process": []}
{"id": "data-sa-035", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to run a linear regression using the Fama-French three-factor model to analyze the returns of a portfolio adjusted for risk (Portfolio_Excess) based on the instructions provided in tips.md. After running the regression, print the regression's adjusted R-squared value and write the results in the format specified in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-035"]}}], "post_process": []}
{"id": "data-sa-036", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset task. In this task, you need to calculate the skewness and fourth moment of the daily stock returns based on the instructions provided in tips.md. After calculating these metrics, fill in the results in the format specified in the result.csv template.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-036"]}}], "post_process": []}
{"id": "data-sa-037", "source": "One tailed z-test | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset. You are required to use a z-test to calculate the p-value for the hypothesis \"the conversion rate of the experimental group is greater than the conversion rate of the control group,\" and then fill in the results in the format specified in the result.csv file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-037"]}}], "post_process": []}
{"id": "data-sa-038", "source": "https://app.datacamp.com/learn/projects/hypothesis_testing_with_mens_and_womens_soccer_matches", "instruction": "You have a Men's and Women's Soccer Matches dataset. In this task, you are required to perform an appropriate hypothesis test to determine the p-value and decide whether to reject or fail to reject the null hypothesis that the mean number of goals scored in women's international soccer matches is the same as men's, using a 10% significance level. Finally, fill in the results in the format specified in the result.csv file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-038"]}}], "post_process": []}
{"id": "data-sa-039", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2002, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-039"]}}], "post_process": []}
{"id": "data-sa-040", "source": "https://app.datacamp.com/learn/projects/1792", "instruction": "This is a Healthcare dataset. In this task, you need to perform a Two-Sample Proportions Z-Test to compare the adverse effects between two treatment groups (Drug and Placebo). The z-test will be conducted using the proportions_ztest function, which compares the proportions of successes (adverse effects in this case) between the two groups. After conducting the test, fill in the results in the format specified in the z_test_results.csv file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-040"]}}], "post_process": []}
{"id": "data-sa-041", "source": "https://app.datacamp.com/learn/projects/1792", "instruction": "This is a Healthcare dataset. You need to use the Mann-Whitney U test to assess whether there is a significant difference in the distribution of ages between two groups (Drug and Placebo). Conduct a two-sided Mann-Whitney U test, extract the p-value, and fill in the results in the format specified in the age_group_effects_p_value.csv file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-041"]}}], "post_process": []}
{"id": "data-sa-042", "source": "https://www.kaggle.com/code/rewidashabaanmohamed/amazon-book-viz", "instruction": "This is an Amazon Books Dataset. Based on the method described in iqr.txt, detect outliers for “Price” and “No. of People rated”. Count the number of outliers and calculate the upper and lower bounds. Record the results in \"result.csv\" following the template provided in sample_result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-042"]}}], "post_process": []}
{"id": "data-sa-043", "source": "https://www.kaggle.com/code/adarsh0063/airlines-review", "instruction": "This is an Airline Reviews Dataset. You need to calculate the correlation matrix between Overall Rating, Seat Comfort, Staff Service, and Food & Beverages using this dataset. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-043"]}}], "post_process": []}
{"id": "data-sa-044", "source": "https://app.datacamp.com/learn/projects/modeling_car_insurance_claim_outcomes", "instruction": "This is a Modeling Car Insurance Claim Outcomes dataset. Your task is to calculate the correlation between other variables (excluding ‘id’) and the ‘outcome’ column. Find the variable with the highest correlation and fill in the corresponding column name into result.csv according to the format provided in result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-044"]}}], "post_process": []}
{"id": "data-sa-045", "source": "Courses - DataCamp Learn", "instruction": "This is a data analysis dataset. It is observed that the beaks of G. scandens birds on Daphne Major appear to have become deeper. Assuming that the means of two groups of data are the same, you need to calculate the probability of observing the difference in means as per the instructions in tips.md. Output the p-value and write the calculated results in the format specified in sample_result.csv into the result.csv file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-045"]}}], "post_process": []}
{"id": "data-sa-046", "source": "Courses - DataCamp Learn", "instruction": "This is a data analysis dataset. You are required to analyze the G. scandens bird beak dataset for Daphne Major in 1975 and 2012 according to the instructions in tips.md. Calculate the mean ratio for each year and compute the 99% confidence interval. Finally, write the results in the format specified in result.csv into this file.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-046"]}}], "post_process": []}
{"id": "data-sa-047", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/advanced-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset. You need to estimate the 99% Conditional Value at Risk (CVaR) for a portfolio using two different distributions: the T distribution and the Gaussian Kernel Density Estimation (KDE). Please fill in the calculation results into the result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-047"]}}], "post_process": []}
{"id": "data-sa-048", "source": "https://projects.datacamp.com/projects/20", "instruction": "You have a dataset titled \"the Discovery of Handwashing,\". You need to perform a Bootstrap analysis and calculate a 95% confidence interval for the reduction of deaths due to handwashing. Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-048"]}}], "post_process": []}
{"id": "data-sa-049", "source": "https://app.datacamp.com/learn/projects/66", "instruction": "You have a dataset titled \"Risk and Returns: The Sharpe Ratio,\". You need to calculate the annualized Sharpe ratio of Amazon and Facebook. The Sharpe ratio measures the extra return per unit of risk. The annual factor is the square root of 252 (approximately the number of trading days per year). Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-049"]}}], "post_process": []}
{"id": "data-sa-050", "source": NaN, "instruction": "You have a dataset titled \"Risk and Returns: The Sharpe Ratio,\". You need to analyze the returns of FAANG stocks and calculate the expected return and Sharpe ratio of an equally weighted portfolio. Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-050"]}}], "post_process": []}
{"id": "data-sa-051", "source": NaN, "instruction": "You need to identify the region and source with the highest CO2 emissions and perform ANOVA tests to compare emissions across different fuel sources within each geographical region. Please fill in the results into the anova_results.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-051"]}}], "post_process": []}
{"id": "data-sa-052", "source": NaN, "instruction": "You need to analyze CO2 emissions from different fuel sources across various geographical regions and output Bonferroni corrected P-values along with the fuel source comparisons. Please fill in the results into the bonferroni_corrected_p_values.csv file following the provided template.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-052"]}}], "post_process": []}
{"id": "data-sa-053", "source": "Courses - DataCamp Learn", "instruction": "You need to compute the heritability for G. scandens and G. fortis. Then, compute a 95% confidence interval using pairs bootstrap. Heritability is defined as the ratio of the covariance of the trait in parents and offspring divided by the variance of the trait in the parents. Acquire 1000 bootstrap replicates of the heritability using pairs bootstrap for G. scandens and G. fortis (set the random seed to 42). Please save the results into the result.csv file following the provided format.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-053"]}}], "post_process": []}
{"id": "data-sa-054", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset. Calculate 10,000 bootstrap replicates of the variance in annual rainfall at the Sheffield Weather Station. Divide the data into 50 bins, compute the bin center and corresponding probability density function (PDF) for each bin. For convenience, convert the variance to units of square centimeters. Save the results to a file named result.csv, following the template provided in sample_result.csv. (Set the random seed to 42)", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-054"]}}], "post_process": []}
{"id": "data-sa-055", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset. The average strike force of Frog B was 0.71 Newtons (N), and that of Frog D was 0.42 N, for a difference of 0.29 N. It is possible that the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. Please use a permutation test with a test statistic of the difference of means to test this hypothesis and print the p-value. (seed=42). Save the results in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-055"]}}], "post_process": []}
{"id": "data-sa-056", "source": "https://campus.datacamp.com/courses/statistical-thinking-in-python-part-2/introduction-to-hypothesis-testing?ex=11", "instruction": "You have a Statistical thinking dataset. Following the instructions in tips.md, determine if Frog D and Frog E have similar impact forces. Compute the corresponding p-value and save the result in the format of sample_result.csv to a file named result.csv. (Set the random seed to 42)", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-056"]}}], "post_process": []}
{"id": "data-sa-057", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset. Following the instructions in tips.md, test the hypothesis that Frog B and Frog D have the same mean impact force, but not necessarily the same distribution. Compute the corresponding p-value and save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-057"]}}], "post_process": []}
{"id": "data-sa-058", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset. Evaluate the hypothesis that the party of a House member has no bearing on his or her vote. Use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. Compute the p-value and save the result in the format of sample_result.csv to a file named result.csv. (Please set the random seed to 42)\n", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-058"]}}], "post_process": []}
{"id": "data-sa-059", "source": "https://campus.datacamp.com/courses/statistical-thinking-in-python-part-2/putting-it-all-together-a-case-study?ex=4", "instruction": "You have a Statistical thinking dataset. Take 10,000 bootstrap replicates of the mean for both the 1975 and 2012 beak depths. Estimate the difference in mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. (Set the random seed to 42). Save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-059"]}}], "post_process": []}
{"id": "data-sa-060", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=9", "instruction": "You have a Quantitative Risk Management dataset. Calculate the sum of squared residuals (SSR) from a regression analysis where quarterly minimum portfolio returns are predicted based on mortgage delinquencies for the period 2005-2010. Save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "level1", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-060"]}}], "post_process": []}
