{"id": "plot-bar-001", "source": "https://www.kaggle.com/code/arnabpml/streaming-shows-exploratory-data-analysis", "instruction": "This is a dataset of TV shows on Netflix, Prime Video, Hulu, and Disney+. The relevant description is in README.md. You are required to identify the top ten artists based on sales from this dataset and plot their sales figures in a bar chart. Save the image as sales.jpg, with the title \"Top Ten Artists Based on Sales,\" a size of (6, 6), y-axis labeled as \"Artist,\" and x-axis labeled as \"Total Sales.\"", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-001"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-002", "source": "https://www.kaggle.com/code/soylevbeytullah/zomato-data-analysis", "instruction": "This is a Zomato dataset, with the relevant description provided in README.md. You are required to draw a stacked bar chart that displays the percentage of restaurants offering online ordering options versus those not offering online ordering options across different rating levels. Save this chart as 'result.jpg'. The title of the image should be \"Percentage of Restaurants' Online Order Option by Rating\", with the xlabel as \"Rating\", and the ylabel as \"Percentage of Online Orders\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-003", "source": "https://www.kaggle.com/code/rewidashabaanmohamed/amazon-book-viz", "instruction": "This is an Amazon Books Dataset, with relevant descriptions in the README.md file. Based on the dataset, your task is to identify the top ten authors with the highest average book prices. Create a bar chart with dimensions of 18 by 12, label the y-axis as 'Author', the x-axis as 'Average Price', and the title as 'Most Expensive Author', and save the chart in 'result.jpg'", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-003"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-004", "source": "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "instruction": "This is the Titanic dataset, with relevant descriptions in the README.md file. Your task is to plot a bar chart showing the number of male and female survivors in each passenger class. The partially completed code is provided in analysis.py. Save the plotted image as 'result.jpg'. The image should have dimensions of 14 by 12, with the title 'Sex vs Pclass Survival Comparison'. The x-axis should be labeled 'Sex' with categories 'Male' and 'Female', and the y-axis should be labeled 'Survived'. Additionally, the bars representing each class (1st, 2nd, and 3rd) should be colored as follows: \"#e9d4d0\" for 1st class, \"a16f8d\" for 2nd class, and \"#2d223a\" for 3rd class.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-005", "source": "https://www.kaggle.com/code/aakashnain/is-it-better-than-2017", "instruction": "This is the 2018 Kaggle Machine Learning & Data Science Survey dataset, with relevant descriptions in the README.md file. Your task is to draw a bar chart showing the number of people in each age group who filled out the survey. The method for dividing age groups is outlined in AgeGroup.md. Save the plotted bar chart as 'result.png'. The title of the chart should be 'Age Group Distribution', with the x-axis labeled as 'Age Group' and the y-axis labeled as 'Count'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-006", "source": "https://www.kaggle.com/code/youssefismail20/international-football-matches-1872-to-2023#Host-Countries-and-Tournaments-üèü%EF%B8%8F", "instruction": "This dataset contains international football results from 1872 to 2024, with relevant descriptions in the README.md file. Based on this dataset, you need to compile the top 10 teams by overall scores from the years 2000 to 2023 and their scores. Then, create a bar chart sorted by total score in descending order. The method to calculate the total score can be found in the 'overall_score.txt' file. The generated image should be saved as 'team.png' with a height and width of 12 and 8, respectively. The bar chart should have the title 'Best Teams from 2000 to 2023', the x-axis title 'Football Team', the x-axis labels should be the names of the teams you filtered, and the y-axis title 'Overall Score'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-007", "source": "https://www.kaggle.com/code/tahaahmedt/seattle-airbnb-listings-eda", "instruction": "This is a Seattle Airbnb Open Data dataset, with relevant descriptions in the README.md file. Based on this dataset and the calculation method recorded in 'NeighborhoodCalculate.txt', create a bar chart showing the distribution of the top 10 neighborhoods by the number of listings. Save the bar chart as 'result.png' with a height and width of 16 and 8, respectively. The chart should have the title \"Top 10 Neighborhoods by Number of Listings\", the x-axis title \"Neighborhood\", with labels for the identified neighborhoods, and the y-axis title \"Number of Listings\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-007"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-008", "source": "https://www.kaggle.com/code/aakashnain/is-it-better-than-2017", "instruction": "This is a 2018 Kaggle Machine Learning & Data Science Survey dataset. Following the instructions in 'Tips.txt', you need to calculate the gender distribution (categorized as Male, Female, Others) for each country and create a bar chart, and save the chart in 'distribution.png'. The chart should have a size of 16 by 8, with the title \"Country-wise Gender Distribution\", the x-axis title \"Country\", and the y-axis title \"Percentage\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-008"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-009", "source": "https://www.kaggle.com/code/prena0808/tokyo-olympics-data-analysis", "instruction": "This is a Tokyo Olympics dataset, with relevant descriptions in the README.md file. You need to identify the top 10 countries with the highest number of coaches and determine the number of gold medals these countries won in the Olympics. Create a bar chart to record the number of coaches and gold medals for these countries, ordered by the number of coaches in descending order, and save the result in 'result.png'. The size of the chart should be (12, 6), with the title \"Number of Coaches and Gold Medals by Country\", the x-axis title \"Countries\", and the y-axis title \"Count\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-009"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-010", "source": "https://www.kaggle.com/code/abdallahwagih/house-price-prediction", "instruction": "This is a House price prediction dataset, with relevant descriptions in the README.md file. Following the instructions in guidance.txt, you need to calculate the mutual information between other variables and the price, and create a bar chart of the results. Save the chart as result.png. The chart should have a height and width of (12, 6), with the title \"Mutual Information Score\", and the variable names labeled on the x-axis.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-010"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-011", "source": "https://www.kaggle.com/code/hamdallak/who-is-killed-by-us-police-why-how-and-where", "instruction": "This is a Fatal Police Shootings in the US dataset, with relevant descriptions in the README.md file. Following the instructions in region.txt, count the number of people killed by police gunfire in the Western, Central, Southern, and Northeastern regions of the United States who showed no signs of mental illness. Then, plot these counts in a bar chart and save the resulting image as 'result.png'. The colors of the bars should be ['#0000FF', '#008000', '#FFA500', '#FF0000'], the title should be \"Number of Victims Killed by Gunshot without Mental Illness by Region\", the x-axis title should be \"Region\", and the y-axis title should be \"Number of Victims\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-011"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-012", "source": "https://www.kaggle.com/code/keremdlkmn/student-alcohol-data-science", "instruction": "This is a Student Alcohol Consumption dataset, with relevant descriptions in the README.md file. Following the instructions in guidance.txt, analyze the living arrangements of teenagers and adult students, specifically whether they live with or apart from their parents, and create a bar chart representing the results. Save the image as \"result.png\". Ensure the chart size is set to (15, 10), with the title \"Where do children between 15 and 22 live\", the x-axis title \"Living Together Sum and Living Apart Sum\", and the y-axis title \"Age\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-012"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-013", "source": "https://www.kaggle.com/code/ehsanmalik/icc-hall-of-fame-dataset-analysis/notebook", "instruction": "This is a dataset related to the ICC Hall of Fame, with relevant descriptions available in the README.md file. Your task is to analyze the data of cricketers who have been inducted into the ICC Hall of Fame and visualize the player roles statistically. Create a bar chart to represent this information and save it as 'result.png'. The horizontal axis should be labeled as 'Player Role' and the vertical axis as 'Count'. The dimensions of the image should be 12 by 6.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-013"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-014", "source": "https://www.kaggle.com/code/swish9/vehicle-safety-analysis/notebook", "instruction": "This is a Synthetic Indian Automobile Crash Data dataset, with relevant descriptions provided in the README.md file. Your task is to analyze the dataset by counting all the features that have missing values and create a bar chart displaying the missing values for each feature in descending order. The figure size should be (8, 6), with the title \"Feature Missing Values Count\", x-axis labeled as \"Features\" with the column names as labels on the x-axis, and y-axis labeled as \"Missing Values Count\". Save the chart as output.png.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-014"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-015", "source": "https://www.kaggle.com/code/csanskriti/amazon-sales-data-analysis", "instruction": "This is an E-Commerce Sales Dataset and Amazon Sales Dataset, with relevant descriptions provided in the README.md file. Your task is to compile the sales data for products of each size sold on Amazon, sort the sizes in ascending order, and plot these figures in a bar chart. The sales should be measured in units of $10,000. Save the resulting image as result.png. The image should have a size of (12, 6), with the title 'Sales by Product Size', x-axis labeled as 'Product Size', and y-axis labeled as 'Net Revenue in 10,000 dollars'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-016", "source": "https://www.kaggle.com/code/youssefismail20/sql-e-commerce", "instruction": "This is an E-commerce dataset by Olist (SQLite), with relevant descriptions provided in the README.md file. Your task is to draw a bar chart showing the top 10 product categories by count and save the resulting plot as 'result.png'. The image should have a size of (12, 8), with the title 'Top 10 Product Categories by Count', x-axis labeled as 'Product Category', and y-axis labeled as 'Count'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-016"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-017", "source": "https://www.kaggle.com/code/bhanupratapbiswas/disclosure-of-electoral-bonds-india/notebook", "instruction": "This is an Electoral Bonds dataset, with relevant descriptions provided in the README.md file. Your task is to plot the total donations to each political party as a bar chart and save the resulting plot as 'party_sums_plot.png'. The image should have a size of (14, 8), with the title 'Total Denominations by Political Party', x-axis labeled as 'Total Denominations', and y-axis labeled as 'Political Party'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-017"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-018", "source": "https://www.kaggle.com/code/romanniki/building-permits-cleaning-eda-prediction", "instruction": "This is a Kickstarter Projects dataset, with relevant descriptions in the README.md file. You need to calculate the average days to issue for each day of the week and plot the results in a bar chart, saving it as 'result.png'. The chart should have a size of (12, 6), with the title 'Average Days to Issue by Weekday', the x-axis titled 'Weekday', and the y-axis titled 'Average Days to Issue'. The color of the bars should be 'skyblue'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-018"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-019", "source": "https://www.kaggle.com/code/victorferino/amazon-sales-clean-eda-sentiment-analysis", "instruction": "This is an Amazon Sales dataset, with relevant descriptions in the README.md file. Following the instructions in guidance.txt, filter out the top 10 sub-categories by quantity and count their numbers. Then, create a bar chart and save it as result.png. The chart should have a width and height of (16, 6), with the title \"Most Amount of Products by Category\", the x-axis titled \"Count\", and the y-axis titled \"Product Sub-Category\". The labels on the y-axis should be the corresponding sub-category names.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-019"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-001", "source": "https://www.kaggle.com/code/povilaskorop/nba-score-growth-and-3-pointers-analysis", "instruction": "This is an NBA Database, with the relevant description provided in README.md. You should create a line graph showing the average NBA game scores and save it as ‚Äòpoint.png‚Äô. The line color should be blue, the title of the image should be ‚ÄúNBA: Average Total Points per Game‚Äù, the x-axis label should be ‚Äúgame_year‚Äù, and the y-axis label should be ‚Äúaverage_points‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-001"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-002", "source": "https://www.kaggle.com/code/ash316/let-s-play-cricket", "instruction": "This is an Indian Premier League (Cricket) dataset, with relevant descriptions in the README.md file. Following the instructions in teamabbreviations.md, filter out the teams that have participated in more than 100 matches and replace their team names with abbreviations. Then, plot a line graph showing their total runs scored in each over and save the graph as result.png. The x-axis title should be ‚Äúover‚Äù and the y-axis title should be ‚Äútotal runs scored‚Äù. Make sure to label each line with the corresponding team abbreviation.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-003", "source": "https://www.kaggle.com/code/shawkyelgendy/fake-news-analysis-and-dashboard/notebook", "instruction": "This is a \"Getting Real about Fake News\" dataset, with relevant descriptions in the README.md file. You are required to separately calculate the spam scores for each day crawled and published, and plot them as a line graph. Save the plotted result as 'output.png'. The dimensions of the plotted image should be (30, 7), with the x-axis labeled as \"day\" and the y-axis labeled as \"spam_score\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-003"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-004", "source": "https://www.kaggle.com/code/youssefismail20/sql-e-commerce", "instruction": "This is an E-commerce dataset by Olist (SQLite), with relevant descriptions provided in the README.md file. Your task is to plot multiple lines based on order_status in chronological order and save the resulting plot as 'result.png'. The image should have a size of (18, 6), with the title \"Trend of Order Statuses Over Time\", x-axis labeled as 'Month', y-axis labeled as 'Order Count', and the line legend titled as 'Order Status'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-005", "source": "https://www.kaggle.com/code/jacopoferretti/world-happiness-score-data-analytics-statistics", "instruction": "This is a World Happiness Report dataset, with relevant descriptions in the README.md file. You need to calculate the average Happiness Score from 2015 to 2019 for the regions mentioned in Region.md, then plot them in a line graph in chronological order and save the image as result.png. The graph should have a width and height of (12, 6), with the title \"Happiness Scores in 2015-2019\", and the labels in the legend should be the abbreviations of the corresponding regions.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-006", "source": "https://www.kaggle.com/code/muzammilbaloch/global-temperature-analysis", "instruction": "This is a Climate Change: Earth Surface Temperature Data dataset, with relevant descriptions in the README.md file. Following the instructions in tips.txt, you need to plot the global seasonal average temperature changes in a line graph and save the image as result.png. The graph should have a width and height of (16, 6), with the title \"Average temperature in each season\", the x-axis titled \"Year\", and the y-axis titled \"Average temperature\". The legend title should be \"Seasons Average Temperature\", with legend labels as \"Spring\", \"Summer\", \"Autumn\", and \"Winter\". The line colors should be set to \"#ffa500\" for Spring, \"#0000ff\" for Summer, \"#008000\" for Autumn, and \"#ff0000\" for Winter.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-007", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, create a line plot showing the average size (MB) of four major game genres over the years (2008-2019). Save the plot as result.png. The plot should have a size of (16, 6), with the title \"Game size changes over 12 years by Genre\". Label the x-axis as \"Year\" and the y-axis as \"Game Size in MB\". Use the colors dark blue, green, red, and orange for the four lines respectively.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-007"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-008", "source": "https://www.kaggle.com/code/justinas/nba-height-and-weight-analysis", "instruction": "This is an NBA Players dataset with a description in README.md. Based on the instructions in BMI.txt, calculate the average BMI of all players for each season and plot this as a line chart. Save the line chart as result.png. The chart should have a size of (16, 8), with the title \"Average BMI Each Season\", the x-axis labeled as \"Season\", and the y-axis labeled as \"BMI\". Use the color dark blue (#17408b) for the line.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-008"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-009", "source": "https://www.kaggle.com/code/waleedfaheem/temperature-analysis-and-visualization", "instruction": "This is a Daily Temperature of Major Cities dataset with a description in README.md. Based on the instructions in tips.txt, plot the average temperatures (in Celsius) for Karachi and Islamabad from 1995 to 2019 as a line chart. Save the chart as result.png. The chart should have a size of (12, 8), with the title \"Average Temperature Comparison between Karachi and Islamabad\", the x-axis labeled as \"Year\", and the y-axis labeled as \"AvgTemperature\". Use green and red for the lines representing Karachi and Islamabad respectively, and include the city names in the legend.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-009"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-010", "source": "Visualizing the History of Nobel Prize Winners - DataCamp Learn", "instruction": "This is a History of Nobel Prize Winners Dataset, with detailed descriptions available in the README.md file. Your task is to calculate the proportion of Nobel Prize winners born in the most common birth country by decade and plot it as a line graph. Save the plot as result.jpg with the image size set to (10,5). Set the title of the plot to ‚ÄúProportion of Nobel Prize Winners Born in the most common birth country of Nobel laureates by Decade‚Äù, the x-axis label to ‚ÄúDecade‚Äù, and the y-axis label to ‚ÄúProportion of USA-born Winners‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-010"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-011", "source": "Visualizing the History of Nobel Prize Winners - DataCamp Learn", "instruction": "This is a dataset on the History of Nobel Prize Winners, with detailed descriptions available in the README.md file. You need to calculate the percentage of Nobel Prize winners of the gender with the most laureates for each decade and category, and then plot this as a line graph. Save the image as result.jpg. The image should be sized (10,6), with the title ‚ÄúProportion of Top Gender Nobel Prize Winners by Decade and Category‚Äù, the x-axis labeled ‚ÄúDecade‚Äù, and the y-axis labeled ‚ÄúPercentage of Top Gender Winners‚Äù. Also, include the category names in the legend.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-011"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-012", "source": "UPI Transactions EDA (TIme series) (kaggle.com)", "instruction": "This is a UPI Transactions Dataset with descriptions in the README.md file. You need to draw a line plot to visualize the average monthly withdrawal and deposit amounts, and save the resulting plot as result.jpg. The title of the line plot should be ‚ÄúAverage Withdrawal and Deposit Amounts per Month‚Äù, the legend labels should be ‚ÄúWithdrawal‚Äù and ‚ÄúDeposit‚Äù, the x-axis title should be ‚ÄúMonth‚Äù, and the y-axis title should be ‚ÄúAverage Amount‚Äù.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-012"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-013", "source": "SQL | Beginner to Advanced with Practical Examples (kaggle.com)", "instruction": "This is a Bike Store Relational Database with descriptions in the README.md file. You need to calculate a 30-day moving average of orders by store, and then visually represent this data using a line plot. Save the resulting plot as result.jpg with a size of (10,4). The title of the plot should be ‚Äú30-Day Moving Average of Orders‚Äù, and the legend title should be ‚Äústore_id‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-013"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-014", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of three different portfolio strategies (default portfolio, equal-weight portfolio, and market value-weighted portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg. The line colors should be distinct, the plot title should be ‚ÄúCumulative Returns Over Time‚Äù, the x-axis title should be ‚ÄúDate‚Äù, and the y-axis title should be ‚ÄúCumulative Returns‚Äù.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-014"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-015", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of four different portfolio strategies (equal-weight portfolio, market value-weighted portfolio, the portfolio with the highest Sharpe ratio, and the global minimum volatility portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg with a size of (12,6). The plot title should be ‚ÄúCumulative Returns Over Time‚Äù, the legend labels should be ‚ÄúCumulative EW‚Äù, ‚ÄúCumulative MCap‚Äù, ‚ÄúCumulative MSR‚Äù, and ‚ÄúCumulative GMV‚Äù, the x-axis title should be ‚ÄúDate‚Äù, and the y-axis title should be ‚ÄúCumulative Returns‚Äù.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-016", "source": "Line Charts (kaggle.com)", "instruction": "This is an Interesting Data to Visualize dataset with descriptions in the README.md file. You need to create a line chart that shows how the number of visitors to Avila Adobe has evolved over time, and save the result to result.png. The image size should be (14,6), the line color should be blue, the image title should be ‚ÄúNumber of visitors to Avila Adobe Museum from 2014 to 2019‚Äù, the x-axis title should be ‚ÄúDate‚Äù, the y-axis title should be ‚ÄúAvila Adobe‚Äù, and the legend label should be ‚ÄúAvila Adobe‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-016"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-001", "source": "https://www.kaggle.com/code/niratpatel/upi-transaction-analysis", "instruction": "This is a UPI Transactions Dataset, with relevant descriptions in README.md. Please analyze the data, count the largest number of transactions in each category, and draw the top four categories in a pie chart. Save the image as answer.png. The length and width of these two pictures are 8 and 6", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-001"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-002", "source": "https://www.kaggle.com/code/ainurrohmanbwx/ipl-analytics/notebook", "instruction": "This is an IPL Complete Dataset (2008-2023) dataset, with relevant descriptions provided in the README.md file. Your task is to plot the distribution of different types of runs (1-6 runs) scored by the cricketer V Kohli in a cricket match. Save the resulting pie chart as 'distribution.png'. The image should have a title \"Distribution of Batsman Runs for V Kohli\" and the legend should be named 'Runs'.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-003", "source": "https://www.kaggle.com/code/niratpatel/upi-transaction-analysis", "instruction": "This is a UPI Transactions dataset, with relevant descriptions in the README.md file. You need to identify the top 4 categories with the highest number of transactions and create a pie chart to display these four categories and their distribution. Save the image as \"result.jpg\", with a size of (8, 6), and the title \"Transaction Distribution by Category\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-003"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-004", "source": "https://www.kaggle.com/code/kanncaa1/does-alcohol-affect-success", "instruction": "This is a Student Alcohol Consumption dataset, with relevant descriptions in the README.md file. You need to create a pie chart showing the weekly alcohol consumption levels (from 2 to 10) of students with their final grades, and save the image as result.png. The colors for the segments should be 'lime', 'blue', 'orange', 'cyan', 'grey', 'purple', 'brown', 'red', and 'darksalmon'. The chart should have the title \"Final Grade\" and the x-axis title \"Students grade distribution according to weekly alcohol consumption\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-005", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, complete the analysis.py script to count the number of games for the four major genres and plot their proportions in a pie chart. Save the chart as result.png. The chart should have a size of (12, 8), with the pie colors set to Green, Orange, Blue, and Red. Include the genre names in the legend.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-006", "source": "https://www.kaggle.com/code/hakeemtfrank/ufo-sightings-data-exploration", "instruction": "This is a UFO Sightings dataset with a description in README.md. Count the number of UFO sightings in 'USA', 'Canada', 'United Kingdom', 'Australia', and 'Germany', and plot their proportions in a bar chart. Save the chart as result.png. Set the bar colors to 'lightblue', 'gold', 'yellowgreen', 'lightcoral', and 'orange' respectively. Title the chart \"UFO Sightings by Country\", label the y-axis as \"Count\", and include the five countries in the legend.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-007", "source": "https://www.kaggle.com/code/shohanursobuj/exploratory-data-analysis-eda-with-reports", "instruction": "This is a Diabetes Health Indicators Dataset with a description in README.md. Identify the CSV file where the target variable has three categories, and count its proportions of 'Healthy', 'Diabetic', and 'Pre-Diabetic' individuals. Plot these proportions in a pie chart and save the chart as \"result.png\". The pie chart should have a size of (8, 8) and use the colors '#1f77b4', '#ff7f0e', and '#2ca02c' respectively. Title the chart \"Proportion of Different Diabetes States\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-007"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-008", "source": "https://www.kaggle.com/code/brunoguedesf/projeto-eda-delivery-center", "instruction": "This is a Delivery Center: Food & Goods orders in Brazil dataset with descriptions in the README.md file. You need to find the hub city with the largest Biker Average Delivery Distance, and plot the percentage of orders delivered by driver modal for that city as a pie chart. Save the resulting image as result.jpg. The pie chart colors should be [‚Äù#ffdb00‚Äù, ‚Äú#ffb600‚Äù], the image size should be (8,8), and the image title should be ‚ÄúPercentage of orders delivered by type of driver in CURITIBA‚Äù", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-008"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-001", "source": "https://drive.google.com/drive/folders/1aj-XnE6UM5-S23oT7qITBuyCIgLOLjsU", "instruction": "This is a Netflix dataset, with relevant descriptions in the README.md file. You need to identify the countries with the most titles in each genre and plot the results into a scatter graph, saving it as 'result.png'. The title of the image should be 'IMDb Score vs Genre', with the x-axis labeled as 'Title' and the y-axis labeled as 'IMDb Score'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-001"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-002", "source": "https://www.kaggle.com/code/fahadrehman07/data-science-job-salary-prediction-glassdoor", "instruction": "This is a Data Science Jobs Salaries Dataset, with relevant descriptions provided in the README.md file. You are tasked with creating a scatter plot to visualize the relationship between observed probabilities and predicted probabilities of football match outcomes, based on parsing a CSV file compressed in gzip format following the instructions in 'guidance.txt'. Please generate the plot and save the result as 'result.png'. The x-axis should be labeled as \"estimated prob\", the y-axis as \"observed prob\", and the scatter plot labels should be \"home victory\", \"draw\", and \"away victory\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-003", "source": "https://www.kaggle.com/code/gpreda/plotly-tutorial-120-years-of-olympic-games", "instruction": "This is a 120 years of Olympic history: athletes and results dataset, with relevant descriptions in the README.md file. You need to plot a scatter plot showing the number of athletes participating in the Olympics each year (x-axis: year, y-axis: number of athletes) and save it as result.png. The plot should have a width and height of (12, 8), with the title \"Athletes per Olympic game\", the x-axis title \"Year\", the scatter points colored in blue, the y-axis title \"Number of athletes\", and the y-axis labels as '0', '2k', '4k', '6k', '8k', '10k', '12k'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-003"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-004", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, create a scatter plot showing the relationship between the time since the release date and the user rating count. Save the plot as result.png. The plot should have a size of (16, 6), with the scatter points colored \"blue\". Label the x-axis as \"Updated version date since release (days)\" and the y-axis as \"User Rating count\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-005", "source": "https://www.kaggle.com/code/hmyleda/steam-game-analysis", "instruction": "This is a Steam Store Games dataset with a description in README.md. Based on the instructions in standard.txt, calculate the Pearson correlation coefficient between the standardized Average Playtime and standardized Positive Ratings. Plot the results in a scatter plot and save the image as result.png. The scatter plot should have a size of (10, 6), with the title \"Correlation Between Average Playtime and Positive Ratings\", the x-axis labeled as \"Average Playtime (Standardized)\", and the y-axis labeled as \"Positive Ratings (Standardized)\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-006", "source": "Amazon book viz (kaggle.com)", "instruction": "This is an Amazon Books Dataset, with detailed descriptions available in the README.md file. You need to complete the given analysis.py to draw a scatter plot of book price versus rating. For aesthetic purposes, plot the log of the price. Save the image as ‚Äòresult.jpg‚Äô. Ensure the image size is set to (12, 8), the scatter plot color is blue, the title is ‚ÄúPrice vs. Rating of Books‚Äù, the y-axis is labeled ‚ÄúPrice‚Äù, and the x-axis is labeled ‚ÄúRating‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-006"]}}], "post_process": ["plot_process"]}
{"id": "dm-csv-001", "source": "https://drive.google.com/drive/folders/1OU_cqpOHpBpXk-BuvOz11GFr96Z7a7gC?usp=sharing", "instruction": "This is a UFC Fighters‚Äô Statistics 2024 dataset with a description in README.md.  You should filter out the best fighters in each UFC weight class and determine how many fighters in each class have never been defeated. Finally, fill in the results into ‚Äòundefeated.csv‚Äô", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-001"]}}], "post_process": []}
{"id": "dm-csv-002", "source": "https://drive.google.com/drive/folders/1sp-WM5vMDhbmtz7I3u_Z7-S9CPuc26KE?usp=share_link", "instruction": "This is a Dropout and Success: Student Data Analysis dataset with a description in README.md. Count the number of students who are ‚ÄúNever Married‚Äù and ‚ÄúPreviously Married‚Äù, and fill the results into result.csv, following the template in sample_result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-002"]}}], "post_process": []}
{"id": "dm-csv-003", "source": "https://drive.google.com/drive/folders/1pnuwL2YklpB3Ulybz9Iq9NUqjsJCdKPI?usp=share_link", "instruction": "This is a Dropout and Success: Student Data Analysis dataset with a description in README.md. Determine the top five most common educational qualifications for both mothers and fathers. Analyze the data to find the frequency of each qualification and save the results in a file named top_qualifications.csv. The file should include the qualification names instead of codes and should be sorted in decreasing order of frequency. Ensure the top_qualifications.csv file has the following column names: ‚ÄúMother‚Äôs Qualification‚Äù, ‚ÄúMother‚Äôs Frequency‚Äù, ‚ÄúFather‚Äôs Qualification‚Äù, ‚ÄúFather‚Äôs Frequency‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-003"]}}], "post_process": []}
{"id": "dm-csv-004", "source": "https://drive.google.com/drive/folders/1YN5lSZInIb75vYQfdEaWKGau-RTCKF_2?usp=share_link", "instruction": "This is an Olympics 124 years Dataset (till 2020) with a description in README.md. You should onduct an analysis to determine the total number of Games held for both the Summer and Winter Olympics, and record this information in ‚ÄúallGames.csv‚Äù following the template in ‚Äúsample_allGames.csv‚Äù. Additionally, for each edition of both the Summer and Winter Olympics, identify the host city and the total number of events conducted, and compile this detailed information in ‚ÄúallEvents.csv‚Äù following the template in ‚Äúsample_allEvents.csv‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-004"]}}], "post_process": []}
{"id": "dm-csv-005", "source": "https://drive.google.com/drive/folders/1XvKXqZNPLXco8efGE33fpMN3zVF9PmZ6?usp=share_link", "instruction": "This is a Book Recommendation Dataset with a description in README.md. Based on the instructions in age.txt, count the number of people in each age group and record the results in result.csv following the template provided in sample_result.csv. Note that if there are any missing values in the Age column, you need to fill them with 30.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-005"]}}], "post_process": []}
{"id": "dm-csv-006", "source": "https://drive.google.com/drive/folders/1jwQT4xLOG4uHCjWSvEqbgAwOan7051qC?usp=share_link", "instruction": "This is a Bike Store Relational Database dataset with a description in README.md. Determine which products are frequently purchased together. Specifically, find the different products that are purchased together in the same order and calculate the number of times they are purchased together. Following the format of sample_result.csv, write the information of the top two pairs with the highest purchase count into result.csv. The column names in result.csv should be ‚Äúproduct_a‚Äù for the name of the first product in the pair, ‚Äúproduct_b‚Äù for the name of the second product in the pair, and ‚Äúco_purchase_count‚Äù for the number of times the two products are purchased together.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-006"]}}], "post_process": []}
{"id": "dm-csv-007", "source": "https://drive.google.com/drive/folders/1wVs85eVx4KYhN0qa-eLjlNEBSLEoJGQc?usp=share_link", "instruction": "This is a TMDB 5000 Movie Dataset with a description in README.md. According to the guidance in ‚ÄòwrFormula.tex‚Äô, you should identify the top 10 most popular movies and record their name in result.csv following the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-007"]}}], "post_process": []}
{"id": "dm-csv-008", "source": "https://drive.google.com/drive/folders/1GpdVwKoDj3voNMSuoxD7RqLJ90xWUyZv?usp=sharing", "instruction": "\nYou have a correlation computation task described in README.d. Your goal is to calculate the correlation coefficient between leverage_ratio and profitability_ratio. Save the results in the format specified in correlation_results.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-008"]}}], "post_process": []}
{"id": "dm-csv-009", "source": "https://drive.google.com/drive/folders/14793kyfMJ5GRbe5djEr0uzvQwXkwr-JM?usp=share_link", "instruction": "This is an Amazon Books Dataset with a description in README.md. Following the guidance provided in guidance.txt, identify the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors. Record the results in author.csv following the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-009"]}}], "post_process": []}
{"id": "dm-csv-010", "source": "https://drive.google.com/drive/folders/1V339AXpqR-f1DZK9wJNd0CN_svQf-I7o?usp=sharing", "instruction": "This is a Bike Store Relational Database, with related descriptions provided in the README.md file. You need to calculate the monthly average sales volume for each bike category and write the results into avg_units_sold.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-010"]}}], "post_process": []}
{"id": "dm-csv-011", "source": "https://drive.google.com/drive/folders/1tO4GVH2DYJJwF2-HmSf3zTEzOUopthwH?usp=sharing", "instruction": "This is a Bike Store Relational Database, with relevant descriptions provided in the README.md file. You need to calculate and aggregate the total quantity sold and total sales revenue for each bike type from 2016 to 2018. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-011"]}}], "post_process": []}
{"id": "dm-csv-012", "source": "https://drive.google.com/drive/folders/1cAh_ZTFINYS6PoEnV6VHfo3_cCS4QAnZ?usp=share_link", "instruction": "This is a Bike Store Relational Database, with relevant descriptions provided in the README.md file. You need to assist in selecting the names of the top five users ranked by total expenditure along with their corresponding total expenditure, and then write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-012"]}}], "post_process": []}
{"id": "dm-csv-013", "source": "https://drive.google.com/drive/folders/1WaS17AYMtntVQPjmg7uEboQJTQLMt8sz?usp=share_link", "instruction": "This is a FIFA 20 complete player dataset with relevant descriptions provided in the README.md file. You need to identify the club with the best average physicality among the top 20 clubs with the highest average scores in FIFA 20. Additionally, according to the description in BMI.txt, you are required to determine the top scorer of the selected club. Record the name of the club and the full name of the player into result.csv following the template provided in sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-013"]}}], "post_process": []}
{"id": "dm-csv-014", "source": "https://drive.google.com/drive/folders/1ffMCL3EzHaFgQhXKSQX_n4uEZDWQiExV?usp=share_link", "instruction": "This is the Amazon Musical Instruments Reviews dataset, with relevant descriptions provided in the README.md file. You need to calculate each review‚Äôs score using the formula designed in Wilson.tex, then extract summaries of the top 3 reviews. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-014"]}}], "post_process": []}
{"id": "dm-csv-015", "source": "https://drive.google.com/drive/folders/1yAz4jIJ4nuBf8qKEvzPYHrzja9RBMdek?usp=share_link", "instruction": "This is a dataset of baseball games, with relevant descriptions provided in the README.md file. You need to help me calculate the player with the most games played, the player with the most runs scored, the player with the most hits, and the player with the most home runs. Each statistical data should include the player‚Äôs name and their corresponding data; for example, for the player with the most games played, record their number of games played, and for the player with the most runs scored, record their runs scored. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-015"]}}], "post_process": []}
{"id": "dm-csv-016", "source": "https://drive.google.com/drive/folders/1Nfx9mAWRZzmY6e3GPrIb2oAo2JSXfsIs?usp=share_link", "instruction": "This is a dataset for football matches, with relevant descriptions provided in the README.md file. I would like you to help me compile statistics for the seasons and the corresponding winning clubs hosted by `IT1` and `BESC`. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-016"]}}], "post_process": []}
{"id": "dm-csv-017", "source": "https://drive.google.com/drive/folders/1f5HSt3QO3WNThqE1QYnuLxFE-0TAQaRC?usp=sharing", "instruction": "You need to follow guidance from data_standard.md to calculate the Net Promoter Score (NPS) for a dataset and write the results in the format specified by result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-017"]}}], "post_process": []}
{"id": "dm-csv-018", "source": "https://drive.google.com/drive/folders/196KJ6-Xt_zpAyvYUjTuB-j7wzYQrDSka?usp=share_link", "instruction": "This is a Data Science Programs list dataset with relevant descriptions provided in the README.md file. You need to identify the top 10 universities offering the most affordable data science programs and record their tuition fees. Finally, write the results into result.csv following the template of sample_result.csv, ordered by fees from lowest to highest.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-018"]}}], "post_process": []}
{"id": "dm-csv-019", "source": "https://drive.google.com/drive/folders/1FBBwFmmHPwIq5CaEasj2ngfjEnIXknEI?usp=share_link", "instruction": "This is a La Liga Complete Dataset with relevant descriptions provided in the README.md file. You need to calculate the top 20 teams by average score and their corresponding number of draws. Finally, write the results into result.csv following the template of sample_result.csv, sorted by the number of draws from highest to lowest.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-019"]}}], "post_process": []}
{"id": "dm-csv-020", "source": "https://drive.google.com/drive/folders/18pL4CXpmnngXz8GYbjog9c1Pkj1kaSRD?usp=share_link", "instruction": "This is a Telco customer churn dataset, with relevant descriptions provided in the README.md file. You need to identify the top 10 cities with the highest telecom customer churn rate and write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-020"]}}], "post_process": []}
{"id": "dm-csv-021", "source": "https://drive.google.com/drive/folders/18pL4CXpmnngXz8GYbjog9c1Pkj1kaSRD?usp=share_link", "instruction": "This is a Marketing Campaigns Dataset, with relevant descriptions provided in the README.md file. You need to identify the top 3 most popular product categories among families with 0, 1, and 2 children, and write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-021"]}}], "post_process": []}
{"id": "dm-csv-022", "source": "https://drive.google.com/drive/folders/1p8b_Tyx_J9g7wpyhn-LlrKSl-pgkk4m6?usp=drive_link", "instruction": "This is a Marketing Campaigns Dataset with relevant descriptions provided in the README.md file. Assuming the current year is 2024, you need to calculate the Mean Campaign Acceptance Rate by Age for each country according to the year ranges specified in result.csv, and fill in the results in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-022"]}}], "post_process": []}
{"id": "dm-csv-023", "source": "https://drive.google.com/drive/folders/1BlxnCEL8hNS5s5TwapGsf_FsF-8i0ca_?usp=share_link", "instruction": "This is a Marketing Campaigns Dataset with relevant descriptions provided in the README.md file. You need to calculate the average number of campaigns it takes for customers with each education level to receive their first offer, and write the results into result.csv following the template of sample_result.csv.\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-023"]}}], "post_process": []}
{"id": "dm-csv-024", "source": "https://drive.google.com/drive/folders/1GDZHUgkCjeXax63vd-fRo6kfo8gUWIri?usp=share_link", "instruction": "This is a Marketing Campaigns Dataset with relevant descriptions provided in the README.md file. Following the instructions in tips.txt, you need to calculate the total number of online purchases and in-store purchases for different income groups, and fill in the results in purchase_by_income.csv. The template for purchase_by_income.csv has been provided.\n", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-024"]}}], "post_process": []}
{"id": "dm-csv-025", "source": "https://drive.google.com/drive/folders/1_ZmzR5S8YNzsl9HhK1YX06y91zoX-xLR?usp=share_link", "instruction": "This is an IPL Complete Dataset (2008-2024) with relevant descriptions provided in the README.md file. Your task is to continue the development of the existing code in analys.py to calculate the winning and losing probabilities for each team according to the instructions in tips.txt. Afterwards, you need to identify the team with the highest probability of winning and the team with the highest probability of losing. Finally, write their names and corresponding (winning or losing) probabilities into team_probabilities.csv following the template provided in sample_result.csv.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-025"]}}], "post_process": []}
{"id": "dm-csv-026", "source": "https://drive.google.com/drive/folders/1MNmqFKeUQR7hpgVxc7pPJOcymQa35K9p?usp=share_link", "instruction": "This is a dataset containing information on FIFA 22 players, with detailed descriptions provided in the README.md file. Your task is to complete the analysis.py script to rank the players based on the specified score at the position level from 2015 to 2022. Relevant hints can be found in some text (txt) and Markdown (md) files. Ensure that the results are written to the result.csv file in the format demonstrated in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-026"]}}], "post_process": []}
{"id": "dm-csv-027", "source": "https://drive.google.com/drive/folders/1bzqJXG2xjnXjbzZdW8UYyH7MYVF0ZHsw?usp=share_link", "instruction": "This is a Netflix Data dataset, with detailed descriptions provided in the README.md file. Your task is to identify the top 10 countries by the number of produced shows and their corresponding counts, as well as the top 10 movie genres by the number of occurrences and their corresponding counts. Record the results in the provided templates: Top_10_countries.csv and Top_10_Movies.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-027"]}}], "post_process": []}
{"id": "dm-csv-028", "source": "https://drive.google.com/drive/u/0/folders/1FVNk4S8BxmjlJWER84a8rCsQP4ZUZj3z", "instruction": "This is a Sales Data dataset, with detailed descriptions provided in the README.md file. Your task is to calculate the total profit for each state by subtracting cost from revenue, sort the profits in descending order, and write the results into the state_profit.csv file using the provided template.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-028"]}}], "post_process": []}
{"id": "dm-csv-029", "source": "https://drive.google.com/drive/folders/1ibfAui8NvdVlZTQGW5MQab57BKPQeAH_?usp=share_link", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to identify the top 3 users with the most pull requests and record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-029"]}}], "post_process": []}
{"id": "dm-csv-030", "source": "https://drive.google.com/drive/folders/1Wyg0lPtEKqh3F_sZOnR7wNZFb2iW7yt8?usp=share_link", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to identify the latest 10 pull requests, merge the related file data, and find the unique files involved in these pull requests. Record the results in the unique_files.csv file using the sample_result.csv template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-030"]}}], "post_process": []}
{"id": "dm-csv-031", "source": "https://drive.google.com/drive/folders/1evC6AsX-2sevVmpbE5nlpqrfO1LZgmUJ", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to identify the users who made the last 6 pull requests for the specific file \"src/compiler/scala/reflect/reify/phases/Calculate.scala\". Record the results in the users_last_6.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-031"]}}], "post_process": []}
{"id": "dm-csv-032", "source": "https://drive.google.com/drive/folders/1y7Z4-O-2b5O4j3a20CnORFZ3dOcn9cq-?usp=share_link", "instruction": "This is a dataset titled ‚ÄúMobile-Games-Testing-with-Cookie-Cats,‚Äù with detailed descriptions provided in the README.md file. Your task is to calculate the proportion of 1-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_by_version.csv file using the provided template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-032"]}}], "post_process": []}
{"id": "dm-csv-033", "source": "https://drive.google.com/drive/folders/13jdtj7v3Ie4Hj9aasZiNjQUc46PE9eH3?usp=sharing", "instruction": "This is a dataset titled ‚ÄúMobile-Games-Testing-with-Cookie-Cats,‚Äù with detailed descriptions provided in the README.md file. Your task is to compare the 1-day retention rates between two versions of a game (gate_30 and gate_40) according to the instructions in guidance.txt. Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 and record the result in the probability_greater_retention.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-033"]}}], "post_process": []}
{"id": "dm-csv-034", "source": "https://drive.google.com/drive/folders/19EUef4q8aewkLlRDndOMM55OPCCBsFmo?usp=share_link", "instruction": "This is a dataset titled ‚ÄúMobile-Games-Testing-with-Cookie-Cats,‚Äù with detailed descriptions provided in the README.md file. Your task is to calculate the proportion of 7-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_7_by_version.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-034"]}}], "post_process": []}
{"id": "dm-csv-035", "source": "https://drive.google.com/drive/folders/17qStaA8VTAsCf50HWVlZT4x-e7nw2haB?usp=share_link", "instruction": "This dataset is titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to convert all time columns to Coordinated Universal Time (UTC) and record each user‚Äôs pulled files based on their pid. If a user pulled multiple files within one pid, you need to record these files separately. Write the results in the output.csv file using the format provided in sample_output.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-035"]}}], "post_process": []}
{"id": "dm-csv-036", "source": "https://drive.google.com/drive/folders/1o7QlFnyB-_RcHE9Kp7Jfa3VPu1Yf72-S?usp=share_link", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to identify the last six users who made pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala`. and fill in their usernames in the template file users_last_6.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-036"]}}], "post_process": []}
{"id": "dm-csv-037", "source": "https://drive.google.com/drive/folders/1G-zP7Vu07sIpX4mw598XqS2v8muQXRtf?usp=share_link", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. Your task is to identify the top three users who made the most pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala` and fill in their usernames in the template file top_3_developers.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-037"]}}], "post_process": []}
{"id": "dm-csv-038", "source": "https://drive.google.com/drive/folders/1X9N6aLhTfXbic4dgXAt0e8G2x8TaTHir?usp=share_link", "instruction": "This is a dataset titled ‚ÄúThe-GitHub-History-of-the-Scala-Language,‚Äù with detailed descriptions provided in the README.md file. I want to know the number of pull requests made by two users with the nicknames ‚Äúsoc‚Äù and ‚Äúxeno-by‚Äù for each year between 2011 and 2016. Please fill in your statistical results in the provided template file pull_requests_by_year_and_author.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-038"]}}], "post_process": []}
{"id": "dm-csv-039", "source": "https://drive.google.com/drive/folders/16Hk8i_1SqBwYWI3sp-hp4rMAgyynqsjk?usp=share_link", "instruction": "This is an Avocado Toast dataset, with detailed descriptions provided in the README.md file. You need to conduct a supply chain analysis of three ingredients used in an avocado toast according to the instructions in tips.md, utilizing the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file using the provided template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-039"]}}], "post_process": []}
{"id": "dm-csv-040", "source": "https://drive.google.com/drive/folders/1VVAuXJEKeeM2MuLAa7Yh68y0JZSjXbhR?usp=sharing", "instruction": "This is a dataset titled ‚ÄúCategorizing salaries,‚Äù with detailed descriptions provided in the README.md file. You need to divide the salary scale into four levels according to the salaries of all employees. For the company size with the largest number of top-level employees, find the designations of the top 10 paid employees and their salaries. Write the results in the result.csv file using the format provided in sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-040"]}}], "post_process": []}
{"id": "dm-csv-041", "source": "https://drive.google.com/drive/folders/1IQSRWG74RhkmgzDmE-bzYNpmTlf6uAnw?usp=drive_link", "instruction": "This is a dataset titled ‚ÄúPolice Activity,‚Äù with detailed descriptions provided in the README.md file. You need to count the stop outcomes for female and male drivers when they are pulled over for speeding, and express them as proportions. Save the results to the stop_outcomes_by_gender.csv file using the provided template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-041"]}}], "post_process": []}
{"id": "dm-csv-042", "source": "https://drive.google.com/drive/folders/1qrL2U5vJWbYCNVeAGMiIcXutZULmQnBU?usp=share_link", "instruction": "This task involves working with a user Segmation dataset. Detailed descriptions are provided in the README.md file. Your objective is to calculate the total number of days between the Invoice date and the Cohort date for each entry in the dataset. Save this calculated difference as CohortIndex in the result.csv file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-042"]}}], "post_process": []}
{"id": "dm-csv-043", "source": "https://drive.google.com/drive/folders/1izUKLoSM66ofPI6CV_l4_gzhM3uIZI5N?usp=share_link", "instruction": "You are provided with a Customer Segmation dataset, and detailed descriptions can be found in the README.md file. Your task is to calculate the retention data for each cohort according to the instructions provided in the tips.txt file. Save the calculated results to a file named retention.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-043"]}}], "post_process": []}
{"id": "dm-csv-044", "source": "https://drive.google.com/drive/folders/1mgjfhA3CF-Ksq7U2WHLCXspVV-RJNWvP?usp=share_link", "instruction": "This is a Customer Segmation dataset, and detailed descriptions are provided in the README.md file. Your task is to calculate the average unit price paid for items by groups of customers over time. Each group should be determined by the month of their first purchase. Save the calculated results in a file named average_price.csv, utilizing the provided template file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-044"]}}], "post_process": []}
{"id": "dm-csv-045", "source": "https://drive.google.com/drive/folders/1rkI23NdN7xI8LPrKfxQkAiIY0EhQIFJA?usp=share_link", "instruction": "This is a customer Segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.txt, you need to use the RFM method to calculate values for each CustomerID in the datasmart.csv for the recent 12 months. Save the results in datasmart.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-045"]}}], "post_process": []}
{"id": "dm-csv-046", "source": "https://drive.google.com/drive/folders/1dyc8eZCeu6zKmv0UPn1pIdPZ66KUll-6?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Your task is to calculate the annualized volatility and annualized variance of Microsoft trading data. Fill in the calculated results in result.csv according to the template provided in sample_result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-046"]}}], "post_process": []}
{"id": "dm-csv-047", "source": "https://drive.google.com/drive/folders/1NwxdeEnmJkl6G0Mx9luiN4eLyKsuholH?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions provided in the README.md file. Your task involves calculating the historical beta of FamaFrenchData using the Capital Asset Pricing Model (CAPM). Write the calculated results into result.csv, with the column named ‚Äúresult‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-047"]}}], "post_process": []}
{"id": "dm-csv-048", "source": "https://drive.google.com/drive/folders/1DrVlEaMwsnu482InXkR_-IjmAwb8BRmH?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, and detailed descriptions are available in the README.md file. Your task is to calculate the drawdown of USO, an ETF that tracks oil prices. Fill in the results into result.csv, as provided in the corresponding file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-048"]}}], "post_process": []}
{"id": "dm-csv-050", "source": "https://drive.google.com/drive/folders/1DQdtNaN-K38D6s5WMpqkGUUW1n9mdkTX?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, you need to calculate the cumulative returns of three different portfolio strategies: the default portfolio, the equal-weight portfolio, and the market value-weighted portfolio. Write the results (rounded to three significant figures) into result.csv according to the template provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-050"]}}], "post_process": []}
{"id": "dm-csv-051", "source": "https://drive.google.com/drive/folders/1wajE6rnIL5gxdtPzPg1DU2qLcSdr2RdV?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions provided in the README.md file. Following the instructions in tips.md, you need to calculate the portfolio volatility of the default portfolio and provide the answer in percentage. Write the result into output.csv, with the column named ‚Äúresult‚Äù.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-051"]}}], "post_process": []}
{"id": "dm-csv-052", "source": "https://drive.google.com/drive/folders/1fLUJMT9WE5_NbHjnlMG_gvgejo8TrhlL?usp=share_link", "instruction": "This is a customer segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, use a 1-4 scale to calculate the RFM Score and complete the segmentation of users (dividing users into three groups based on their RFM Score). Write the segmentation results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-052"]}}], "post_process": []}
{"id": "dm-csv-053", "source": "https://drive.google.com/drive/folders/1FXrR01v8UUqjqSO28ukK6UUkIFDe8Jm-?usp=share_link", "instruction": "This is a customer segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, use a 1-3 scale to calculate the RFM Score. For each CustomerID in RFM_Score.csv, calculate the corresponding user‚Äôs RFM score and fill in the results in RFM_Score.csv (the file format has been provided; please fill in the results accordingly).\n", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-053"]}}], "post_process": []}
{"id": "dm-csv-054", "source": "https://drive.google.com/drive/folders/1uChU1XaP_wqGWPCIYtUF2eUO4y7UMAmc?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, calculate the Sharpe Ratio for each asset, and find the minimum and maximum values of the Sharpe ratios. Write the results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-054"]}}], "post_process": []}
{"id": "dm-csv-055", "source": "https://drive.google.com/drive/folders/1LnrifRqeAvrKIaTQR8HTCN6xuGNImKjz?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Your task is to calculate the cumulative returns of the portfolio (‚ÄúPortfolio‚Äù) and the excess returns over the risk-free rate (‚ÄúPortfolio_Excess‚Äù) of FamaFrenchData over time. After calculating, ensure to fill in your results into result.csv. Please note that the Date column data in result.csv has already been provided.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-055"]}}], "post_process": []}
{"id": "dm-csv-056", "source": "https://drive.google.com/drive/folders/1liDYNKkhlboeM-b4opXRQJfxd842I2Lu?usp=drive_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, select the equal-weighted portfolio and calculate the historical beta of FamaFrenchData. Write the results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-056"]}}], "post_process": []}
{"id": "dm-csv-057", "source": "https://drive.google.com/drive/folders/1jsG1ULrKBLm_CCSpnfAOszZmLJ-yY9-X?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Your task is to calculate a set of summary statistics about the purchase data, broken out by ‚Äòdevice‚Äô (Android or iOS) and ‚Äògender‚Äô (Male or Female). Fill the corresponding blanks in purchase_summary.csv with these summary statistics.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-057"]}}], "post_process": []}
{"id": "dm-csv-058", "source": "https://drive.google.com/drive/folders/1waeq3xZJljGcT0rCM31bPf9bpKZ5b1MM?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Your task is to calculate and print the confidence interval for the difference in conversion rates (lift) between a test group and a control group. Write the results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-058"]}}], "post_process": []}
{"id": "dm-csv-059", "source": "https://drive.google.com/drive/folders/1t1cWSI9zD7hKexuPQtIBxd2fNsXYAQlb?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Based on the registration dates in result.csv, you need to analyze the purchasing behavior of users within the first week of their registration date. Calculate the average number of purchases made in the first week on a daily basis, and then fill the results into result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-059"]}}], "post_process": []}
{"id": "dm-csv-060", "source": "https://drive.google.com/drive/folders/1xUuZ27EPlYFTIxpY4T3IUANcuflqx-4m?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file.  Your task is to calculate the average daily purchases and paywall views for the entire population. Write the results into result.csv according to the format provided in sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-060"]}}], "post_process": []}
{"id": "dm-csv-061", "source": "https://drive.google.com/drive/folders/16BgZCLUksJ1Kg1hV9tOfOr8sRMHevom7?usp=share_link", "instruction": "This is an Airbnb Market Trends dataset, with detailed descriptions available in the README.md file. Your task is to calculate the dates of the earliest and most recent reviews, the numbers of private rooms, and the average listing price. Fill in the results into result.csv according to the template provided in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-061"]}}], "post_process": []}
{"id": "dm-csv-062", "source": "https://drive.google.com/drive/folders/1FzIhPKRfeqQVhfml0DIYldhsngjKtkLh?usp=sharing", "instruction": "You have a dataset documenting the history of handwashing, and you know that people started washing their hands on 1847-06-01. Your task is to calculate the average reduction in the monthly proportion of deaths following the start of handwashing. Save the results in the format specified in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-062"]}}], "post_process": []}
{"id": "dm-csv-063", "source": "https://drive.google.com/drive/folders/1u84GU0Cxb7znHRIGY7_Q2ShT2BUSLiNC?usp=share_link", "instruction": "This is a World‚Äôs Oldest Businesses dataset, with detailed descriptions available in the README.md file. Your task is to find the oldest business on each continent and fill in the relevant information into result.csv according to the fomat specified in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-063"]}}], "post_process": []}
{"id": "dm-csv-064", "source": "https://drive.google.com/drive/folders/1yVJ8DAhrJXrAgDgNO1LEh_IR8VAkKooh?usp=sharing", "instruction": "This is a World‚Äôs Oldest Businesses dataset, with detailed descriptions available in the README.md file. Your task is to identify the restaurants in our dataset that have been around since before the year 1800. Then, fill in their relevant information into result.csv according to the format specified in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-064"]}}], "post_process": []}
{"id": "data-sa-001", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2000, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided in sample_result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-001"]}}], "post_process": []}
{"id": "data-sa-002", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset, with relevant descriptions provided in the README.md file. Following the instructions provided in tips.md, complete the hypothesis test for \"The weekday has an impact on the number of electric bicycle rentals\" and write the results into ab_test_results.csv as required.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-002"]}}], "post_process": []}
{"id": "data-sa-003", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset, with relevant descriptions provided in the README.md file. Following the instructions provided in tips.md, complete the hypothesis test for \"the number of rental bikes in different seasons are similar or different\" and write the corresponding results into kruskal_wallis_results.csv according to the format required.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-003"]}}], "post_process": []}
{"id": "data-sa-004", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, complete the hypothesis test for \"if the number of bike rentals is similar or different in different weather conditions\" and write the corresponding results into weather.csv according to the required format.\n\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-004"]}}], "post_process": []}
{"id": "data-sa-005", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"229ee8\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-005"]}}], "post_process": []}
{"id": "data-sa-006", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"430b63\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-006"]}}], "post_process": []}
{"id": "data-sa-007", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"739bc9\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-007"]}}], "post_process": []}
{"id": "data-sa-008", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"82e2a0\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-008"]}}], "post_process": []}
{"id": "data-sa-009", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"8ee6f3\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-009"]}}], "post_process": []}
{"id": "data-sa-010", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"bedda4\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-010"]}}], "post_process": []}
{"id": "data-sa-011", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"246d26\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-011"]}}], "post_process": []}
{"id": "data-sa-012", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"2fc4ad\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-012"]}}], "post_process": []}
{"id": "data-sa-013", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"5277ed\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-013"]}}], "post_process": []}
{"id": "data-sa-014", "source": "https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/data?select=train.csv", "instruction": "You have an AI Mathematical Olympiad - Progress Prize 1 dataset, with relevant descriptions provided in the README.md file. You need to retrieve the problem with ID \"d7e9c9\" in input_without_answer.csv and devise a method to answer the question corresponding to this ID. Then, write the result into result.csv following the required format in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-014"]}}], "post_process": []}
{"id": "data-sa-015", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, etc.), with the corresponding template already provided.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-015"]}}], "post_process": []}
{"id": "data-sa-016", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, polynomial_degree_20.csv), with the corresponding template already provided.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-016"]}}], "post_process": []}
{"id": "data-sa-017", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv), with the corresponding template already provided.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-017"]}}], "post_process": []}
{"id": "data-sa-018", "source": "https://www.kaggle.com/code/eugenedurymanov/poincare-polynomial-coeffs-matched-with-norm-distr", "instruction": "You have a Growth in finite groups dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, verify that the distribution of its coefficients conforms to a normal distribution up to a certain precision. Write the coefficients and the fitted normal distribution values to a CSV file (named polynomial_degree_10.csv, polynomial_degree_15.csv, polynomial_degree_20.csv), with the corresponding template already provided.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-018"]}}], "post_process": []}
{"id": "data-sa-019", "source": "https://www.kaggle.com/code/kanchana1990/calculus-2-maximum-minimum", "instruction": "You have a Calculus - Maximum/Minimum task, with the description provided in README.md. You need to analyze a cubic function, which is x^3-6x^2+9x+15. Find the turning points of the function and determine their nature (maximum, minimum, or points of inflection), and write the results in the specified format into turning_points.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-019"]}}], "post_process": []}
{"id": "data-sa-020", "source": "https://www.kaggle.com/code/kanchana1990/calculus-2-maximum-minimum", "instruction": "You have an Optimizing Cylinder Dimensions task. For this task, you will read a fixed surface area (S) in square units from a specified file, calculate the cylinder dimensions (radius (r) and height (h)) that maximize its volume based on (S), and then write the optimal dimensions back to the same file.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-020"]}}], "post_process": []}
{"id": "data-sa-021", "source": "https://www.kaggle.com/code/yaserrahmati/inverting-amplifier-circuit", "instruction": "You have an Inverting Amplifier Circuit task, with relevant descriptions provided in the README.md file. In this task, you need to follow the instructions in tips.md to perform calculations. Given a specific input voltage (vin‚Äã=5sin(3t) mV), resistance (R1‚Äã=4.7 kŒ© and Rf‚Äã=47 kŒ©), further calculate the input voltage and the output voltage at times 0, 1, 2, 3, 4, and 5 seconds. Write the calculation results into inverting_amplifier_output.csv following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-021"]}}], "post_process": []}
{"id": "data-sa-022", "source": "https://www.kaggle.com/code/yaserrahmati/probability-density-functions", "instruction": "You have a probability calculation task. In this task, you need to, given a specific probability density function  \\\\( \\\\frac{1}{\\\\sqrt{2\\\\pi }}e^{-\\\\frac{(x-10)^2}{2}} \\\\)  calculate the integral \\\\( \\\\int_{11}^{12}p(x)dx \\\\) to determine the probability of a grade between Number_A and Number_B. The values of Number_A and Number_B are provided in probability_density_function_result.csv file. Write the probability rounded to two decimal places the calculated results into that file.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-022"]}}], "post_process": []}
{"id": "data-sa-023", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/risk-and-return-recap?ex=11", "instruction": "You have a Quantitative Risk Management dataset, with relevant descriptions provided in the README.md file. Based on the descriptions in epochs_description.txt, you need to calculate and save efficient covariance matrices of a portfolio over different time periods. Please save the results in the format provided by template.csv to \"{epoch}_covariance.csv\", where the epochs are \"before\", \"during\", and \"after\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-023"]}}], "post_process": []}
{"id": "data-sa-024", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/risk-and-return-recap?ex=10", "instruction": "You have a Quantitative Risk Management dataset, with relevant descriptions provided in the README.md file. You need to calculate and save two covariance matrices: the sample covariance matrix and the efficient covariance matrix obtained using the Ledoit-Wolf method. Please save the results in the format provided by template.csv to efficient_covariance_matrix.csv and sample_covariance_matrix.csv, respectively.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-024"]}}], "post_process": []}
{"id": "data-sa-025", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=3", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate key risk metrics for the portfolio losses, which include the mean and standard deviation of the losses, the 95% Value at Risk (VaR), and the Conditional Value at Risk (CVaR) for the worst 5% of cases. It is important to note that daily portfolio losses are calculated by equally weighted. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-025"]}}], "post_process": []}
{"id": "data-sa-026", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to compute the 99% Conditional Value at Risk (CVaR) for a portfolio based on 2008-2009 data. This involves loading and preprocessing the data, calculating daily returns, and computing portfolio losses. You will then fit a Student's t-distribution to these losses using t.fit, determine the 99% Value at Risk (VaR) with t.ppf, and calculate the 99% CVaR using t.expect to integrate tail losses beyond the 99% VaR threshold. Finally, save the results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-026"]}}], "post_process": []}
{"id": "data-sa-027", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=11", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate the optimal portfolio weights that minimize Conditional Value at Risk (CVaR) at a 95% confidence level using historical stock price data. After determining the optimal weights, write the results in the format specified in result.csv", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-027"]}}], "post_process": []}
{"id": "data-sa-028", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=14", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate the Black-Scholes option prices for an IBM stock using historical data. This involves computing the annualized volatility based on the stock's daily returns and then calculating the option price using the Black-Scholes model for the computed volatility and for twice the volatility. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-028"]}}], "post_process": []}
{"id": "data-sa-029", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=16", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform delta hedging for an investment portfolio with IBM stock using a European put option following the instructions in step.md. After performing the delta hedging, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-029"]}}], "post_process": []}
{"id": "data-sa-030", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=5", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to estimate the 95% Value at Risk (VaR) for a stock portfolio over two periods (2005-2006 and 2007-2009) by calculating daily returns, computing portfolio returns with equal weights, and determining the 95th percentile of the losses. Finally, fill in the calculated results in the format specified in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-030"]}}], "post_process": []}
{"id": "data-sa-031", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=10", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform a Chow test to determine if the 2008 financial crisis caused a significant change in the relationship between mortgage delinquency rates and portfolio returns. After conducting the Chow test, write the results in the format specified in result.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-031"]}}], "post_process": []}
{"id": "data-sa-032", "source": null, "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate and compare the 30-day rolling window volatility of an equally-weighted investment bank portfolio with and without Citibank. This involves computing the daily returns for both portfolios using equal weights for the remaining banks, calculating their volatilities, and combining these volatilities into a single CSV file named result.csv, with the template already provided.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-032"]}}], "post_process": []}
{"id": "data-sa-033", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/advanced-risk-management?ex=4", "instruction": "You have a GEV risk estimation dataset task, with the description provided in README.md. In this task, you need to estimate the Conditional Value at Risk (CVaR) at the 99% confidence level for GE stock. This involves calculating daily losses from historical stock data, determining the maximum weekly losses, fitting a Generalized Extreme Value (GEV) distribution to these maxima, and using it to compute the amount needed in reserve to cover expected maximum weekly losses for a ‚Ç¨1,000,000 GE stock holding over January 2010. Finally, save the results in the format specified in result.csv template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-033"]}}], "post_process": []}
{"id": "data-sa-034", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical Thinking dataset task, with the description provided in README.md. In this task, you need to compute the Pearson correlation coefficient between female illiteracy and fertility based on the tips provided in tips.md. After calculating the correlation coefficient, fill in the result in the provided result.csv template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-034"]}}], "post_process": []}
{"id": "data-sa-035", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to run a linear regression using the Fama-French three-factor model to analyze the returns of a portfolio adjusted for risk (Portfolio_Excess) based on the instructions provided in tips.md. After running the regression, print the regression's adjusted R-squared value and write the results in the format specified in result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-035"]}}], "post_process": []}
{"id": "data-sa-036", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate the skewness and fourth moment of the daily stock returns based on the instructions provided in tips.md. After calculating these metrics, fill in the results in the format specified in the result.csv template.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-036"]}}], "post_process": []}
{"id": "data-sa-037", "source": "One tailed z-test | Python (datacamp.com)", "instruction": "You have a Quantitative Risk Management dataset, with the relevant description provided in the README.md file. You are required to use a z-test to calculate the p-value for the hypothesis \"the conversion rate of the experimental group is greater than the conversion rate of the control group,\" and then fill in the results in the format specified in the result.csv file.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-037"]}}], "post_process": []}
{"id": "data-sa-038", "source": "https://app.datacamp.com/learn/projects/hypothesis_testing_with_mens_and_womens_soccer_matches", "instruction": "You have a Men's and Women's Soccer Matches dataset, with the relevant description provided in the README.md file. In this task, you are required to perform an appropriate hypothesis test to determine the p-value and decide whether to reject or fail to reject the null hypothesis that the mean number of goals scored in women's international soccer matches is the same as men's, using a 10% significance level. Finally, fill in the results in the format specified in the result.csv file.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-038"]}}], "post_process": []}
{"id": "data-sa-039", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2002, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-039"]}}], "post_process": []}
{"id": "data-sa-040", "source": "https://app.datacamp.com/learn/projects/1792", "instruction": "This is a Healthcare dataset, with the relevant description provided in the README.md file. In this task, you need to perform a Two-Sample Proportions Z-Test to compare the adverse effects between two treatment groups (Drug and Placebo). The z-test will be conducted using the proportions_ztest function, which compares the proportions of successes (adverse effects in this case) between the two groups. After conducting the test, fill in the results in the format specified in the z_test_results.csv file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-040"]}}], "post_process": []}
{"id": "data-sa-041", "source": "https://app.datacamp.com/learn/projects/1792", "instruction": "This is a Healthcare dataset, with the relevant description provided in README.md. You need to use the Mann-Whitney U test to assess whether there is a significant difference in the distribution of ages between two groups (Drug and Placebo). Conduct a two-sided Mann-Whitney U test, extract the p-value, and fill in the results in the format specified in the age_group_effects_p_value.csv file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-041"]}}], "post_process": []}
{"id": "data-sa-042", "source": "https://www.kaggle.com/code/rewidashabaanmohamed/amazon-book-viz", "instruction": "This is an Amazon Books Dataset with a description in README.md. Based on the method described in iqr.txt, detect outliers for ‚ÄúPrice‚Äù and ‚ÄúNo. of People rated‚Äù. Count the number of outliers and calculate the upper and lower bounds. Record the results in \"result.csv\" following the template provided in sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-042"]}}], "post_process": []}
{"id": "data-sa-043", "source": "https://www.kaggle.com/code/adarsh0063/airlines-review", "instruction": "This is an Airline Reviews Dataset, with relevant descriptions provided in the README.md file. You need to calculate the correlation matrix between Overall Rating, Seat Comfort, Staff Service, and Food & Beverages using this dataset. Finally, write the results into result.csv following the template of sample_result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-043"]}}], "post_process": []}
{"id": "data-sa-044", "source": "https://app.datacamp.com/learn/projects/modeling_car_insurance_claim_outcomes", "instruction": "This is a Modeling Car Insurance Claim Outcomes dataset, with detailed descriptions provided in the README.md file. Your task is to calculate the correlation between other variables (excluding ‚Äòid‚Äô) and the ‚Äòoutcome‚Äô column. Find the variable with the highest correlation and fill in the corresponding column name into result.csv according to the format provided in result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-044"]}}], "post_process": []}
{"id": "data-sa-045", "source": "Courses - DataCamp Learn", "instruction": "This is a data analysis dataset, with the relevant description provided in README.md. It is observed that the beaks of G. scandens birds on Daphne Major appear to have become deeper. Assuming that the means of two groups of data are the same, you need to calculate the probability of observing the difference in means as per the instructions in tips.md. Output the p-value and write the calculated results in the format specified in sample_result.csv into the result.csv file.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-045"]}}], "post_process": []}
{"id": "data-sa-046", "source": "Courses - DataCamp Learn", "instruction": "This is a data analysis dataset, with the relevant description provided in README.md. You are required to analyze the G. scandens bird beak dataset for Daphne Major in 1975 and 2012 according to the instructions in tips.md. Calculate the mean ratio for each year and compute the 99% confidence interval. Finally, write the results in the format specified in result.csv into this file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-046"]}}], "post_process": []}
{"id": "data-sa-047", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/advanced-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset, with relevant descriptions provided in the README.md file. You need to estimate the 99% Conditional Value at Risk (CVaR) for a portfolio using two different distributions: the T distribution and the Gaussian Kernel Density Estimation (KDE). Please fill in the calculation results into the result.csv file following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-047"]}}], "post_process": []}
{"id": "data-sa-048", "source": "https://projects.datacamp.com/projects/20", "instruction": "You have a dataset titled \"the Discovery of Handwashing,\" with relevant descriptions provided in the README.md file. You need to perform a Bootstrap analysis and calculate a 95% confidence interval for the reduction of deaths due to handwashing. Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-048"]}}], "post_process": []}
{"id": "data-sa-049", "source": "https://app.datacamp.com/learn/projects/66", "instruction": "You have a dataset titled \"Risk and Returns: The Sharpe Ratio,\" with relevant descriptions provided in the README.md file. You need to calculate the annualized Sharpe ratio of Amazon and Facebook. The Sharpe ratio measures the extra return per unit of risk. The annual factor is the square root of 252 (approximately the number of trading days per year). Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-049"]}}], "post_process": []}
{"id": "data-sa-050", "source": null, "instruction": "You have a dataset titled \"Risk and Returns: The Sharpe Ratio,\" with relevant descriptions provided in the README.md file. You need to analyze the returns of FAANG stocks and calculate the expected return and Sharpe ratio of an equally weighted portfolio. Please fill in the results into the result.csv file following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-050"]}}], "post_process": []}
{"id": "data-sa-051", "source": null, "instruction": "You need to identify the region and source with the highest CO2 emissions and perform ANOVA tests to compare emissions across different fuel sources within each geographical region. Please fill in the results into the anova_results.csv file following the provided format.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-051"]}}], "post_process": []}
{"id": "data-sa-052", "source": null, "instruction": "You need to analyze CO2 emissions from different fuel sources across various geographical regions and output Bonferroni corrected P-values along with the fuel source comparisons. Please fill in the results into the bonferroni_corrected_p_values.csv file following the provided template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-052"]}}], "post_process": []}
{"id": "data-sa-053", "source": "Courses - DataCamp Learn", "instruction": "You need to compute the heritability for G. scandens and G. fortis. Then, compute a 95% confidence interval using pairs bootstrap. Heritability is defined as the ratio of the covariance of the trait in parents and offspring divided by the variance of the trait in the parents. Acquire 1000 bootstrap replicates of the heritability using pairs bootstrap for G. scandens and G. fortis (set the random seed to 42). Please save the results into the result.csv file following the provided format.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-053"]}}], "post_process": []}
{"id": "data-sa-054", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Calculate 10,000 bootstrap replicates of the variance in annual rainfall at the Sheffield Weather Station. Divide the data into 50 bins, compute the bin center and corresponding probability density function (PDF) for each bin. For convenience, convert the variance to units of square centimeters. Save the results to a file named result.csv, following the template provided in sample_result.csv. (Set the random seed to 42)", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-054"]}}], "post_process": []}
{"id": "data-sa-055", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. The average strike force of Frog B was 0.71 Newtons (N), and that of Frog D was 0.42 N, for a difference of 0.29 N. It is possible that the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. Please use a permutation test with a test statistic of the difference of means to test this hypothesis and print the p-value. (seed=42). Save the results in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-055"]}}], "post_process": []}
{"id": "data-sa-056", "source": "https://campus.datacamp.com/courses/statistical-thinking-in-python-part-2/introduction-to-hypothesis-testing?ex=11", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Following the instructions in tips.md, determine if Frog D and Frog E have similar impact forces. Compute the corresponding p-value and save the result in the format of sample_result.csv to a file named result.csv. (Set the random seed to 42)", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-056"]}}], "post_process": []}
{"id": "data-sa-057", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Following the instructions in tips.md, test the hypothesis that Frog B and Frog D have the same mean impact force, but not necessarily the same distribution. Compute the corresponding p-value and save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-057"]}}], "post_process": []}
{"id": "data-sa-058", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Evaluate the hypothesis that the party of a House member has no bearing on his or her vote. Use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. Compute the p-value and save the result in the format of sample_result.csv to a file named result.csv. (Please set the random seed to 42)\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-058"]}}], "post_process": []}
{"id": "data-sa-059", "source": "https://campus.datacamp.com/courses/statistical-thinking-in-python-part-2/putting-it-all-together-a-case-study?ex=4", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Take 10,000 bootstrap replicates of the mean for both the 1975 and 2012 beak depths. Estimate the difference in mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval. (Set the random seed to 42). Save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-059"]}}], "post_process": []}
{"id": "data-sa-060", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=9", "instruction": "You have a Quantitative Risk Management dataset, with the description provided in README.md. Calculate the sum of squared residuals (SSR) from a regression analysis where quarterly minimum portfolio returns are predicted based on mortgage delinquencies for the period 2005-2010. Save the result in the format of sample_result.csv to a file named result.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-060"]}}], "post_process": []}
{"id": "ml-binary-001", "source": "https://www.kaggle.com/code/pratik250/eda-predicting-model#Model-2:-Decision-Tree", "instruction": "I have a dataset for sentiment classification, and the dataset information can be found in the README.md file.  Please help memake predictions on test.csv. Finally, write the predicted character labels (\"Negative\", \"Positive\") into result.csv, following the provided result.csv template.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-001"]}}], "post_process": []}
{"id": "ml-binary-002", "source": "https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction", "instruction": "This is a Health Insurance Cross Sell Prediction dataset, and the relevant description is in README.md. You need to predict whether the users in test.csv will respond, and write the results into submission.csv in the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-002"]}}], "post_process": []}
{"id": "ml-binary-003", "source": "https://www.kaggle.com/code/ranahafezz/twitter-sentiment-analysis-nlp", "instruction": "I have a sentiment classification dataset for statements from various platforms, with the dataset description available in README.md. Please help me predict the sentiment categories for the texts in twitter_validation.csv. Fill your prediction results into prediction.csv, following the provided template.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-003"]}}], "post_process": []}
{"id": "ml-binary-004", "source": "https://www.kaggle.com/code/computervisi/best-models-spaceship-titanic", "instruction": "I have a Spaceship Titanic dataset, with the dataset description available in README.md. You need to predict whether passengers were transported to an alternate dimension after the spaceship collision based on this dataset. The data to be predicted is in test.csv. You need to write the predicted data into submission.csv, following the template provided in sample_submission.csv. Please strictly adhere to the template.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-004"]}}], "post_process": []}
{"id": "ml-binary-005", "source": "https://www.kaggle.com/code/bhatnagardaksh/pca-and-lda-implementation", "instruction": "This is a Breast Cancer Wisconsin (Diagnostic) Data Set, with the dataset description available in README.md. You need to predict the tumor diagnosis result (B or M) based on the given information. The data to be predicted is in test.csv. You need to write the predicted results into label.csv with the column name 'result'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-005"]}}], "post_process": []}
{"id": "ml-binary-006", "source": "https://www.kaggle.com/code/zimhadi1/logestic-reg-svm-naive-bayes-random-forset", "instruction": "I have a dataset containing real and fake news, with the dataset description available in README.md. You need to help me predict the authenticity of the news in validation.csv. Write the prediction results into result.csv with the column name 'result'.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-006"]}}], "post_process": []}
{"id": "ml-binary-007", "source": "https://www.kaggle.com/code/aadyadewangan/churn-predictor", "instruction": "This is a dataset about customer churn, with the dataset description available in README.md. You need to predict whether customers will churn based on this dataset in customer_churn_dataset-testing-master.csv. Write the prediction results into result.csv with the column name 'result'.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-007"]}}], "post_process": []}
{"id": "ml-binary-008", "source": "https://www.kaggle.com/code/yonatanrabinovich/loan-prediction-dataset-ml-project", "instruction": "I have a dataset on loan approval status. You need to predict the loan approval status in test.csv based on this dataset. Write the prediction results (Y or N) into result.csv with the column name 'Loan_Status'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-008"]}}], "post_process": []}
{"id": "ml-binary-009", "source": "https://www.kaggle.com/code/teejmahal20/classification-predicting-customer-satisfaction", "instruction": "This is an airline passenger satisfaction survey dataset, with the description available in README.md. You need to predict the passenger satisfaction in test.csv based on this dataset. Write the prediction results into result.csv with the column name 'satisfaction'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-009"]}}], "post_process": []}
{"id": "ml-binary-010", "source": "https://www.kaggle.com/code/namanmanchanda/heart-attack-eda-prediction-90-accuracy", "instruction": "I have a dataset for predicting heart attack occurrences, with the dataset description available in README.md. You need to predict the likelihood of a heart attack for individuals in test.csv based on this dataset. Please fill in the prediction results in result.csv, following the provided template.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-010"]}}], "post_process": []}
{"id": "ml-binary-011", "source": "https://www.kaggle.com/code/kamaumunyori/20th-century-usa-income-prediction-96-f1-score", "instruction": "I currently have an income prediction dataset, with the description available in README.md. You need to predict whether the individuals in Test.csv have an income above the income limit based on this dataset. Write your prediction results into results.csv, with the column names \"ID\" (user id) and \"income_above_limit\" (your predicted value).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-011"]}}], "post_process": []}
{"id": "ml-binary-012", "source": "https://www.kaggle.com/code/adityarahul/twitter-sentiment-analysis", "instruction": "I have an Emoticon-Based Sentiment in Tweets dataset, with the description available in README.md. You need to predict the sentiment classification of the comments in testdata.manual.2009.06.14.csv based on this dataset (please use characters to represent the sentiment). Write your prediction results into sentiment.csv, with the column name 'emotion'.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-012"]}}], "post_process": []}
{"id": "ml-binary-013", "source": "https://www.kaggle.com/code/lilyhyseni/renewind-energy-tuning-pipelining-ml-models", "instruction": "I have a dataset on wind turbine failures, with the description available in README.md. You need to predict whether the turbine will fail based on the data in Test.csv and write the prediction results into target.csv, with the column name \"Target\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-013"]}}], "post_process": []}
{"id": "ml-binary-014", "source": "https://www.kaggle.com/code/arvindkhoda/classification-for-customer-churn", "instruction": "I have a customer churn prediction dataset, with the description available in README.md. You need to predict whether the users in test.csv will churn based on their data and write the prediction results into churn.csv, with the column name \"Churn\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-014"]}}], "post_process": []}
{"id": "ml-binary-015", "source": "https://www.kaggle.com/code/haliloglumehmet/genetic-variant-classification", "instruction": "This is a Genetic Variant Classifications dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named class.csv, with the column name \"CLASS\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-015"]}}], "post_process": []}
{"id": "ml-binary-016", "source": "https://www.kaggle.com/code/bansodesandeep/credit-card-default-prediction", "instruction": "This is a Credit Card Default dataset, with relevant descriptions provided in the README.md file. Based on the credit card client information in test.csv, you need to predict whether they will default and write the predicted results into a file named defaulter.csv, with the column name \"IsDefaulter\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-016"]}}], "post_process": []}
{"id": "ml-binary-017", "source": "https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial", "instruction": "This is a Rain in Australia dataset, with relevant descriptions provided in the README.md file. Based on the data in test.csv, you need to predict whether it will rain the next day for each row and write the predicted results into a file named tomorrow.csv, with the column name \"RainTomorrow\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-017"]}}], "post_process": []}
{"id": "ml-binary-018", "source": "https://www.kaggle.com/code/bhuvanchennoju/data-storytelling-auc-focus-on-strokes", "instruction": "This is a Stroke Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict whether the individuals will have a stroke and write the predicted results into a file named stroke.csv, with the column name \"stroke.\"", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-018"]}}], "post_process": []}
{"id": "ml-binary-019", "source": "https://www.kaggle.com/code/mig555/mushroom-classification", "instruction": "This is a Mushroom dataset, with relevant descriptions provided in the README.md file. Based on the mushroom information in test.csv, you need to predict whether they are poisonous and write the predicted results into a file named class.csv, with the column name \"class.\"", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-019"]}}], "post_process": []}
{"id": "ml-binary-020", "source": "https://www.kaggle.com/code/tumpanjawat/heart-disease-eda-fe-resam-xgboost", "instruction": "This is a Cardiovascular Diseases Risk Prediction dataset, with relevant descriptions provided in the README.md file. Based on the body information in test.csv, you need to predict whether they have heart disease and write the predicted results into a file named disease.csv, with the column name \"Heart_Disease\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-020"]}}], "post_process": []}
{"id": "ml-binary-021", "source": "https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp", "instruction": "This is a Diabetes Prediction dataset, with relevant descriptions provided in the README.md file. Based on the body information in test.csv, you need to predict whether they have diabetes and write the predicted results into a file named diabetes.csv, with the column name \"diabetes.\"", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-021"]}}], "post_process": []}
{"id": "ml-binary-022", "source": "https://www.kaggle.com/code/marto24/bankruptcy-detection", "instruction": "This is a Company Bankruptcy Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict whether the company will go bankrupt and write the predicted results into a file named bankrupt.csv, with the column name \"Bankrupt?\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-022"]}}], "post_process": []}
{"id": "ml-cluster-001", "source": "https://www.kaggle.com/datasets/sonalisingh1411/mallcustomersdataset", "instruction": "This is a dataset of annual income and spending scores in a mall. You need to perform clustering analysis on the annual income and spending scores based on this dataset. Write the clustering results into cluster.csv, where result.csv consists of 'Feature_i' (the ith value of the processed feature vector) and 'Cluster' column (the clustering label results).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-001"]}}], "post_process": []}
{"id": "ml-cluster-002", "source": "https://www.kaggle.com/code/ayushisharma23/wine-kmeans", "instruction": "I have a dataset of wine chemical components, with the dataset description available in README.md. You need to perform clustering on the data in wine-clustering.csv. Write the clustering results into result.csv, where result.csv consists of 'Feature_i' (the ith value of the processed feature vector) and 'Cluster' column (the clustering label results).", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-002"]}}], "post_process": []}
{"id": "ml-cluster-003", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of car ratings and car feature data, with the description available in README.md. You need to perform a clustering task based on the New York car data and write the clustering results into cluster.csv, with column names \"Feature_i\" (i being the ith value in the feature vector) and \"Cluster\" (the cluster label of the data).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-003"]}}], "post_process": []}
{"id": "ml-cluster-004", "source": "https://www.kaggle.com/code/esraameslamsayed/clustering-for-facebook-interactions", "instruction": "This is a Facebook Live dataset, with the description available in README.md. You need to perform a clustering task on Facebook interactions based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-004"]}}], "post_process": []}
{"id": "ml-cluster-005", "source": "https://www.kaggle.com/code/jingtianji12138/quiz-kmeans-solutions", "instruction": "There is now a clustering task dataset, with the description available in README.md. You need to perform clustering tasks on blob_dataset.csv, and write the clustering results into cluster_blob.csv, respectively. The column names should be \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).\n\n\n\n\n", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-005"]}}], "post_process": []}
{"id": "ml-cluster-006", "source": "https://www.kaggle.com/code/katherineoktaviani/patient-clustering", "instruction": "This is a Patient Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-006"]}}], "post_process": []}
{"id": "ml-cluster-007", "source": "https://www.kaggle.com/code/zabihullah18/clustering-analysis-of-students", "instruction": "This is a Students' Social Network Profile dataset, with the description available in README.md. You need to perform a clustering task based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-007"]}}], "post_process": []}
{"id": "ml-cluster-008", "source": "https://www.kaggle.com/code/pavitsu/eda-and-k-mode-cluster-model-interpretation", "instruction": "This is a dataset of user activity records in a real estate system, with the description available in README.md. You need to perform a clustering task based on property.csv, dividing the data into 4 clusters. Write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-008"]}}], "post_process": []}
{"id": "ml-cluster-009", "source": "https://www.kaggle.com/code/dipansujoshi/global-country-information-clustering", "instruction": "This is a global countries dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into 3 clusters. Write the results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label)", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-009"]}}], "post_process": []}
{"id": "ml-cluster-010", "source": "https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters", "instruction": "This is a Forest Cover Type Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into an appropriate number of clusters. Write the clustering results into cluster.csv, with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-010"]}}], "post_process": []}
{"id": "ml-cluster-011", "source": "https://www.kaggle.com/code/hellbuoy/online-retail-k-means-hierarchical-clustering", "instruction": "This is an Online Retail dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into an appropriate number of clusters. Write the clustering results into cluster.csv, with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from clustering).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-011"]}}], "post_process": []}
{"id": "ml-cluster-012", "source": "https://www.kaggle.com/code/vishakhdapat/customer-segmentation-using-kmeans", "instruction": "This is a Customer Segmentation dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-012"]}}], "post_process": []}
{"id": "ml-cluster-013", "source": "https://www.kaggle.com/code/vipulgohel/clustering-pca", "instruction": "This is a dataset of socioeconomic and health factors for various countries, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-013"]}}], "post_process": []}
{"id": "ml-cluster-014", "source": "https://www.kaggle.com/code/karnikakapoor/customer-segmentation-clustering", "instruction": "This is a dataset of customers' personal and purchase data, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-014"]}}], "post_process": []}
{"id": "ml-cluster-015", "source": "https://www.kaggle.com/code/petebleackley/clustering-proteins", "instruction": "This is a Breast Cancer Proteomes dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to analyze the breast cancer proteome data and perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-015"]}}], "post_process": []}
{"id": "ml-cluster-016", "source": "https://www.kaggle.com/code/khusheekapoor/clustering-using-rfm-analysis-in-python", "instruction": "This is an Online Retail II UCI dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment the customers into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-016"]}}], "post_process": []}
{"id": "ml-cluster-017", "source": "https://www.kaggle.com/code/shawkyelgendy/customer-segmentation-eda-k-means-pca", "instruction": "This is a Bank Customer Segmentation dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment the bank customers into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-017"]}}], "post_process": []}
{"id": "ml-cluster-018", "source": "https://www.kaggle.com/code/mahmoudelfahl/cohort-analysis-customer-segmentation-with-rfm", "instruction": "This is an Online Retail  dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment different types of customer groups into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-018"]}}], "post_process": []}
{"id": "ml-cluster-019", "source": "https://www.kaggle.com/code/pinardogan/rfm-customer-segmentation-using-k-means", "instruction": "This is an Online Retail II Data Set dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment different types of customer groups into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-019"]}}], "post_process": []}
{"id": "ml-cluster-020", "source": "https://www.kaggle.com/code/daniilkrasnoproshin/k-means-simplified-expedition-into-clustering", "instruction": "This is an Iris Flower Dataset, with relevant descriptions provided in the README.md file. You need to perform clustering on these flower samples, selecting an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-020"]}}], "post_process": []}
{"id": "ml-competition-001", "source": "https://www.kaggle.com/code/abdmental01/bank-churn-lightgbm-and-catboost-0-8945", "instruction": "This is a dataset for a Bank customer data for churn prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-001"]}}], "post_process": []}
{"id": "ml-competition-002", "source": "https://www.kaggle.com/code/arunklenin/ps3e24-smoking-cessation-prediction-binary", "instruction": "This is a dataset for a health indicators and smoking status prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-002"]}}], "post_process": []}
{"id": "ml-competition-003", "source": "https://www.kaggle.com/code/tetsutani/icr-iarc-eda-ensemble-and-stacking-baseline", "instruction": "This is a dataset for a health features and medical condition prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-003"]}}], "post_process": []}
{"id": "ml-competition-004", "source": "https://www.kaggle.com/code/lucamassaron/steel-plate-eda-xgboost-is-all-you-need", "instruction": "This is a dataset for a Steel plates data for defect prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-004"]}}], "post_process": []}
{"id": "ml-competition-005", "source": "https://www.kaggle.com/code/markuslill/s3e26-xgbclassifer", "instruction": "This is a dataset for a Patient data for cirrhosis outcomes prediction competition, along with an additional dataset. The descriptions are available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-005"]}}], "post_process": []}
{"id": "ml-competition-006", "source": "https://www.kaggle.com/code/abhi011097/s3-e5-eda-multi-approach-models-w-o-smote", "instruction": "This is a dataset for a Wine Quality Prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-006"]}}], "post_process": []}
{"id": "ml-competition-007", "source": "https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard", "instruction": "You are now participating in a House Prices prediction competition, with the description available in README.md. You need to complete the price prediction and write the prediction results into submission.csv. You can refer to sample_submission.csv for the format of the csv file.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-007"]}}], "post_process": []}
{"id": "ml-competition-008", "source": "https://www.kaggle.com/code/aritra100/notebook-linear-regression-vs-regressor-r2score85", "instruction": "This is a flood prediction competition dataset, with the description available in README.md. As a participant, you need to design a method to predict the probability of flood occurrence in test.csv and write the prediction results into submission.csv, following the format of the sample_submission.csv template.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-008"]}}], "post_process": []}
{"id": "ml-competition-009", "source": "https://www.kaggle.com/code/rohangulati14/leading-sol-regression-with-an-abalone-dataset", "instruction": "This is a competition based on an abalone dataset, with the description available in README.md. As a participant, you need to design a solution to complete this competition. Write your prediction results into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-009"]}}], "post_process": []}
{"id": "ml-competition-010", "source": "https://www.kaggle.com/code/iqmansingh/crab-age-voting-regression-synthetic-data", "instruction": "This is a crab age prediction competition, with the description available in README.md. To achieve a higher ranking, an additional dataset of crab ages is provided. As a participant, you need to design and optimize a method to complete this competition. Write the prediction results into submission.csv, following the format of 'sample_submission.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-010"]}}], "post_process": []}
{"id": "ml-competition-011", "source": "https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e14-2023-first-place-winning-solution", "instruction": "This is a wild blueberry yield prediction competition, with the description available in README.md. Additionally, I have provided an extra dataset. As a participant, you need to design and optimize a method to complete this competition. Write the prediction results into submission.csv, following the format of \"sample_submission.csv\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-011"]}}], "post_process": []}
{"id": "ml-competition-012", "source": "https://www.kaggle.com/code/kacperrabczewski/rwanda-co2-step-by-step-guide", "instruction": "This is a dataset for a CO2 emissions prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-012"]}}], "post_process": []}
{"id": "ml-competition-013", "source": "https://www.kaggle.com/code/tumpanjawat/s3e19-course-eda-fe-lightgbm", "instruction": "This is a dataset for a sales volume prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-013"]}}], "post_process": []}
{"id": "ml-competition-014", "source": "https://www.kaggle.com/code/cdeotte/linear-regression-baseline-lb-1-092", "instruction": "This is a dataset for a U.S. county-level monthly microbusiness data competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-014"]}}], "post_process": []}
{"id": "ml-competition-015", "source": "https://www.kaggle.com/code/hardikgarg03/regression-with-mohs-hardness", "instruction": "This is a dataset for a Mineral properties data for hardness prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\n\n\n\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-015"]}}], "post_process": []}
{"id": "ml-competition-016", "source": "https://www.kaggle.com/code/yantxx/xgboost-binary-classifier-machine-failure", "instruction": "This is a dataset for the Binary Classification of Machine Failures competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-016"]}}], "post_process": []}
{"id": "ml-competition-017", "source": "https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799", "instruction": "You are participating in an Automated Essay Scoring competition. The dataset and relevant descriptions are provided in the README.md file. As a participant, your task is to generate predictions of data in test.csv and write the results into a submission file named submission.csv in the format specified by sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-017"]}}], "post_process": []}
{"id": "ml-competition-018", "source": "https://www.kaggle.com/code/yongsukprasertsuk/ai-generated-text-mod-weight-add-more-data-0-929", "instruction": "This is a dataset for the LLM - Detect AI Generated Text competition, with relevant descriptions provided in the README.md file. To balance the number of categories, an additional dataset has been prepared, with relevant descriptions provided in the External Dataset.md file. As a participant, you need to complete the competition requirements by predicting the results for test_essays.csv and writing them into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-018"]}}], "post_process": []}
{"id": "ml-competition-019", "source": "https://www.kaggle.com/competitions/playground-series-s4e2/data", "instruction": "This is a dataset for the Multi-Class Prediction of Obesity Risk competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-019"]}}], "post_process": []}
{"id": "ml-competition-020", "source": "https://www.kaggle.com/code/hardikgarg03/horse-health-prediction-using-stacked-models", "instruction": "This is a dataset for the Predict Health Outcomes of Horses competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-020"]}}], "post_process": []}
{"id": "ml-multi-001", "source": "https://www.kaggle.com/code/jillanisofttech/human-activity-recognition-with-smartphones-99-acc", "instruction": "I have a human activity classification dataset, with the description available in README.md. You need to predict the activity categories in activity_test.csv based on this dataset. Write your prediction results into activity.csv, with the column name \"Activity\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-001"]}}], "post_process": []}
{"id": "ml-multi-002", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of car ratings and car feature data, with the description available in README.md. You need to predict the status and type of the cars based on the car information in test.csv, and write the prediction results into result.csv with the column name \"new&used\".\n\n\n\n\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-002"]}}], "post_process": []}
{"id": "ml-multi-003", "source": "https://www.kaggle.com/code/benvarghese/linkedin-job-posting-machine-learning", "instruction": "This is a job information dataset, with the description available in README.md. You need to predict the experience level of job positions in test.csv based on this dataset and write the results into result.csv, with the column name \"formatted_experience_level\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-003"]}}], "post_process": []}
{"id": "ml-multi-004", "source": "https://www.kaggle.com/code/wonduk/predict-eda-on-adaptivity-in-online-education", "instruction": "This is a dataset on Students Adaptability Level in Online Education, with relevant descriptions provided in the README.md file. Based on the student information in test.csv, you need to predict their activity level and write the predicted results into a file named level.csv, with the column name \"Adaptivity Level\".\n", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-004"]}}], "post_process": []}
{"id": "ml-multi-005", "source": "https://www.kaggle.com/code/hanyjr/emotion-detection-text-processing-ml", "instruction": "This is an Emotions dataset for NLP, with relevant descriptions provided in the README.md file. Based on the text information in test.txt, you need to predict the corresponding emotions and write the predicted results into a file named emotions.csv, with the column name \"emotion\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-005"]}}], "post_process": []}
{"id": "ml-multi-006", "source": "https://www.kaggle.com/code/redpen12/feature-engineering-catboost", "instruction": "This is a Credit Score Classification dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding credit scores and write the predicted results into a file named score.csv, with the column name \"Credit_Score\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-006"]}}], "post_process": []}
{"id": "ml-multi-007", "source": "https://www.kaggle.com/code/ahmetesencan/multiclass-date-classification-with-some-models", "instruction": "This is a Fruit Classification dataset, with relevant descriptions provided in the README.md file. Based on the fruit information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named class.csv, with the column name \"Class\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-007"]}}], "post_process": []}
{"id": "ml-multi-008", "source": "https://www.kaggle.com/code/steremma/news-exploration-using-gensim-and-sklearn", "instruction": "This is a News Aggregator Dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named category.csv, with the column name \"CATEGORY\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-008"]}}], "post_process": []}
{"id": "ml-multi-009", "source": "https://www.kaggle.com/code/thisishusseinali/malicious-url-detection", "instruction": "This is a Malicious URLs dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named type.csv, with the column name \"type\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-009"]}}], "post_process": []}
{"id": "ml-multi-010", "source": "https://www.kaggle.com/datasets/kukuroo3/body-performance-data/data", "instruction": "This is a Body Performance dataset, with relevant descriptions provided in the README.md file. Based on the data in test.csv, you need to make predictions and write the predicted results into a file named class.csv, with the column name \"class\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-010"]}}], "post_process": []}
{"id": "ml-multi-011", "source": "https://www.kaggle.com/code/datatattle/battle-of-ml-classification-models", "instruction": "This is a Coronavirus Tweets NLP - Text Classification dataset, with relevant descriptions provided in the README.md file. Based on the text information in Corona_NLP_test.csv, you need to predict their sentiment (classifying them as \"Positive,\" \"Negative,\" or \"Neutral\") and write the predicted results into a file named sentiment.csv, with the column name \"Sentiment.\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-011"]}}], "post_process": []}
{"id": "ml-multi-012", "source": "https://www.kaggle.com/code/krishd123/predicting-earthquake-damage-lgbm/notebook#4.-Prediction-on-Test-data", "instruction": "I have a dataset about earthquake-affected building damages. The dataset information can be found in the README.md file. Please help me predict the damage status of the buildings listed in the incomplete.csv file. Save the results to a file named prediction.csv with the following column names: building_id, damage_grade.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-012"]}}], "post_process": []}
{"id": "ml-regression-001", "source": "https://www.kaggle.com/code/varunsaikanuri/flight-fare-prediction-10-ml-models", "instruction": "This is a flight booking information dataset, with the description available in README.md. You need to predict the flight prices in test.csv based on this dataset. Write the prediction results into result.csv with the column name 'price'.", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-001"]}}], "post_process": []}
{"id": "ml-regression-002", "source": "https://www.kaggle.com/code/nigelclinton/energy-price-prediction-ml", "instruction": "I have a dataset on Spanish electricity and weather, with the description available in README.md. You need to predict the electricity prices in test.csv based on this dataset. Write your prediction results into result.csv, with the column name \"price actual\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-002"]}}], "post_process": []}
{"id": "ml-regression-003", "source": "https://www.kaggle.com/code/aslanahmedov/walmart-sales-forecasting", "instruction": "This is a Walmart sales dataset, with the description available in README.md. You need to predict the weekly sales based on this dataset and write the prediction results into submission.csv, with the column name \"Weekly_Sales\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-003"]}}], "post_process": []}
{"id": "ml-regression-004", "source": "https://www.kaggle.com/code/dima806/spotify-song-ratings-predict-catboost-shap", "instruction": "I currently have a pop music dataset, with the description available in README.md. You need to predict the popularity of the songs in test.csv based on this dataset. Write the prediction results into popularity.csv, with the column name \"Poplarity\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-004"]}}], "post_process": []}
{"id": "ml-regression-005", "source": "https://www.kaggle.com/code/minkhantmk19/used-car-prediction", "instruction": "I currently have an income prediction dataset, with the description available in README.md. You need to predict the individual incomes in test.csv based on this dataset. Write your prediction results into price.csv, with the column name \"price\".\n\n\n\n\n", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-005"]}}], "post_process": []}
{"id": "ml-regression-006", "source": "https://www.kaggle.com/code/hknaralasetty/lego-regression-55-23-randomforest-mae-e-5", "instruction": "This is a Lego sets and price [1955 - 2023] dataset, with the description available in README.md. You need to predict the star rating for the data in test.csv. Write your prediction results into result.csv, with the column name \"Star rating\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-006"]}}], "post_process": []}
{"id": "ml-regression-007", "source": "https://www.kaggle.com/code/dima806/job-satisfaction-eurostat-autoviz-catboost-shap", "instruction": "This data is collected from Eurostat, with the description available in README.md. You need to predict the average job satisfaction in test.csv and write your prediction results into job_satisfaction.csv, with the column name \"AVG_JOB_SATISFACTION\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-007"]}}], "post_process": []}
{"id": "ml-regression-008", "source": "https://www.kaggle.com/code/michaelminhpham/predict-unit-sold-mens-women-shoes-on-tiki", "instruction": "This is a dataset of product information from the TIKI e-commerce platform, with the description available in README.md. You need to predict the product sales based on the product information in test.csv. Please note that all items in test.csv are shoes. Write the prediction results into quantity.csv, with the column name \"quantity_sold\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-008"]}}], "post_process": []}
{"id": "ml-regression-009", "source": "https://www.kaggle.com/code/renjiabarai/barcelona-airbnb-regression", "instruction": " have an Airbnb listing dataset for Barcelona, with the description available in README.md. You need to predict the prices of the listings in test.csv based on their information. Write the prediction results into price.csv, with the column name \"price\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-009"]}}], "post_process": []}
{"id": "ml-regression-010", "source": "https://www.kaggle.com/code/renjiabarai/drug-reviews-regression", "instruction": "This is a dataset of comprehensive drug ratings, with the description available in README.md. You need to predict the usefulness of drugs in drugsComTest_raw.csv based on this dataset and write the prediction results into \"Usefulness.csv\", with the column name \"usefulness\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-010"]}}], "post_process": []}
{"id": "ml-regression-011", "source": "https://www.kaggle.com/code/mehmetisik/u-s-farms-biogas-ml-prediction-livestock", "instruction": "Here is a biogas dataset, with the description available in README.md. You need to predict the Biogas Generation Estimate (cu-ft/day) in test.csv based on this dataset, and write the prediction results into result.csv, with the column name 'biogas_generation_estimate_cuftday'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-011"]}}], "post_process": []}
{"id": "ml-regression-012", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of vehicle information in New York, with the description available in README.md. You need to predict the Mileage of vehicles in test.csv based on this dataset and write the results into result.csv, with the column name \"Mileage\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-012"]}}], "post_process": []}
{"id": "ml-regression-013", "source": "https://www.kaggle.com/code/farzadnekouei/polynomial-regression-regularization-assumptions", "instruction": "This is a Vehicle dataset, with relevant descriptions provided in the README.md file. Based on the vehicle data in test.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"Selling_Price\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-013"]}}], "post_process": []}
{"id": "ml-regression-014", "source": "https://www.kaggle.com/code/qusaybtoush1990/car-prices-poland", "instruction": "This is a Car Prices Poland dataset, with relevant descriptions provided in the README.md file. Based on the vehicle information in test.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"price\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-014"]}}], "post_process": []}
{"id": "ml-regression-015", "source": "https://www.kaggle.com/code/msand1984/appliance-energy-prediction", "instruction": "This is an Appliances Energy Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding appliance energy consumption and write the predicted results into a file named appliance.csv, with the column name \"Appliances\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-015"]}}], "post_process": []}
{"id": "ml-regression-016", "source": "https://www.kaggle.com/code/arnabchaki/flight-fare-prediction-0-96-r2-score", "instruction": "This is an Airfare ML: Predicting Flight Fares dataset, with relevant descriptions provided in the README.md file. Based on the flight information in test.csv, you need to predict the corresponding fares and write the predicted results into a file named fare.csv, with the column name \"Fare\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-016"]}}], "post_process": []}
{"id": "ml-regression-017", "source": "https://www.kaggle.com/code/alperkaraca1/steam-score-prediction", "instruction": "This is a Steam Releases dataset, with relevant descriptions provided in the README.md file. Based on the game information in test.csv, you need to predict the corresponding ratings and write the predicted results into a file named rating.csv, with the column name \"rating\".", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-017"]}}], "post_process": []}
{"id": "ml-regression-018", "source": "https://www.kaggle.com/code/anoopashware/food-demand-forecasting-predict-orders", "instruction": "This is a Food Demand Forecasting dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding orders and write the predicted results into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-018"]}}], "post_process": []}
{"id": "ml-regression-019", "source": "https://www.kaggle.com/code/wkirgsn/eda-and-baseline-on-motor-temperature-estimation", "instruction": "This is an Electric Motor Temperature dataset, with relevant descriptions provided in the README.md file. Based on the sensor data in test.csv, you need to predict the rotor temperature and write the predicted results into a file named pm.csv, with the column name \"pm\".", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-019"]}}], "post_process": []}
{"id": "ml-regression-020", "source": "https://www.kaggle.com/code/saloni1712/deliverontime-food-delivery-duration-predictor", "instruction": "This is a Food Delivery Dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the required delivery time and write the predicted results into submission.csv following the format of sample_submission.csv.", "hints": null, "hardness": "Medium", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-020"]}}], "post_process": []}
{"id": "ml-regression-021", "source": "https://www.kaggle.com/code/iabhishekmaurya/used-car-price-prediction", "instruction": "This is a Used Cars Price Prediction dataset, with relevant descriptions provided in the README.md file. Based on the vehicle information in test-data.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"price.\"", "hints": null, "hardness": "Easy", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-021"]}}], "post_process": []}
