{"id": "dm-csv-007", "source": "https://drive.google.com/drive/folders/1wVs85eVx4KYhN0qa-eLjlNEBSLEoJGQc?usp=share_link", "instruction": "This is a TMDB 5000 Movie Dataset with a description in README.md. According to the guidance in ‘wrFormula.tex’, you should identify the top 10 most popular movies and record their name in result.csv following the template provided in sample_result.csv.\n\nSteps for Reference:\n\n1. **Data Loading**:\n   - Use the `pandas` library to load two CSV files: one containing movie details (`tmdb_5000_movies.csv`) and another containing movie credits (`tmdb_5000_credits.csv`).\n\n2. **Data Merging**:\n   - Merge the two datasets based on the movie ID (`id` and `movie_id`) to combine movie details with credits.\n\n3. **Weighted Rating Calculation**:\n   - Define the `weightedRating` function to calculate the weighted rating for each movie using the formula:\n     \\[\n     \\text{Weighted Rating} = \\frac{(v \\times R) + (m \\times C)}{v + m}\n     \\]\n     where `v` is the number of votes, `R` is the average rating, `m` is the vote count threshold (percentile), and `C` is the mean vote average.\n\n4. **Top-Rated Movies Retrieval**:\n   - Define the `getTopRatedMovies` function to retrieve top-rated movies based on optional filters (genre, verdict) and a specified percentile for vote count (`percentile=0.85`):\n     - **Filtering**: Apply genre and verdict filters if specified.\n     - **Mean and Quantile Calculation**: Compute the mean vote average (`C`) and the vote count threshold (`m`) at the specified percentile.\n     - **Data Selection**: Select movies with a vote count greater than or equal to `m`.\n     - **Weighted Rating Calculation**: Compute the weighted rating for each selected movie.\n     - **Sorting**: Sort movies by their weighted rating in descending order.\n     - **Return**: Return the top 10 movies based on the weighted rating.\n\n5. **Result Saving**:\n   - Obtain the top-rated movies without specifying genre or verdict.\n   - Write the results to a csv file (`result.csv`), with the column named \"Moive\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-007"]}}], "post_process": []}
{"id": "dm-csv-009", "source": "https://drive.google.com/drive/folders/14793kyfMJ5GRbe5djEr0uzvQwXkwr-JM?usp=share_link", "instruction": "This is an Amazon Books Dataset with a description in README.md. Following the guidance provided in guidance.txt, identify the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors. Record the results in author.csv following the template provided in sample_result.csv.\n\nSteps for Reference:\n1.Review the provided three datasets. You can refer to the README.md for an introduction to these datasets and their columns.\n2.Import the datasets and remove unnecessary columns. For the price column, transform its data into a handleable type.\n3.Fill the missing values in the author column with \"Unknown\".\n4.Group the data by 'Author' to calculate the following metrics:\n5.Average rating of each author's books.\n6.Average price of each author's books.\n7.Total number of ratings accumulated by each author.\n8.Average number of ratings per book for each author.\n9.Set a minimum threshold for the number of average ratings per book to ensure reliability. In this case, the threshold is set as 1000.\n10.According to the definitions of the three types of authors in README.md, count the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors.\n11.Refer to the format defined in sample_result.csv and output the results to the specified file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-009"]}}], "post_process": []}
{"id": "dm-csv-010", "source": "https://drive.google.com/drive/folders/1V339AXpqR-f1DZK9wJNd0CN_svQf-I7o?usp=sharing", "instruction": "This is a Bike Store Relational Database, with related descriptions provided in the README.md file. You need to calculate the monthly average sales volume for each bike category and write the results into avg_units_sold.csv following the template of sample_result.csv.\n\nSteps for Reference:\n1. Read every csv files and sql.yml which indicates the relationships between different tables, which may help you to understand them.\n2. Establish a connection to a SQLite database using sqlite3.\n3. Insert the data from each DataFrame into corresponding tables in the database.\n4. Then start a SQL Query. Firstly, join tables relative with products table to get product categories.\n5. Group the sales data by year, month, and product to calculate the total units sold for each product in each month.\n6. Join the sales data with product categories data to group the information by month and category.\n7. Calculate the average units sold for each category in each month.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-010"]}}], "post_process": []}
{"id": "dm-csv-011", "source": "https://drive.google.com/drive/folders/1tO4GVH2DYJJwF2-HmSf3zTEzOUopthwH?usp=sharing", "instruction": "This is a Bike Store Relational Database, with relevant descriptions provided in the README.md file. You need to calculate and aggregate the total quantity sold and total sales revenue for each bike type from 2016 to 2018. Finally, write the results into result.csv following the template of sample_result.csv.\n\nSteps for Reference:\n1. Read every csv files and sql.yml which indicates the relationships between different tables, which may help you to understand them.\n2. Establish a connection to a SQLite database using sqlite3.\n3. Insert the data from each DataFrame into corresponding tables in the database.\n4. Create a temporary table , join tables relative with product names, model years, brand names, and category names.\n5. Join temporary table with order items, select fields such as `quantity`, `list_price`, `discount` and so on.\n6. Calculate the total_quantity sold and total_price for each category by multiplying list_price with (1 - discount).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-011"]}}], "post_process": []}
{"id": "dm-csv-015", "source": "https://drive.google.com/drive/folders/1yAz4jIJ4nuBf8qKEvzPYHrzja9RBMdek?usp=share_link", "instruction": "This is a dataset of baseball games, with relevant descriptions provided in the README.md file. You need to help me calculate the player with the most games played, the player with the most runs scored, the player with the most hits, and the player with the most home runs. Each statistical data should include the player’s name and their corresponding data; for example, for the player with the most games played, record their number of games played, and for the player with the most runs scored, record their runs scored. Finally, write the results into result.csv following the template of sample_result.csv.\n\nSteps for Reference:\n1. **Data Loading**:\n   - Use the `sqlite3` library to connect to a SQLite database and the `pandas` library to load multiple tables from the database into DataFrame objects.\n   - The tables loaded include `all_star`, `appearances`, `batting`, `batting_postseason`, `college`, `fielding`, `fielding_outfield`, `fielding_postseason`, `hall_of_fame`, `home_game`, `manager`, `manager_award`, `manager_award_vote`, `manager_half`, `player`, and `park`.\n\n2. **SQL Query Execution**:\n   - Construct and execute a SQL query to retrieve specific batting statistics aggregated by player.\n   - The query includes:\n     - The player with the most games played.\n     - The player with the most runs.\n     - The player with the most hits.\n     - The player with the most home runs.\n   - The query uses subqueries to calculate the required statistics for each player by joining the `player` and `batting` tables and grouping the results by `player_id`.\n\n3. **Result Saving**:\n   - Convert the SQL query result into a pandas DataFrame.\n   - Save the DataFrame to a CSV file named `result.csv` without including the DataFrame index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-015"]}}], "post_process": []}
{"id": "dm-csv-016", "source": "https://drive.google.com/drive/folders/1Nfx9mAWRZzmY6e3GPrIb2oAo2JSXfsIs?usp=share_link", "instruction": "This is a dataset for football matches, with relevant descriptions provided in the README.md file. I would like you to help me compile statistics for the seasons and the corresponding winning clubs hosted by `IT1` and `BESC`. Finally, write the results into result.csv following the template of sample_result.csv.\n\nSteps for Reference:\n1. **Data Loading**:\n   - Use `pandas` to read data from CSV files into DataFrame objects: `competitions`, `games`, `club_games`, `appearances`, and `clubs`.\n\n2. **Data Preparation**:\n   - **Finals DataFrame**: Filter the `games` DataFrame to include only the finals.\n   - **Matchdays DataFrame**: Filter the `games` DataFrame to include only matchdays, extract the round number, and group by `competition_id` and `season` to find the last matchday.\n   - **Finals Grouping**: Group the `finals` DataFrame by `competition_id`, `season`, and `round`, and sum the home club goals.\n   - **Combine DataFrames**: Combine `finals` and `last_matchday` DataFrames, remove duplicates, and reset the index to create a `competition` DataFrame.\n\n3. **Defining League Champions Function**:\n   - Define `league_champions` function to determine the league champion based on club position:\n     - Use `pandasql` to execute SQL queries on DataFrames.\n     - Select the club that finished in the first position at the end of the season.\n\n4. **Defining Cup Champions Function**:\n   - Define `cup_champions` function to determine the cup champion based on the final match result:\n     - Use `pandasql` to execute SQL queries on DataFrames.\n     - Select the club with more goals in the final match or mark it as a draw if goals are equal.\n\n5. **Defining General Champions Function**:\n   - Define `champions` function to determine the champion for either league or cup competitions:\n     - Identify if the competition is a league or a cup using the `competition` DataFrame.\n     - Call `league_champions` if the competition is a league.\n     - Call `cup_champions` if the competition is a cup.\n     - Return an error message if the competition ID is not found.\n\n6. **Getting Champions Data**:\n   - Get the champions for the Italian league (\"IT1\") and another league (\"BESC\").\n   - Add a column to each DataFrame indicating the league.\n\n7. **Saving Results**:\n   - Combine the champions DataFrames and save the result to a CSV file named `result.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-016"]}}], "post_process": []}
{"id": "dm-csv-025", "source": "https://drive.google.com/drive/folders/1_ZmzR5S8YNzsl9HhK1YX06y91zoX-xLR?usp=share_link", "instruction": "This is an IPL Complete Dataset (2008-2024) with relevant descriptions provided in the README.md file. Your task is to continue the development of the existing code in analys.py to calculate the winning and losing probabilities for each team according to the instructions in tips.txt. Afterwards, you need to identify the team with the highest probability of winning and the team with the highest probability of losing. Finally, write their names and corresponding (winning or losing) probabilities into team_probabilities.csv following the template provided in sample_result.csv.\n\n\nSteps for Reference:\n1. **Data Loading**:\n   - Read data from CSV files\n\n2. **Data Cleaning**:\n   - Rename instances of \"Rising Pune Supergiant\" to \"Rising Pune Supergiants\" in the `match_data` DataFrame for consistency.\n\n3. **Winning Probability Calculation**:\n   - Define the `winning_probability` function to calculate the winning and losing probabilities for a given team:\n     - Extract data for matches involving the given team.\n     - Calculate the total number of matches played, won, and lost by the team.\n     - Compute the winning and losing probabilities as follows:\n       \\[\n       \\text{Winning Probability} = \\frac{\\text{Total Matches Won}}{\\text{Total Matches Played}}\n       \\]\n       \\[\n       \\text{Losing Probability} = \\frac{\\text{Total Matches Lost}}{\\text{Total Matches Played}}\n       \\]\n\n4. **Probability Distribution Calculation**:\n   - Iterate over all IPL teams to calculate and store their winning and losing probabilities in dictionaries (`winning_probability_map` and `losing_probability_map`).\n\n5. **Data Sorting and Saving**:\n   - Sort the winning and losing probabilities in descending order.\n   - Convert the sorted lists to numpy arrays and save them as `.npy` files.\n\n6. **Identify Teams with Extreme Probabilities**:\n   - Find the team with the highest winning probability.\n   - Find the team with the highest losing probability.\n   - Print the results.\n\n7. **Save Results to CSV**:\n   - Create a DataFrame with the teams having the highest winning and losing probabilities along with their probabilities and types (winning/losing).\n   - Save this DataFrame to a CSV file named `team_probabilities.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-025"]}}], "post_process": []}
{"id": "dm-csv-026", "source": "https://drive.google.com/drive/folders/1MNmqFKeUQR7hpgVxc7pPJOcymQa35K9p?usp=share_link", "instruction": "This is a dataset containing information on FIFA 22 players, with detailed descriptions provided in the README.md file. Your task is to complete the analysis.py script to rank the players based on the specified score at the position level from 2015 to 2022. Relevant hints can be found in some text (txt) and Markdown (md) files. Ensure that the results are written to the result.csv file in the format demonstrated in sample_result.csv.\n\nSteps for Reference:\n1. **Data Loading**:\n   - Read CSV files for male and female players for the years 2016 to 2022 into separate dictionaries using pandas.\n\n2. **Database Setup**:\n   - Create an SQLite in-memory database using SQLAlchemy.\n   - Save each year's data as separate tables in the SQLite database for both male and female players.\n\n3. **Data Union**:\n   - Combine all player data into a single dataset (`raw_data`) by unioning all year tables for both genders, adding columns for gender and year.\n\n4. **Position Mapping**:\n   - Create a mapping of player positions to full position names, forming a new dataset (`player_full_position`).\n\n5. **Data Preparation**:\n   - Join the `raw_data` with `player_full_position` to get a comprehensive dataset (`raw_data2`) that includes full position names and various player attributes.\n\n6. **Role Level Score Calculation**:\n   - Compute a `role_level_score` for each player based on their position and relevant attributes, creating another dataset (`raw_data3`).\n\n7. **Role Level Percentage Calculation**:\n   - Calculate the `role_level_percentage` by normalizing the `role_level_score` against the maximum possible score for each position.\n\n8. **Filter and Export**:\n   - Filter out substitute players and export the top 20 rows of the resulting dataset to a CSV file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-026"]}}], "post_process": []}
{"id": "dm-csv-027", "source": "https://drive.google.com/drive/folders/1bzqJXG2xjnXjbzZdW8UYyH7MYVF0ZHsw?usp=share_link", "instruction": "This is a Netflix Data dataset, with detailed descriptions provided in the README.md file. Your task is to identify the top 10 countries by the number of produced shows and their corresponding counts, as well as the top 10 movie genres by the number of occurrences and their corresponding counts. Record the results in the provided templates: Top_10_countries.csv and Top_10_Movies.csv.\n\nSteps for Reference:\n1. Identify Top 10 Countries:\n   - Identify the top 10 countries by the number of produced shows\n   - List the corresponding counts for each country\n\n2. Identify Top 10 Movie Genres:\n   - Identify the top 10 movie genres by the number of occurrences\n   - List the corresponding counts for each genre\n\n3. Output Results to CSV:\n   - Record the top 10 countries and their counts in Top_10_countries.csv\n   - Record the top 10 movie genres and their counts in Top_10_Movies.csv\n```​", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-027"]}}], "post_process": []}
{"id": "dm-csv-029", "source": "https://drive.google.com/drive/folders/1ibfAui8NvdVlZTQGW5MQab57BKPQeAH_?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the top 3 users with the most pull requests and record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template.\n\nSteps for Reference:\n1. Identify Top Users:\n   - Identify the top 3 users with the most pull requests\n\n2. Count Pull Requests:\n   - Count the number of pull requests made by these top 3 users\n\n3. Output Results to CSV:\n   - Record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-029"]}}], "post_process": []}
{"id": "dm-csv-030", "source": "https://drive.google.com/drive/folders/1Wyg0lPtEKqh3F_sZOnR7wNZFb2iW7yt8?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the latest 10 pull requests, merge the related file data, and find the unique files involved in these pull requests. Record the results in the unique_files.csv file using the sample_result.csv template.\n\nSteps for Reference:\n1. Identify Latest 10 Pull Requests:\n   - Identify the latest 10 pull requests\n\n2. Merge Related File Data:\n   - Merge the related file data for these pull requests\n\n3. Find Unique Files:\n   - Find the unique files involved in these pull requests\n\n4. Output Results to CSV:\n   - Record the results in the unique_files.csv file using the sample_result.csv template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-030"]}}], "post_process": []}
{"id": "dm-csv-031", "source": "https://drive.google.com/drive/folders/1evC6AsX-2sevVmpbE5nlpqrfO1LZgmUJ", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the users who made the last 6 pull requests for the specific file \"src/compiler/scala/reflect/reify/phases/Calculate.scala\". Record the results in the users_last_6.csv file using the provided template.\n\nSteps for Reference:\n1. Identify Target File:\n   - Define the file of interest\n\n2. Extract Relevant Pull Request Data:\n   - Filter the pull request data to include only entries for the specified file\n\n3. Identify Last Six Users:\n   - Identify the last six users who made pull requests for the specified file\n\n4. Output Results to CSV:\n   - Write the results to a CSV file named users_last_6.csv", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-031"]}}], "post_process": []}
{"id": "dm-csv-033", "source": "https://drive.google.com/drive/folders/13jdtj7v3Ie4Hj9aasZiNjQUc46PE9eH3?usp=sharing", "instruction": "This is a dataset titled “Mobile-Games-Testing-with-Cookie-Cats,” with detailed descriptions provided in the README.md file. Your task is to compare the 1-day retention rates between two versions of a game (gate_30 and gate_40) according to the instructions in guidance.txt. Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 and record the result in the probability_greater_retention.csv file using the provided template.\n\nSteps for Reference:\n1. Calculate 1-Day Retention Rates:\n   - Calculate the 1-day retention rates for both gate_30 and gate_40 versions\n\n2. Compare Retention Rates:\n   - Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 compared to gate_40\n\n3. Output Results to CSV:\n   - Write the result in the probability_greater_retention.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-033"]}}], "post_process": []}
{"id": "dm-csv-034", "source": "https://drive.google.com/drive/folders/19EUef4q8aewkLlRDndOMM55OPCCBsFmo?usp=share_link", "instruction": "This is a dataset titled “Mobile-Games-Testing-with-Cookie-Cats,” with detailed descriptions provided in the README.md file. Your task is to calculate the proportion of 7-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_7_by_version.csv file using the provided template.\n\nSteps for Reference:\n1. Calculate Proportion of 7-Day Retention:\n   - Calculate the proportion of 7-day retention being True for both gate_30 and gate_40 versions\n\n2. Record Results by Version:\n   - Record the results separately for the gate_30 and gate_40 versions\n\n3. Output Results to CSV:\n   - Write the results in the retention_7_by_version.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-034"]}}], "post_process": []}
{"id": "dm-csv-036", "source": "https://drive.google.com/drive/folders/1o7QlFnyB-_RcHE9Kp7Jfa3VPu1Yf72-S?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the last six users who made pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala`. and fill in their usernames in the template file users_last_6.csv.\n\nSteps for Reference:\n1. Identify Target File:\n   Define the file of interest.\n\n2. Extract Relevant Pull Request Data:\n   Filter the pull request data to include only entries for the specified file.\n\n3. Identify Last Six Users:\n   Identify the last six users who made pull requests for the specified file.\n\n4. Output Results to CSV:\n   Write the results to a CSV file named `users_last_6.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-036"]}}], "post_process": []}
{"id": "dm-csv-037", "source": "https://drive.google.com/drive/folders/1G-zP7Vu07sIpX4mw598XqS2v8muQXRtf?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the top three users who made the most pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala` and fill in their usernames in the template file top_3_developers.csv.\n\nSteps for Reference:\n\n\n1. Identify Target File: Define the file of interest.\n2. Extract Relevant Pull Request Data: Filter the pull request data to include only entries for the specified file.\n3. Identify Top Developers: Identify the top three users who made the most pull requests for the specified file.\n4. Output Results to CSV: Write the results to a CSV file named `top_3_developers.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-037"]}}], "post_process": []}
{"id": "dm-csv-038", "source": "https://drive.google.com/drive/folders/1X9N6aLhTfXbic4dgXAt0e8G2x8TaTHir?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. I want to know the number of pull requests made by two users with the nicknames “soc” and “xeno-by” for each year between 2011 and 2016. Please fill in your statistical results in the provided template file pull_requests_by_year_and_author.csv.\n\nSteps for Reference:\n\n\n1. Identify Authors: Define the users of interest.\n2. Extract Relevant Pull Request Data: Filter the pull request data to include only entries for the specified authors and years (2011-2016).\n3. Filter Data by Year: Ensure the pull request data includes only the years 2011 to 2016.\n4. Count Pull Requests: Count the number of pull requests made by \"xeno-by\" and \"soc\" for each year from 2011 to 2016.\n5. Output Results to CSV: Write the results to a CSV file named `pull_requests_by_year_and_author_and_file.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-038"]}}], "post_process": []}
{"id": "dm-csv-039", "source": "https://drive.google.com/drive/folders/16Hk8i_1SqBwYWI3sp-hp4rMAgyynqsjk?usp=share_link", "instruction": "This is an Avocado Toast dataset, with detailed descriptions provided in the README.md file. You need to conduct a supply chain analysis of three ingredients used in an avocado toast according to the instructions in tips.md, utilizing the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file using the provided template.\n\nSteps for Reference:\n 1. Read in the avocado data\n- Read the avocado data from a tab-delimited CSV file.\n- Subset the DataFrame to include only a smaller number of relevant columns.\n- Read in the relevant category tags for avocados from a text file.\n\n 2. Filter avocado data using relevant category tags\n- Drop rows with null values in the `categories_tags` column.\n- Convert the `categories_tags` column from comma-separated strings to lists.\n- Filter the DataFrame to keep only rows with relevant category tags.\n\n 3. Determine the top origin country for UK avocados\n- Filter the avocado DataFrame for rows where `countries` equals \"United Kingdom.\"\n- Count and order the unique values in the `origins_tags` column.\n- Identify the top country of origin for avocados in the UK.\n- Clean up the country string to remove any leading characters or hyphens.\n\n 4. Create a user-defined function for ingredient analysis\n- Create a function called `read_and_filter_data()` that:\n  - Takes a filename and a list of relevant categories as arguments.\n  - Performs the same steps as above to read, subset, filter, and analyze the data.\n  - Returns the top country of origin for the ingredient.\n\n 5. Analyze other ingredients\n- Use the relevant categories data to determine the top origin countries for olive oil and sourdough by calling the `read_and_filter_data()` function.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-039"]}}], "post_process": []}
{"id": "dm-csv-043", "source": "https://drive.google.com/drive/folders/1izUKLoSM66ofPI6CV_l4_gzhM3uIZI5N?usp=share_link", "instruction": "You are provided with a Customer Segmation dataset, and detailed descriptions can be found in the README.md file. Your task is to calculate the retention data for each cohort according to the instructions provided in the tips.txt file. Save the calculated results to a file named retention.csv.\n\nSteps for Reference:\n1. Extract  year, month, and day from InvoiceDate, and store the result in a new column called InvoiceDay.\n2. Group the data by CustomerID and select the InvoiceDay value.\n3. Assign the minimum InvoiceDay value for each customer to a new column named CohortDay.\n4. Get the month of  CohortDay column, and save the result to CohortMonth.\n5. Calculate the number of months between InvoiceDay and CohortMonth and store in a new column named CohortIndex.\n6. Group by the data by CohortMonth and CohortIndex.\n7. Within each group, count the number of unique CustomerID values, reset the index of the resulting DataFrame and store it in cohort_data.\n8. Create a pivot with monthly cohort in the index, cohort index in the columns and the customer ID in the values.\n9. Select the first column and store it to cohort_sizes. Divide the cohort count by cohort sizes along the rows.Create a Pivot Table:\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-043"]}}], "post_process": []}
{"id": "dm-csv-044", "source": "https://drive.google.com/drive/folders/1mgjfhA3CF-Ksq7U2WHLCXspVV-RJNWvP?usp=share_link", "instruction": "This is a Customer Segmation dataset, and detailed descriptions are provided in the README.md file. Your task is to calculate the average unit price paid for items by groups of customers over time. Each group should be determined by the month of their first purchase. Save the calculated results in a file named average_price.csv, utilizing the provided template file.\n\nSteps for Reference:\n1. Extract  year, month, and day from InvoiceDate, and store the result in a new column called InvoiceDay.\n2. Group the data by CustomerID and select the InvoiceDay value.\n3. Assign the minimum InvoiceDay value for each customer to a new column named CohortDay.\n4. Get the month of  CohortDay column, and save the result to CohortMonth.\n5. Calculate the number of months between InvoiceDay and CohortMonth and store in a new column named CohortIndex.\n6. Group the data by CohortMonth and CohortIndex.\n7. Calculate the average UnitPrice for each cohort.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-044"]}}], "post_process": []}
{"id": "dm-csv-050", "source": "https://drive.google.com/drive/folders/1DQdtNaN-K38D6s5WMpqkGUUW1n9mdkTX?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, you need to calculate the cumulative returns of three different portfolio strategies: the default portfolio, the equal-weight portfolio, and the market value-weighted portfolio. Write the results (rounded to three significant figures) into result.csv according to the template provided in sample_result.csv.\n\nSteps for Reference:\n1. Load the corresponding csv files.\n2. Define portfolio weights with the given weights for each of the 9 stocks in README.md.\n3. Calculate weighted stock returns: multiply the StockReturns by portfolio weights defined in last step.\n4. Sum the WeightedReturns of same day, calculate the cumulative returns of it.\n5. Calculate the cumulative returns of equal-weight portfolio and market value-weighted portfolio, the market capitalizations of the companies is given in README.md.\n6. Calulate the ratio of market cap of the company to the total market cap of all companies, multipy weights ratios and areturns to calculate the market capitalization weighted portfolio returns.\n7. Merge the cumulative returns of three different portfolio strategies.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-050"]}}], "post_process": []}
{"id": "dm-csv-052", "source": "https://drive.google.com/drive/folders/1fLUJMT9WE5_NbHjnlMG_gvgejo8TrhlL?usp=share_link", "instruction": "This is a customer segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, use a 1-4 scale to calculate the RFM Score and complete the segmentation of users (dividing users into three groups based on their RFM Score). Write the segmentation results into result.csv according to the format provided in sample_result.csv.\n\nSteps for Reference:\n1. Group the data by CustomerID and perform the following aggregations, calculate following variables:\nRecency: Calculate the number of days since the last purchase for each customer.\nFrequency: Count the number of purchases (invoices) for each customer.\nMonetary Value: Sum the total amount spent by each customer across all purchases.\n2. Define labels for Recency and Frequency to categorize customers into different groups based on their values.\n3. Group Recency and Frequency values into four equal percentiles, assign the labels defined earlier.\n4. Define labels for MonetaryValue to categorize customers into different groups based on their spending.\n5. Group MonetaryValue into four equal percentiles, assign the labels defined earlier.\n6. Calculate the RFM score by summing the values of labels of Recency, Frequency, and Monetary.\n7. Define RFM level based on its value:\n'Top' for scores 10 or higher.\n'Middle' for scores between 6 (inclusive) and 10.\n'Low' for scores less than 6.\n8. Calculate RFM level of each customer, and save the result to output file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-052"]}}], "post_process": []}
{"id": "dm-csv-059", "source": "https://drive.google.com/drive/folders/1t1cWSI9zD7hKexuPQtIBxd2fNsXYAQlb?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Based on the registration dates in result.csv, you need to analyze the purchasing behavior of users within the first week of their registration date. Calculate the average number of purchases made in the first week on a daily basis, and then fill the results into result.csv.\n\nSteps for Reference:\n1.Read the CSV Files:\nLoad the CSV files in the dataset. If necessary, organize the data into a single DataFrame by considering the relationships between the columns.\n2.Calculate the Number of Purchases in the First Week:\nAnalyze the data based on the user's registration time and their purchase information. Count the number of purchases made by each user during their first week of registration.\n3.Calculate the Average Daily Purchases in the First Week:\nCompute the average number of purchases made by each user per day during their first week of registration.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-059"]}}], "post_process": []}
{"id": "ml-binary-002", "source": "https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction", "instruction": "This is a Health Insurance Cross Sell Prediction dataset, and the relevant description is in README.md. You need to predict whether the users in test.csv will respond, and write the results into submission.csv in the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Loading:**\n   - Load the training and testing datasets from CSV files.\n\n2. **Feature Identification:**\n   - Identify numerical and categorical features within the dataset.\n\n3. **Categorical Variable Handling:**\n   - Encode categorical variables using appropriate techniques like mapping or one-hot encoding to represent them numerically.\n\n4. **Data Type Consistency:**\n   - Ensure all categorical variables are consistently represented as strings or integers, depending on the encoding method chosen.\n\n5. **Numerical Feature Standardization:**\n   - Standardize numerical features so they have a mean of 0 and standard deviation of 1. This ensures features are on a similar scale and prevents any one feature from dominating others.\n\n6. **Normalization:**\n   - Normalize numerical features to a specific range, such as scaling them between 0 and 1. This can be achieved using techniques like Min-Max scaling to handle different ranges of numerical data appropriately.\n\n7. **Feature Engineering:**\n   - Perform necessary transformations or create new features based on domain knowledge or statistical analysis. This step can improve the model's ability to capture relevant patterns in the data.\n\n8. **Column Dropping:**\n   - Remove columns that do not provide useful information for model training or evaluation. This could include columns with constant values or those highly correlated with other features.\n\n9. **Test Data Transformation:**\n   - Apply the same preprocessing steps used for training data to the test dataset. This ensures that the model sees data in the same format during training and evaluation, preventing data leakage and ensuring consistency in predictions.\n\n10. **Hyperparameter Definition:**\n    - Define a range of hyperparameters to optimize during model training. This includes parameters like learning rate, regularization strength, or tree depth in decision trees. The goal is to find the combination that maximizes model performance without overfitting to the training data.\n11. **Output submission**\n    - make prediction on test data and write the results into submission.csv in the format of sample_submission.csv.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-002"]}}], "post_process": []}
{"id": "ml-binary-005", "source": "https://www.kaggle.com/code/bhatnagardaksh/pca-and-lda-implementation", "instruction": "This is a Breast Cancer Wisconsin (Diagnostic) Data Set, with the dataset description available in README.md. You need to predict the tumor diagnosis result (B or M) based on the given information. The data to be predicted is in test.csv. You need to write the predicted results into label.csv with the column name 'result'.\n\nSteps for Reference:\n1. **Load Data**:\n   - Load training data and test data from \"test.csv\" \n2. **Drop Unnecessary Columns**:\n   - Remove columns \"id\" and \"Unnamed: 32\" from the training data.\n   - Remove the \"id\" column from the test data.\n3. **Handle Categorical Data**:\n   - Identify the categorical column \"diagnosis\" which contains the labels.\n4. **Select Numerical Columns**:\n   - Identify numerical columns (type 'float64') from the training data.\n5. **Feature Selection Using Kruskal-Wallis Test**:\n   - Perform Kruskal-Wallis test on each numerical column to assess statistical significance with respect to the categorical column \"diagnosis\".\n   - Drop numerical columns that are not statistically significant (p-value >= alpha).\n6. **Prepare Input and Target Data**:\n   - Separate the remaining columns into input features and target column for both training and test data.\n   - Map categorical target labels ('B' and 'M') to numerical values (0 and 1).\n7. **Data Scaling**:\n   - Scale the input features of both training and test data using MinMaxScaler to ensure they are within a consistent range.\n8. **Handle Class Imbalance**:\n   - Calculate class weights based on the frequency of each class in the training data to address class imbalance effectively.\n9. **Train the Model**:\n    - Initialize a RandomForestClassifier with the computed class weights and train it on the prepared training data.\n10. **Predict on Test Data**:\n    - Use the trained model to predict outcomes on the test data.\n    - Convert numerical predictions back to categorical labels ('B' and 'M') for interpretation.\n11. **Save Predictions**:\n    - Save the predicted labels to a CSV file named \"label.csv\" with a column named \"result\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-005"]}}], "post_process": []}
{"id": "ml-binary-013", "source": "https://www.kaggle.com/code/lilyhyseni/renewind-energy-tuning-pipelining-ml-models", "instruction": "I have a dataset on wind turbine failures, with the description available in README.md. You need to predict whether the turbine will fail based on the data in Test.csv and write the prediction results into target.csv, with the column name \"Target\".\n\nSteps for Reference:\n2. **Load Data**:\n   - Load training data from \"Train.csv\" and test data from \"Test.csv\".\n   \n3. **Initial Data Exploration**:\n   - View the first few rows of both datasets to understand their structure.\n   - Check the dimensions (rows and columns) of each dataset.\n   - Identify and count null values in each column.\n   \n4. **Prepare Input and Target Data**:\n   - Create copies of the training and test datasets.\n   - Separate input features (X) from target labels (y) in the training data.\n\n5. **Handle Missing Values**:\n   - Replace missing values in numeric columns using the median value from the training data.\n   - Apply the same replacement to the test data to maintain consistency.\n\n6. **Data Scaling**:\n   - Standardize the numeric features to have zero mean and unit variance.\n   - Scale the test data using parameters learned from the training data.\n\n7. **Handle Class Imbalance**:\n   - Address class imbalance in the training data using SMOTE, generating synthetic samples for minority class.\n\n8. **Define Pipeline**:\n   - Create a processing pipeline with steps for imputation, scaling, and model training.\n   - Configure AdaBoostClassifier with specific parameters including the base estimator, number of estimators, and learning rate.\n\n9. **Train the Model**:\n   - Fit the pipeline on the training data to train the AdaBoostClassifier model.\n\n10. **Predict on Test Data**:\n    - Use the trained pipeline to make predictions on the test data.\n\n11. **Save Predictions**:\n    - Save the predictions as a CSV file named \"target.csv\" with a column header \"Target\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-013"]}}], "post_process": []}
{"id": "ml-binary-016", "source": "https://www.kaggle.com/code/bansodesandeep/credit-card-default-prediction", "instruction": "This is a Credit Card Default dataset, with relevant descriptions provided in the README.md file. Based on the credit card client information in test.csv, you need to predict whether they will default and write the predicted results into a file named defaulter.csv, with the column name \"IsDefaulter\".\n\nSteps for Reference:\n1. **Load Data**:\n   - Load the train dataset and test dataset\n2. **Rename Columns for Convenience**:\n   - Rename the target column 'default.payment.next.month' to 'IsDefaulter'.\n3. **Clean and Recode Data**:\n   - Recode values in 'EDUCATION' column to merge categories 0, 5, and 6 into category 4.\n   - Recode values in 'MARRIAGE' column to merge category 0 into category 3.\n4. **Categorical Feature Encoding**:\n   - Convert categorical features ['SEX', 'EDUCATION', 'MARRIAGE'] to human-readable strings.\n   - Convert 'AGE' column to integer type.\n5. **Feature Engineering**:\n   - Create new features 'Payement_Value' and 'Dues' based on specified calculations.\n6. **One-Hot Encoding**:\n   - Apply one-hot encoding to 'EDUCATION' and 'MARRIAGE' categorical features.\n7. **Label Encoding**:\n   - Encode 'SEX' feature numerically.\n8. **Prepare Input and Target Data**:\n   - Separate input features (X) and target labels (y), excluding specified columns.\n9. **Data Scaling**:\n    - Standardize the input features (X) using `StandardScaler`.\n10. **Hyperparameter Tuning with Grid Search**:\n    - Define a parameter grid for model hyperparameters.\n    - Use `GridSearchCV` to find the best parameters for the model.\n11. **Model Training**:\n    - Train the optimal model obtained from grid search on the training data.\n12. **Predict on Test Data**:\n    - Generate predictions using the trained model on the test data.\n13. **Save Predictions**:\n    - Export the predictions to a CSV file named \"defaulter.csv\" with a column \"IsDefaulter\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-016"]}}], "post_process": []}
{"id": "ml-binary-017", "source": "https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial", "instruction": "This is a Rain in Australia dataset, with relevant descriptions provided in the README.md file. Based on the data in test.csv, you need to predict whether it will rain the next day for each row and write the predicted results into a file named tomorrow.csv, with the column name \"RainTomorrow\".\n\nSteps for Reference:\n1 **Load Data**:\n   - Read train dataset and test dataset\n2. **Initial Data Cleaning**:\n   - Remove the 'RISK_MM' column from the dataset.\n3. **Identify and Handle Missing Values in Categorical Variables**:\n   - Identify categorical columns with missing values and count the number of missing entries for each.\n4. **Check Cardinality in Categorical Variables**:\n   - Determine the number of unique categories in each categorical column to assess cardinality.\n5. **Date Parsing and Feature Extraction**:\n   - Convert the 'Date' column to datetime format.\n   - Extract 'Year', 'Month', and 'Day' from the 'Date' column and add them as separate features.\n6. **Identify Categorical Variables**:\n   - List and identify all categorical columns in the dataset.\n7. **One-Hot Encoding of Categorical Variables**:\n   - Encode categorical columns 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', and 'RainToday' using one-hot encoding, handling missing values appropriately.\n8. **Identify and Handle Outliers**:\n   - Calculate outliers using Interquartile Range (IQR) for specified numerical columns like 'Rainfall' and 'Evaporation'.\n9. **Impute Missing Values**:\n    - Fill missing values in numerical columns with the median of each respective column from the training set.\n    - Fill missing values in categorical columns with the most frequent value from the training set.\n10. **Cap Maximum Values in Certain Numerical Features**:\n    - Set upper caps on values for 'Rainfall', 'Evaporation', 'WindSpeed9am', and 'WindSpeed3pm' to manage outliers effectively.\n11. **Feature Scaling**:\n    - Normalize or scale numerical features using MinMaxScaler across both training and testing datasets.\n12. **Train Logistic Regression Model**:\n    - Initialize and train a logistic regression model using the preprocessed training data.\n13. **Save Predictions**:\n    - Generate predictions on the test set using the trained logistic regression model.\n    - Save the predictions to a CSV file named \"tomorrow.csv\" with a column header \"RainTomorrow\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-binary-017"]}}], "post_process": []}
{"id": "ml-cluster-006", "source": "https://www.kaggle.com/code/katherineoktaviani/patient-clustering", "instruction": "This is a Patient Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset and write the clustering results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label).\n\nSteps for Reference:\n\n1. **Load Data**:\n   - Read the dataset \"patient_dataset.csv\" \n2. **Data Preprocessing**:\n   - Encode categorical variables 'residence_type' and 'smoking_status' using `LabelEncoder` for numerical representation.\n\n3. **Chi-Square Test**:\n   - Perform chi-square tests to analyze the association between 'residence_type' and other attributes in the dataset.\n\n4. **Data Cleaning**:\n   - Drop the 'residence_type' column following chi-square tests as it's deemed irrelevant or redundant.\n   - Convert the 'gender' column to integer type for consistency.\n\n5. **Standardization**:\n   - Create a standardized version of the dataset by scaling numerical features to have zero mean and unit variance using `StandardScaler`.\n\n6. **Min-Max Scaling**:\n   - Create a Min-Max scaled version where numerical features are scaled to a specified range [-1, 1] using `MinMaxScaler`.\n\n7. **Clustering**:\n   - Define the number of clusters (`k = 3`) for clustering analysis.\n   - Apply KMeans clustering with different initialization methods (`'random'` and `'k-means++'`) on both `df_std` and `df_mm`.\n\n8. **Save Results**:\n    - Incorporate clustered data and original feature columns, with an additional 'Cluster' column indicating cluster labels, and sace it to a CSV file named \"cluster.csv\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-006"]}}], "post_process": []}
{"id": "ml-cluster-009", "source": "https://www.kaggle.com/code/dipansujoshi/global-country-information-clustering", "instruction": "This is a global countries dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into 3 clusters. Write the results into cluster.csv, with the column names \"Feature_i\" (i representing the ith value in the feature vector) and \"Cluster\" (the cluster label)\n\nSteps for Reference:\n1. **Reading Data**:\n   - Load the dataset from the CSV file named `world-data-2023.csv` \n2. **Selecting Important Columns**:\n   - Extract specific columns: 'Country', 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'CPI', 'GDP', 'Tax revenue (%)', 'Unemployment rate' necessary for analysis.\n3. **Data Cleaning and Preparation**:\n   - Convert 'Agricultural Land( %)' to a float after stripping '%' symbols.\n   - Convert 'Land Area(Km2)' and 'Armed Forces size' to float after removing commas for numerical consistency.\n   - Convert 'CPI' and 'GDP' by stripping '$' signs and commas, then converting to float.\n   - Convert 'Tax revenue (%)' and 'Unemployment rate' to float after stripping '%' symbols.\n   - Impute missing values using the mean for columns: 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'GDP', 'Unemployment rate'.\n4. **Principal Component Analysis (PCA)**:\n   - Perform PCA with 3 components on the selected numerical columns: 'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'GDP', 'Unemployment rate'.\n5. **Clustering**:\n   - Apply KMeans clustering with `n_clusters=3` on the PCA-transformed data to group countries based on similar features.\n7. **Saving Results**:\n   - Save the clustered data to a CSV file named `cluster.csv`, including the transformed PCA features and a 'Cluster' label", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-009"]}}], "post_process": []}
{"id": "ml-cluster-010", "source": "https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters", "instruction": "This is a Forest Cover Type Dataset, with the description available in README.md. You need to perform a clustering task based on this dataset, dividing the data into an appropriate number of clusters. Write the clustering results into cluster.csv, with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from clustering).\n\nSteps for Reference:\n1. **Reading the Dataset:**\n   - Load the data from the CSV file `covtype.csv`.\n   - Identify and select numerical columns from the dataset.\n2. **Calculating New Feature:**\n   - Compute `Distance_To_Hydrology` using Euclidean distance based on `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology`.\n   - Drop the original distance columns\n3. **Mapping Categorical Variable:**\n   - Convert numerical  values to their corresponding names for better readability.\n4. **Encoding Categorical Variables:**\n   - Transform categorical variables into dummy variables to prepare them for further analysis.\n5. **Scaling Numerical Data:**\n   - Standardize numerical columns like elevation, aspect, etc., using `StandardScaler` to normalize them.\n6. **TSNE Initialization:**\n   - Initialize TSNE with different dimensionalities (`n_components=1`, `2`, and `3`) and a specific perplexity value (e.g., `perplexity=30`) for dimensionality reduction.\n7. **Performing TSNE Transformation:**\n   - Apply TSNE transformation separately to generate 1D, 2D, and 3D representations of the data.\n8. **Concatenating TSNE Components:**\n   - Combine the TSNE components with the original dataset to create, facilitating visualization and further analysis.\n9. **Saving Results:**\n    - Save the combined data including TSNE components, and 'Cluster' label to a new CSV file named `cluster.csv`, without including the index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-010"]}}], "post_process": []}
{"id": "ml-cluster-013", "source": "https://www.kaggle.com/code/vipulgohel/clustering-pca", "instruction": "This is a dataset of socioeconomic and health factors for various countries, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to divide the data into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\nSteps for Reference:\n\n1. **Read and Review Data:**\n   - Load the dataset `Country-data.csv` from the specified path into a dataframe named `country_data`.\n2. **Identify Feature Types:**\n   - Identify categorical and numerical features in the dataset using appropriate method\n3. **Correcting Data:**\n   - Convert percentage values of 'Exports', 'Imports', and 'Health_Spending' to absolute values based on 'GDP_per_capita' for consistency and accuracy.\n4. **Data Scaling:**\n   - Standardize numerical features using `StandardScaler` to ensure all features contribute equally to the analysis.\n5. **Principal Component Analysis (PCA):**\n   - Apply PCA to reduce the dimensionality of the scaled data to 5 principal components using `IncrementalPCA` for efficiency.\n6. **Hierarchical Clustering:**\n   - Perform hierarchical clustering (`linkage`) on the PCA-transformed data using single linkage and the Euclidean distance metric.\n7. **Cluster Labeling:**\n   - Determine clusters using a specified distance threshold (`2.4`) and assign cluster labels using `fcluster` based on the hierarchical clustering results.\n8. **Save Results:**\n    - Save the DataFrame with cluster labels \"Cluster\" and PCA-transformed features  to a CSV file named `cluster.csv`, ensuring to exclude the index for clarity and ease of use.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-013"]}}], "post_process": []}
{"id": "ml-cluster-016", "source": "https://www.kaggle.com/code/khusheekapoor/clustering-using-rfm-analysis-in-python", "instruction": "This is an Online Retail II UCI dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment the customers into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\nSteps for Reference:\n1. **Reading and Preparing Data:**\n   - Load the dataset `online_retail_II.csv` into a DataFrame `df`, ensuring `InvoiceDate` is parsed as dates and set as the index for time-based analysis.\n   - Extract transactions occurring between April 1, 2011, and December 9, 2011.\n   - Handle missing values by dropping rows where `Customer ID` is null to maintain data integrity.\n   - Filter out credit transactions indicated by 'C' in the `Invoice` column \n   - Remove duplicate transactions, retaining only the first occurrence to maintain transaction accuracy.\n2. **Calculating RFM Metrics:**\n   - Compute Recency (R) based on the month of each transaction to capture how recently customers made purchases.\n   - Initialize Frequency (F) to zero and calculate the number of transactions per customer to gauge customer engagement.\n   - Compute Monetary Value (M) by multiplying `Quantity` and `Price`, then aggregate to find the total revenue per customer.\n3. **Clustering with Agglomerative Clustering:**\n   - Apply Agglomerative Clustering with 3 clusters (`n_clusters=3`) on `df_rfm`, focusing on the features R, F, and M to group customers based on their transaction behavior.\n   - Define a custom function `plot_dendrogram` to visualize the dendrogram of the hierarchical clustering process for better understanding of cluster formation.\n4. **Post-Clustering Actions:**\n   - Assign cluster labels (`Cluster`) obtained from Agglomerative Clustering to each customer.\n   - Create a new DataFramecontaining features R, F, M, and corresponding Cluster labels.\n   - Save `cluster_results` to a CSV file named `cluster.csv`, excluding the index to maintain clarity and simplicity.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-016"]}}], "post_process": []}
{"id": "ml-cluster-019", "source": "https://www.kaggle.com/code/pinardogan/rfm-customer-segmentation-using-k-means", "instruction": "This is an Online Retail II Data Set dataset, with relevant descriptions provided in the README.md file. Based on this dataset, you need to perform a clustering task to segment different types of customer groups into an appropriate number of clusters. Write the clustering results into a file named cluster.csv, with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster obtained from the clustering).\n\nSteps for Reference:\n1. **Reading and Preparing Data:**\n   - Read the Excel file `online_retail_II.xlsx` from sheet \"Year 2009-2010\" using Pandas `read_excel` function.\n   - Identify columns in `df` such as `Customer ID`, `InvoiceDate`, `Invoice`, `Quantity`, `Price`, and `TotalPrice`.\n\n2. **Handling Missing Values:**\n   - Identify and display records where `Customer ID` matches specific missing values\n\n3. **Invoice Analysis:**\n   - Analyze invoices starting with the letter \"C\" to understand quantities and prices\n\n4. **Feature Engineering:**\n   - Create a new feature `TotalPrice` by multiplying `Quantity` with `Price`.\n\n5. **Date Manipulation:**\n   - Define `today_date` as `InvoiceDate` maximum plus 2 days to standardize recency calculation across timezones.\n\n6. **RFM Analysis:**\n   - Group by `Customer ID` to compute RFM metrics: Recency (`InvoiceDate` difference from `today_date`), Frequency (`Invoice` count), and Monetary (`TotalPrice` sum).\n\n7. **Normalization:**\n   - Normalize RFM metrics using `MinMaxScaler` from `sklearn.preprocessing`.\n\n8. **Clustering with KMeans:**\n   - Apply KMeans clustering (`KMeans`) with 6 clusters (`n_clusters=6`) on normalized RFM data (`df`).\n\n9. **Post-Clustering Actions:**\n   - Assign cluster labels (`Cluster`) to `df_clustered` DataFrame.\n   - Save `df_clustered` to CSV file named `cluster.csv` with columns derived from `df`.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-cluster-019"]}}], "post_process": []}
{"id": "ml-competition-001", "source": "https://www.kaggle.com/code/abdmental01/bank-churn-lightgbm-and-catboost-0-8945", "instruction": "This is a dataset for a Bank customer data for churn prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Loading and Preparation:**\n   - Load the training (`train.csv`), testing (`test.csv`), and submission (`sample_submission.csv`) datasets from CSV files.\n   - Check the dimensions and basic statistics of each dataset, including the number of rows, columns, and presence of null values.\n2. **Data Cleaning and Preprocessing:**\n   - Handle missing values in the datasets using appropriate methods such as imputation or removal.\n   - Scale numeric columns using `MinMaxScaler` to ensure consistent ranges.\n   - Encode categorical text features using TF-IDF vectorization to transform them into numerical representations.\n3. **Feature Engineering:**\n   - Create new meaningful features that capture insights from existing data.\n   - One-hot encode categorical features to convert them into a format suitable for machine learning models.\n4. **Modeling:**\n   - Define feature columns for training the model, excluding non-predictive columns \n   - Utilize CatBoostClassifier within a StratifiedKFold cross-validation framework to train and validate the model, ensuring robustness and performance assessment.\n5. **Prediction and Submission Preparation:**\n   - Generate predictions for the test data using the trained CatBoost model, predicting probabilities of customer churn.\n   - Prepare the submission file (`submission_cat3.csv`) by mapping predicted probabilities to the `Exited` column for submission.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-001"]}}], "post_process": []}
{"id": "ml-competition-002", "source": "https://www.kaggle.com/code/arunklenin/ps3e24-smoking-cessation-prediction-binary", "instruction": "This is a dataset for a health indicators and smoking status prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Reading and Exploration:**\n   - Load data from CSV files (`train.csv`, `test.csv`, `sample_submission.csv`) to examine the dimensions and structure of each dataset \n2. **Data Preprocessing:**\n   - **Duplicate Removal:** Remove duplicate entries from the original training dataset\n   - **Column Transformations:**normalize numerical features such as age, height, weight, etc. \n3. **Model Training and Evaluation:**\n   - Evaluate several models using cross-validation with 10 folds:\n     - **k-Nearest Neighbors (kNN):** \n     - **Linear Discriminant Analysis (LDA):**\n     - **Logistic Regression:** Train regularized logistic regression with specified parameters.\n     - **Random Forest (RF), Extra Trees (ET), Gradient Boosting (HistGB, LGBM, XGB):** Configure each with respective settings for boosting, trees, and ensemble methods.\n4. **Ensemble Model Training:**\n   - **Construct Ensembles:** Train several ensemble models using pipelines and the defined column transformer configurations.\n   - **Weighted Ensemble:** Test various weights (`w1`, `w2`, `w3`, `w4`) using `np.random.random_sample` to combine predictions`.\n5. **Final Prediction:**\n   - **Fit Models:** Refit all models  on the entire training dataset.\n   - **Test Predictions:** Generate predictions for the test set (`test`) using these models.\n   - **Ensemble Prediction:** Use the best weights to compute an ensemble prediction.\n6. **Output:**\n   - Save the final submission file (`submission.csv`) containing predictions based on the ensemble model.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-002"]}}], "post_process": []}
{"id": "ml-competition-003", "source": "https://www.kaggle.com/code/tetsutani/icr-iarc-eda-ensemble-and-stacking-baseline", "instruction": "This is a dataset for a health features and medical condition prediction competition, with the description available in README.md. Additionally, I have included an extra dataset. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Loading and Preprocessing**:\n   - Load datasets (`train.csv`, `test.csv`, `greeks.csv`) setting `Id` column as index.\n2. **Handling Missing Values**:\n   - Identify columns with missing values\n   - Print missing value summary for training dataset features.\n3. **Descriptive Statistics**:\n   - Compute descriptive statistics for numeric features\n\n4. **Kernel Density Estimation**:\n   - Define `get_kde_estimation` function for kernel density estimation.\n   - Compute KDE for numeric data.\n\n5. **Data Visualization Setup**:\n   - Prepare subplots and configurations for data visualization.\n\n6. **Correlation Analysis**:\n   - Compute Pearson correlation coefficients between features.\n   - Identify highly correlated feature pairs\n\n7. **Data Deduplication**:\n   - Check and handle duplicated rows based on specified columns \n\n8. **Data Transformation Pipelines**:\n   - **Casual Preprocessing**:\n     - Implement standard scaling, log, reciprocal, and power transformations\n     - Handle categorical features with imputation and ordinal encoding.\n     - Utilize K-nearest neighbors imputation for missing values.\n\n   - **Preliminary Preprocessing**:\n     - Apply similar transformations as Casual Preprocessing.\n     - Address semi-constant columns with binarization.\n\n9. **Dimensionality Reduction**:\n   - **t-SNE Visualization**:\n     - Generate 2D and 3D t-SNE embeddings for processed data \n10. **Time Series Data Handling**:\n    - Convert datetime values to ordinal values.\n    - Normalize ordinal values and handle NaN values.\n11. **Feature Selection**:\n    - **Mutual Information** and **ANOVA F-test**:\n      - Compute mutual information and F-statistic for feature selection.\n12. **Advanced Data Preprocessing**:\n    - Refine transformations based on feature characteristics (\n13. **Machine Learning Model Training**:\n    - **Ensemble Training**:\n      - Train Voting Classifier ensemble (`VotingClassifier` with `LGBMClassifier`, `XGBClassifier`, `SVC`).\n      - Implement cross-validation strategy (`StratifiedKFold`) with undersampling.\n14. **Model Evaluation and Post-processing**:\n    - Aggregate predictions across bags and folds for averaging.\n    - Apply post-processing to probability predictions \n15. **Submission Preparation**:\n    - Generate submission file (`submission.csv`) with formatted predictions.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-003"]}}], "post_process": []}
{"id": "ml-competition-005", "source": "https://www.kaggle.com/code/markuslill/s3e26-xgbclassifer", "instruction": "This is a dataset for a Patient data for cirrhosis outcomes prediction competition, along with an additional dataset. The descriptions are available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Reading and Preparation:**\n   - Read `train.csv` and `test.csv` into `train` and `test` DataFrames.\n   - Drop `N_Days` column from both `train` and `test`.\n2. **Data Encoding and Scaling:**\n   - **Label Encoding:** Encode 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Drug' columns in `train` and `test` using `LabelEncoder`.\n   - **Standard Scaling:** Scale numerical columns with `StandardScaler`.\n   - **Power Transformation:** Apply power transformation using `PowerTransformer` with 'yeo-johnson' method to the same numerical columns.\n3. **Model Training and Optimization:**\n   - **Random Forest (RF) Classifier:** \n     - Optimize hyperparameters using `GridSearchCV` with parameters for `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n     - Select best model based on grid search results.\n4. **Model Evaluation:**\n   - **XGBoost Classifier:**\n     - Train `XGBClassifier`\n     - Make predictions) on `X_test`.\n\n5. **Final Prediction and Submission:**\n     - Generate predictionsand class probabilities for `test` using trained XGBoost model.\n     - Create `submission` DataFrame with 'Status_C', 'Status_D', 'Status_CL' columns and `id` from `test`.\n     - Save `submission` as `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-005"]}}], "post_process": []}
{"id": "ml-competition-006", "source": "https://www.kaggle.com/code/abhi011097/s3-e5-eda-multi-approach-models-w-o-smote", "instruction": "This is a dataset for a Wine Quality Prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv according to the competition requirements. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n\n1. **Data Reading and Outlier Detection:**\n   - Read `WineQT.csv`, `train.csv`, and `test.csv`\n   - Detect outliers using quantiles and the 1.5 IQR rule for each dataset \n\n2. **Model Training and Cross-validation:**\n   - Define classifiers (`RandomForestClassifier`, `LGBMClassifier`, `GradientBoostingClassifier`) with default parameters.\n   - Prepare data for `StratifiedKFold` cross-validation (`kf`) with 5 folds.\n   - Conduct cross-validation using Cohen's kappa score  as the metric.\n\n3. **Model Execution and Prediction:**\n   - For each fold in `StratifiedKFold`:\n     - Prepare training and validation datasets.\n     - Train the model \n     - Compute and store Cohen's kappa score for each fold.\n\n4. **Ensemble Prediction and Submission:**\n   - Aggregate test predictions across all folds using mode to create final predictions for each model.\n   - Read `sample_submission.csv` into submission DataFrame\n   - Adjust predicted quality values and save to `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-006"]}}], "post_process": []}
{"id": "ml-competition-008", "source": "https://www.kaggle.com/code/aritra100/notebook-linear-regression-vs-regressor-r2score85", "instruction": "This is a flood prediction competition dataset, with the description available in README.md. As a participant, you need to design a method to predict the probability of flood occurrence in test.csv and write the prediction results into submission.csv, following the format of the sample_submission.csv template.\n\nSteps for Reference:\n### Detailed Steps from Data Reading to Final Answer Generation\n\n1. **Data Reading and Preparation:**\n   - CSV files `train.csv` and `test.csv` are read into DataFrames `flood_dataset` and `flood_dataset_test_df` respectively.\n   - The column `id` is dropped from `flood_dataset` as it is not needed for modeling.\n\n2. **Data Preprocessing:**\n   - `StandardScaler` is used to standardize the training data (`flood_dataset_train`) and test data (`flood_dataset_test_df`).\n   - Categorical target variable `FloodProbability` in `flood_dataset` is encoded using `LabelEncoder`.\n\n3. **Model Training:**\n   - Logistic Regression (`LogisticRegression`) is trained on the standardized training data (`flood_dataset_train_partially`) and encoded target variable (`dependent_var_transformed_partially`).\n   - Feature importance is extracted from the trained Logistic Regression model.\n\n4. **Stacked Regression Model:**\n   - Multiple regressors (`SGDRegressor`, `BayesianRidge`, `LinearRegression`) are stacked together using `StackingRegressor`, with a final estimator of `RandomForestRegressor`.\n   - The stacked model (`stacked`) is trained on the entire standardized training data (`X_train` and `y_train`).\n\n5. **Test Data Prediction:**\n   - Test data (`flood_dataset_test_df`) is standardized using the previously fitted `StandardScaler`.\n   - Predictions (`testdata_y_pred`) are made using the trained stacked model (`stacked`).\n\n6. **Submission Preparation:**\n   - Predicted values (`testdata_y_pred`) are adjusted (`testdata_y_pred/100`) and saved alongside the corresponding `id` from `flood_dataset_test_df` into a DataFrame `result`.\n   - The final DataFrame `result` is saved to `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-008"]}}], "post_process": []}
{"id": "ml-competition-009", "source": "https://www.kaggle.com/code/rohangulati14/leading-sol-regression-with-an-abalone-dataset", "instruction": "This is a competition based on an abalone dataset, with the description available in README.md. As a participant, you need to design a solution to complete this competition. Write your prediction results into submission.csv following the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Reading and Preparation:**\n   - Read CSV files `abalone.data`, `train.csv`, `test.csv`, and `sample_submission.csv`\n2. **Duplicate Handling:**\n   - Identify and count duplicates\n   - Remove duplicates from train data\n4. **Feature Engineering:**\n   - Encode categorical features using one-hot encoding\n   - Separate target variable from features\n5. **Model Training:**\n   - Initialize `RandomForestRegressor` (`best_forest`) with `random_state=27` and train it \n   - Extract and store feature importances\n6. **Feature Selection:**\n   - Identify highly correlated numeric columns  using Pearson correlation.\n   - Drop columns  based on correlation analysis.\n7. **Train-Validation Split:**\n   - Split data ` into training and validation sets\n8. **Model Stacking:**\n   - Initialize base models (`XGBRegressor`, `LGBMRegressor`, `CatBoostRegressor`, `RandomForestRegressor`) with `random_state=27`.\n   - Define meta-model (`CatBoostRegressor`) with specific hyperparameters.\n   - Train `StackingRegressor` (`stacking_model`) using base models and meta-model \n9. **Prediction and Submission:**\n   - Generate predictions using `stacking_model` \n   - Format results into a DataFrame `submission` with columns `id` and `Rings`.\n   - Save the submission DataFrame as `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-009"]}}], "post_process": []}
{"id": "ml-competition-010", "source": "https://www.kaggle.com/code/iqmansingh/crab-age-voting-regression-synthetic-data", "instruction": "This is a crab age prediction competition, with the description available in README.md. To achieve a higher ranking, an additional dataset of crab ages is provided. As a participant, you need to design and optimize a method to complete this competition. Write the prediction results into submission.csv, following the format of 'sample_submission.csv'.\n\nSteps for Reference:\n1. **Data Reading and Preparation:**\n   - Read CSV files (`train.csv`, `test.csv`) into DataFrames.\n   - Concatenate additional datasetst to train data\n2. **Data Cleaning and Preprocessing:**\n   - Identify and handle duplicates in `crab_train`.\n   - Replace non-standard values in `Sex` column.\n3. **Feature Engineering:**\n   - Encode categorical feature `Sex` using one-hot encoding.\n   - Handle zero values in numeric columns\n4. **Feature Transformation and Creation:**\n   - Calculate new features for both train and test data\n5. **Ensemble Model Building:**\n   - Create a VotingRegressor with models (`GradientBoostingRegressor`, `XGBRegressor`, `HistGradientBoostingRegressor`, `LGBMRegressor`, `CatBoostRegressor`) weighted for blending predictions.\n6. **Prediction and Submission:**\n   - Generate final predictions using `vrmodel.predict(crab_test)`.\n   - Format predictions into submission DataFrame (`crab_submission` with `id` and `Age`).\n   - Save submission as `submission.csv` without index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-010"]}}], "post_process": []}
{"id": "ml-competition-013", "source": "https://www.kaggle.com/code/tumpanjawat/s3e19-course-eda-fe-lightgbm", "instruction": "This is a dataset for a sales volume prediction competition, with the description available in README.md. You are now a contestant in this competition and need to design a method to predict the data in test.csv. Write the results into submission.csv according to the format of sample_submission.csv.\n\nSteps for Reference:\n2. **Data Preparation:**\n   - Read `train.csv` and `test.csv` into DataFrames.\n   - Convert `date` column to datetime format.\n   - Extract `Year`, `Month`, `Day`, and `WeekDay` from `date`.\n3. **Holiday Handling:**\n   - Retrieve holidays for specific countries (`AR`, `CA`, `EE`, `JP`, `ES`).\n   - Create `is_holiday` indicator based on holidays.\n4. **Feature Engineering:**\n   - Create additional features such as cyclical encoding for months and days.\n   - Apply one-hot encoding to categorical features for modeling.\n5. **Encoding Holiday Names:**\n   - Encode holiday names using `OrdinalEncoder`.\n   - Handle missing holiday names in the test set.\n6. **Model Training and Prediction:**\n   - Perform k-fold cross-validation (k=5) using `GroupKFold` on `year`.\n   - Train models (`LGBMRegressor`) with scaled features.\n\n7. **Prediction Aggregation and Submission:**\n   - Calculate mean ratios  weighted by specified years.\n   - Adjust predictionsusing computed weights.\n   - Format and save final submission as `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-013"]}}], "post_process": []}
{"id": "ml-competition-017", "source": "https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799", "instruction": "You are participating in an Automated Essay Scoring competition. The dataset and relevant descriptions are provided in the README.md file. As a participant, your task is to generate predictions of data in test.csv and write the results into a submission file named submission.csv in the format specified by sample_submission.csv.\n\nSteps for Reference:\n1. **Data Loading and Preprocessing:**\n   - Load datasets from CSV files.\n   - Split `full_text` into paragraphs using `\\n\\n` delimiter.\n2. **Paragraph Feature Engineering:**\n   - Explode paragraphs into individual rows.\n   - Preprocess text (lowercase, remove HTML, URLs, digits, special characters).\n   - Compute features: `paragraph_len`, `paragraph_sentence_cnt`, `paragraph_word_cnt`.\n3. **Paragraph Statistical Features:**\n   - Calculate counts of paragraphs based on specified length thresholds.\n   - Aggregate statistics (`max`, `mean`, `min`, `first`, `last`) for paragraph features.\n4. **Sentence Feature Engineering:**\n   - Split `full_text` into sentences based on periods (`.`).\n   - Filter sentences by minimum length.\n   - Compute features: `sentence_len`, `sentence_word_cnt`.\n5. **Sentence Statistical Features:**\n   - Calculate counts of sentences based on specified length thresholds.\n   - Aggregate statistics (`max`, `mean`, `min`, `first`, `last`) for sentence features.\n6. **Word Feature Engineering:**\n   - Split `full_text` into words based on spaces.\n   - Compute features: `word_len`.\n   - Remove zero-length words.\n7. **Word Statistical Features:**\n   - Calculate counts of words based on specified length thresholds.\n   - Compute statistics: `word_len_max`, `word_len_mean`, `word_len_std`, `word_len_q1`, `word_len_q2`, `word_len_q3`.\n8. **TF-IDF Vectorization:**\n   - Use `TfidfVectorizer` to transform `full_text` into TF-IDF features.\n   - Fit on training data and transform both training and test data.\n9. **Model Training:**\n   - Define `qwk_obj` as a custom objective function for LightGBM.\n   - Train LightGBM regressor with 5-fold cross-validation on engineered features.\n10. **Model Prediction:**\n    - Predict scores on test data using trained models.\n    - Average predictions from 5 models and round to integers within [1, 6].\n    - Output predictions to `submission.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-017"]}}], "post_process": []}
{"id": "ml-competition-020", "source": "https://www.kaggle.com/code/hardikgarg03/horse-health-prediction-using-stacked-models", "instruction": "This is a dataset for the Predict Health Outcomes of Horses competition, with relevant descriptions provided in the README.md file. As a participant, you need to complete the competition requirements by predicting the results for test.csv and writing them into submission.csv following the format of sample_submission.csv.\n\nSteps for Reference:\n1. **Data Loading**:\n   - Read training and test data from CSV files (`train.csv`, `test.csv`).\n2. **Handling Missing Values**:\n   - Identify and replace missing values with the most common value per column in both datasets.\n3. **Data Preprocessing**:\n   - **Categorical Encoding**: Convert categorical variables to dummy variables for physical examination columns.\n   - **Binary Encoding**: Convert binary categorical variables to numeric format (`yes`/`no` to `1`/`0`).\n4. **Feature Selection and Preparation**:\n   - **Feature Scaling**: Standardize numerical features using RobustScaler.\n   - **Feature Engineering**: Align and add specific features to ensure consistency between training and test datasets.\n   - **Class Weight Computation**: Calculate class weights for classification balance.\n5. **Model Training**:\n   - **Model Selection**: Train classifiers (`LGBMClassifier`, `XGBClassifier`, `CatBoostClassifier`) with tuned hyperparameters.\n   - **Stacking Classifier**: Combine models using `StackingClassifier` to predict class probabilities.\n6. **Model Evaluation and Submission**:\n   - **Submission Generation**: Predict test dataset outcomes using the stacked model.\n7. **Final Submission**:\n   - Generate predictions in `submission.csv` based on the stacked model.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-competition-020"]}}], "post_process": []}
{"id": "ml-multi-003", "source": "https://www.kaggle.com/code/benvarghese/linkedin-job-posting-machine-learning", "instruction": "This is a job information dataset, with the description available in README.md. You need to predict the experience level of job positions in test.csv based on this dataset and write the results into result.csv, with the column name \"formatted_experience_level\".\n\nSteps for Reference:\n1. **Data Reading and Preprocessing:**\n   - Read train data and test data\n   - Remove rows with missing `formatted_experience_level`.\n2. **Feature Engineering:**\n   - Calculate text-based features\n3. **Feature Addition:**\n   - Create binary features based on keyword presence in descriptions.\n4. **Data Resampling:**\n   - Use `RandomOverSampler` to handle class imbalance in the training set.\n5. **Feature Scaling:**\n   - Normalize features using `MinMaxScaler` for logistic regression.\n8. **Text Vectorization (Bag of Words and TF-IDF):**\n   - Preprocess text data and vectorize using `CountVectorizer` and `TfidfVectorizer`.\n   - Split vectorized data and handle class imbalance with `RandomOverSampler`.\n9. **Model Training on Text Data:**\n   - Train logistic regression and gradient boosting models on Bag of Words and TF-IDF vectorized data.\n   - Evaluate using 10-fold cross-validation and metrics.\n10. **Output**\n - Make prediction on the test data\n - Write the results into result.csv, with the column name \"formatted_experience_level\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-003"]}}], "post_process": []}
{"id": "ml-multi-008", "source": "https://www.kaggle.com/code/steremma/news-exploration-using-gensim-and-sklearn", "instruction": "This is a News Aggregator Dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named category.csv, with the column name \"CATEGORY\".\n\nSteps for Reference:\n1. **Reading Data and Dataset Information:**\n   - Read train data and test\n2. **Date Processing:**\n   - Extract minimum and maximum dates from `TIMESTAMP` to determine dataset timeline.\n   - Generate `MONTH` and `DAY` columns based on `TIMESTAMP`.\n 3. **Category Analysis:**\n   - Map category codes (`'b'`, `'t'`, `'e'`, `'m'`) to category names (`'Business'`, `'Science'`, `'Entertainment'`, `'Health'`).\n4. **Publisher Analysis:**\n   - Identify unique publishers and count their contributions.\n \n5. **Text Tokenization and Preprocessing:**\n   - Tokenize titles using NLTK's `word_tokenize`.\n   - Remove stopwords and punctuation, optionally lemmatize tokens.\n   - Unit test the tokenizer with sample text.\n6. **Text Vectorization and Model Training:**\n   - Use `CountVectorizer` with custom tokenizer.\n   - Encode `CATEGORY` labels using `LabelEncoder`.\n   - Train a `RandomForestClassifier` on the training set.\n7. **Save Results:**\n   - Make predictions on the test data\n    - Save results (e.g., `CATEGORY` predictions) to a new CSV file (`category.csv`).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-008"]}}], "post_process": []}
{"id": "ml-multi-009", "source": "https://www.kaggle.com/code/thisishusseinali/malicious-url-detection", "instruction": "This is a Malicious URLs dataset, with relevant descriptions provided in the README.md file. Based on the text information in test.csv, you need to predict the corresponding categories and write the predicted results into a file named type.csv, with the column name \"type\".\n\nSteps for Reference:\n\n1. **Data Reading and Initial Processing:**\n   - Read data from CSV file and inspect initial structure.\n   \n2. **Data Preprocessing and Feature Engineering:**\n   - **URL Cleaning:** Remove 'www.' from URLs and calculate URL length.\n   - **URL Classification Label Encoding:** Convert categorical labels to numerical values.\n   - **URL Feature Extraction:** Extract primary domain, count characters, detect URL shortening, abnormal URLs, secure HTTPS usage, and presence of IP addresses.\n   - **Text Analysis of URLs:** Count letters, digits, and special characters.\n 3. **Calculate Class Weight:**\n   - Determine class weights to handle class imbalance.\n4. **Model Training and Evaluation:**\n   - **Feature Selection and Training Data Preparation:** Define features and target for model training.\n   - **Model Selection and Cross-validation:** Train ExtraTreesClassifier using pipelines and cross-validation.\n\n5. **Final Model Application and Output:**\n   - **Prediction and Output:** Use the model to predict on test data.\n   - **Output Results:** Export predictions to a CSV file for final submission.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-009"]}}], "post_process": []}
{"id": "ml-multi-011", "source": "https://www.kaggle.com/code/datatattle/battle-of-ml-classification-models", "instruction": "This is a Coronavirus Tweets NLP - Text Classification dataset, with relevant descriptions provided in the README.md file. Based on the text information in Corona_NLP_test.csv, you need to predict their sentiment (classifying them as \"Positive,\" \"Negative,\" or \"Neutral\") and write the predicted results into a file named sentiment.csv, with the column name \"Sentiment.\"\n\nSteps for Reference:\n1. **Data Loading**\n   - **Data Files:** Read train data and test data\n     - Define and apply function to map sentiment classes to \"Positive\", \"Negative\", and \"Neutral\" in a new `label` column.\n2. **Text Preprocessing:**\n     - Apply functions to clean text: remove URLs, HTML, lowercase, remove numbers, punctuation, stopwords, mentions, hashtags, and extra spaces.\n3. **Feature Extraction:**\n     - Initialize `TfidfVectorizer` with specific parameters.\n     - Fit and transform the `OriginalTweet` text to create TF-IDF features.\n4. **Model Training and Evaluation:**\n     - Train LinearSVC and Perform 5-fold cross-validation on TF-IDF features and labels.\n5. **Final Model Training and Prediction:**\n     - Predict sentiment labels on the test set.\n6. **Output:***\n     - Save the DataFrame to `sentiment.csv` with only the `Sentiment` column.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-multi-011"]}}], "post_process": []}
{"id": "ml-regression-002", "source": "https://www.kaggle.com/code/nigelclinton/energy-price-prediction-ml", "instruction": "I have a dataset on Spanish electricity and weather, with the description available in README.md. You need to predict the electricity prices in test.csv based on this dataset. Write your prediction results into result.csv, with the column name \"price actual\".\n\nSteps for Reference:\n1. **Data Loading and Preprocessing:**\n     - Read train data and test data \n     - Compute feature correlations.\n     - Identify and drop columns with zero values.\n      - Drop columns with data leakage or significant null values.\n2. **Modeling Preparation:**\n     - Split DataFrame into target vector (`price_actual`) and feature matrix.\n     - Transform categorical `season` column into numerical values.\n     - Handle missing values with mean imputation.\n3. **Modeling and Prediction:**\n     - Initialize and fit `RandomForestRegressor` model on training data.\n     - Predict `price_actual` for test data.\n     - Create DataFrame with predictions.\n     - Save predictions to `../result.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-002"]}}], "post_process": []}
{"id": "ml-regression-004", "source": "https://www.kaggle.com/code/dima806/spotify-song-ratings-predict-catboost-shap", "instruction": "I currently have a pop music dataset, with the description available in README.md. You need to predict the popularity of the songs in test.csv based on this dataset. Write the prediction results into popularity.csv, with the column name \"Poplarity\".\n\nSteps for Reference:\n1. **Data Loading and Preprocessing:**\n- Read and clean the training and test datasets by removing duplicates.\n- Filter out records with zero popularity in the training set.\n- Convert track duration from milliseconds to minutes for both datasets.\n- Extract the release year from the album release date for both datasets.\n- Group numerical features into larger bins for simplification.\n- Round loudness and tempo values for the test set.\n- Encode rare categorical labels to handle infrequent categories in specific columns.\n\n2. **Modeling:**\n- Use CatBoostRegressor for prediction.\n- Drop irrelevant columns from both datasets.\n- Define the target variable and features from the training set.\n- Identify categorical features and their indices.\n- Create a training pool with the training data and categorical features.\n- Initialize the CatBoostRegressor with specified parameters.\n- Train the model using the training pool.\n- Predict popularity for the test set.\n- Create a DataFrame with the prediction results.\n- Save the predictions to a CSV file.\n\n3. **Output:**\n- Save the prediction results to a CSV file named 'popularity.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-004"]}}], "post_process": []}
{"id": "ml-regression-006", "source": "https://www.kaggle.com/code/hknaralasetty/lego-regression-55-23-randomforest-mae-e-5", "instruction": "This is a Lego sets and price [1955 - 2023] dataset, with the description available in README.md. You need to predict the star rating for the data in test.csv. Write your prediction results into result.csv, with the column name \"Star rating\".\n\nSteps for Reference:\n1. **Data Loading and Preprocessing:**\n- Read and load data from training and test datasets.\n- Remove specific columns ('Sets URL' and 'Part URL') from both datasets.\n- Remove rows with missing values in the training dataset.\n- Standardize the 'Star rating' column to a consistent numeric format.\n- Define numeric features and convert them to numeric types.\n- Encode categorical text features using label encoding.\n- Calculate the correlation matrix and filter features with low correlation.\n- Drop duplicates from the filtered training dataset.\n\n2. **Modeling:**\n- Use RandomForestRegressor for prediction.\n- Split the data into training and testing sets.\n- Create a pipeline including feature scaling and the regressor.\n- Train the model using the training data.\n- Predict the target variable for the test data.\n- Create a DataFrame with prediction results and save it.\n\n3. **Output:**\n- Save the prediction results to a CSV file named 'result.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-006"]}}], "post_process": []}
{"id": "ml-regression-008", "source": "https://www.kaggle.com/code/michaelminhpham/predict-unit-sold-mens-women-shoes-on-tiki", "instruction": "This is a dataset of product information from the TIKI e-commerce platform, with the description available in README.md. You need to predict the product sales based on the product information in test.csv. Please note that all items in test.csv are shoes. Write the prediction results into quantity.csv, with the column name \"quantity_sold\"\n\nSteps for Reference:\n1. **Data Loading and Preprocessing:**\n- Load data from men and women shoes CSV files.\n- Remove the 'Unnamed: 0' column from both datasets.\n- Add a 'category' column to each dataset indicating 'men' or 'women'.\n- Concatenate the datasets into a single DataFrame and reset its index.\n- Remove duplicate rows based on the 'id' column.\n- Clean the 'brand' column by replacing specific text.\n- Create new columns for estimated revenue and discount rate based on price and quantity sold.\n\n2. **Keyword Filtering:**\n- Define a list of keywords for filtering.\n- Convert keywords and relevant columns to ASCII format.\n- Filter rows based on the presence of keywords in the name or description.\n- Remove temporary columns used for filtering.\n\n3. **Data Preparation for Modeling:**\n- Create dummy variables for categorical features.\n- Separate the dataset into features and target variable.\n- Split the data into training and testing sets.\n- Scale the feature values.\n\n4. **Model Training and Evaluation:**\n- Use LazyPredict to initialize and evaluate multiple regression models.\n- Fit the models on the training data and store performance results and predictions.\n\n5. **Output:**\n- Save the model predictions to a CSV file named 'quantity.csv'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-008"]}}], "post_process": []}
{"id": "ml-regression-010", "source": "https://www.kaggle.com/code/renjiabarai/drug-reviews-regression", "instruction": "This is a dataset of comprehensive drug ratings, with the description available in README.md. You need to predict the usefulness of drugs in drugsComTest_raw.csv based on this dataset and write the prediction results into \"Usefulness.csv\", with the column name \"usefulness\".\n\nSteps for Reference:\n1.**Data Loading:**\n- Load the training data from the specified CSV file.\n2. **Data Cleaning:**\n- Replace missing values in the 'condition' column with 'NOT POINTED'.\n- Determine column types using a specified function.\n- Convert 'drugName' and 'condition' columns to numeric values.\n3. **Feature Engineering:**\n- Replace 'review' column values with the length of each review.\n- Create a 'usefulness' column as the product of 'rating' and 'usefulCount'.\n- Drop 'rating' and 'usefulCount' columns.\n4. **Prepare Training Data:**\n- Extract 'usefulness' as the target variable.\n6. **Test Data Cleaning:**\n- Load the test data from the specified CSV file.\n- Drop unnecessary columns.\n- Replace 'drugName' and 'condition' with corresponding numeric values from the training data.\n- Assign new numeric values to any new categorical values in 'drugName' and 'condition'.\n- Replace 'review' column values with the length of each review.\n- Create a 'usefulness' column as the product of 'rating' and 'usefulCount'.\n7. **Model Training and Prediction:**\n- Train a CatBoostRegressor model on the training data.\n- Predict the target variable on the test data using the trained model.\n- Store the predictions into \"Usefulness.csv\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-010"]}}], "post_process": []}
{"id": "ml-regression-011", "source": "https://www.kaggle.com/code/mehmetisik/u-s-farms-biogas-ml-prediction-livestock", "instruction": "Here is a biogas dataset, with the description available in README.md. You need to predict the Biogas Generation Estimate (cu-ft/day) in test.csv based on this dataset, and write the prediction results into result.csv, with the column name 'biogas_generation_estimate_cuftday'.\n\nSteps for Reference:\n1.**Data Loading**:\n- Load the dataset from the CSV file into a DataFrame.\n2. **Data Preprocessing**:\n- Identify and categorize columns as categorical, numeric, and cardinal.\n- Handle missing values by identifying columns with missing data and their ratios.\n- Apply one-hot encoding to categorical columns.\n- Exclude the target variable from numeric columns and apply robust scaling to standardize them.\n- Clean and format column names by replacing spaces with underscores, removing non-alphanumeric characters, and converting to lowercase.\n3. **Feature Selection and Target Variable**:\n- Define the target variable.\n- Create the feature set by removing the target variable from the DataFrame.\n4. **Model Training and Evaluation**:\n- Choose LightGBMRegressor\n**Hyperparameter Tuning**:\n- Define hyperparameter grids for the model.\n- Use grid search for hyperparameter tuning.\n- Train the best model from the grid search \n**Final Model**:\n- Make final predictions on the test set using the best model.\n**Results**:\n- Save the prediction results to a CSV file named `result.csv` with the column name 'biogas_generation_estimate_cuftday'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-011"]}}], "post_process": []}
{"id": "ml-regression-012", "source": "https://www.kaggle.com/code/iamnimish/automotive-data-analysis-and-recommendation-system", "instruction": "This is a dataset of vehicle information in New York, with the description available in README.md. You need to predict the Mileage of vehicles in test.csv based on this dataset and write the results into result.csv, with the column name \"Mileage\".\n\nSteps for Reference:\n1.**Data Loading**:\n- Load the dataset from the CSV file into a DataFrame.\n2. **Data Cleaning**:\n- Replace missing values\n3. **Feature Encoding**:\n- Encode categorical variables using `LabelEncoder`.\n4. **Regression Models (Regression Task)**:\n- Train GradientBoostingRegressor on train data\n- Predict on the test set for the model\n5. **Writing Predictions to CSV**:\n- Write the predictions to a CSV file named `result.csv` with the column name \"Mileage\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-012"]}}], "post_process": []}
{"id": "ml-regression-014", "source": "https://www.kaggle.com/code/qusaybtoush1990/car-prices-poland", "instruction": "This is a Car Prices Poland dataset, with relevant descriptions provided in the README.md file. Based on the vehicle information in test.csv, you need to predict their prices and write the predicted results into a file named price.csv, with the column name \"price\".\n\nSteps for Reference:\n1.**Load Dataset**:\n- Read the CSV file named \"Car_Prices_Poland_Kaggle.csv\" into a DataFrame.\n\n2. **Convert Categorical Features to Numerical**:\n-  encode the 'mark' and '  'fuel' column to numerical values and store it in a new column 'Mark'.\n3. **Drop Unnecessary Columns**:\n4. **Model Training**:\n- Initialize a `RandomForestRegressor` model.\n- Train the model using the training data\n5. **Write Results**:\n- Write the predicted results into a file named `price.csv` with the column name \"price\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-014"]}}], "post_process": []}
{"id": "ml-regression-015", "source": "https://www.kaggle.com/code/msand1984/appliance-energy-prediction", "instruction": "This is an Appliances Energy Prediction dataset, with relevant descriptions provided in the README.md file. Based on the information in test.csv, you need to predict the corresponding appliance energy consumption and write the predicted results into a file named appliance.csv, with the column name \"Appliances\".\n\nSteps for Reference:\n1. **Load Dataset**:\n- Read train data and test data\n\n2. **Standardization**:\n- Scale the training and testing sets using `StandardScaler` to normalize the numerical features.\n\n2. *Hyperparameter Tuning with GridSearch**:\n- Train and Perform hyperparameter tuning for `ExtraTreesRegressor` using `GridSearchCV` to find the best combination of parameters (`max_depth`, `n_estimators`, `max_features`).\n\n3. **Feature Importance**:\n- Get the sorted list of features based on their importance from the best estimator found in GridSearchCV.\n\n4. **Model Refinement**:\n- Clone the best estimator from GridSearchCV with the optimized parameters.\n- Fit the model on a reduced dataset based on important features.\n\n5. **Output Prediction**:\n- Write the predicted results into a file named \"appliance.csv\", containing predictions for the column \"Appliances\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/ml-regression-015"]}}], "post_process": []}
{"id": "data-sa-001", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2000, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided in sample_result.csv.\n\nStep for reference\n\n1.Converting the 'date' column in each DataFrame to datetime format.\n2.Filtering Data for FIFA World Cup Post-2000:\nWomen’s FIFA World Cup matches from the year 2000 and Men’s FIFA World Cup matches from the year 2000.\n3.Calculating Total Goals Scored:\nAdding the home and away scores for each match.\n4.Normality Test (Shapiro-Wilk):\nChecking if goal scores follow a normal distribution for both datasets.\n5.Mann-Whitney U Test (Using Pingouin):\nComparing goal scores between women’s and men’s datasets.\n6.Test if women's scores are greater than men’s scores.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-001"]}}], "post_process": []}
{"id": "data-sa-004", "source": "https://www.kaggle.com/datasets/ranitsarkar01/yulu-bike-sharing-data/data", "instruction": "You have a Business Case dataset, with relevant descriptions provided in the README.md file. Following the instructions in tips.md, complete the hypothesis test for \"if the number of bike rentals is similar or different in different weather conditions\" and write the corresponding results into weather.csv according to the required format.\n\n\n\nStep for reference\n1. Follow Instructions in tips.md: Follow the instructions provided in tips.md to complete the hypothesis test.\n2. Hypothesis Test: Conduct the hypothesis test to determine if the number of bike rentals is similar or different in different weather conditions.\n3. Output Results to CSV: Write the corresponding results into the weather.csv file according to the required format.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-004"]}}], "post_process": []}
{"id": "data-sa-026", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=8", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to compute the 99% Conditional Value at Risk (CVaR) for a portfolio based on 2008-2009 data. This involves loading and preprocessing the data, calculating daily returns, and computing portfolio losses. You will then fit a Student's t-distribution to these losses using t.fit, determine the 99% Value at Risk (VaR) with t.ppf, and calculate the 99% CVaR using t.expect to integrate tail losses beyond the 99% VaR threshold. Finally, save the results in the format specified in result.csv.\n\nStep for reference\n1. **Read Stock Data**: Load the stock data from a CSV file with the date column parsed as datetime and set as the index.\n\n2. **Define Weights**: Define the weight vector for the portfolio, e.g., each stock is assigned an equal weight of 0.25.\n\n3. **Calculate Daily Returns**: Compute the daily percentage change in stock prices to get the daily returns.\n\n4. **Compute Portfolio Returns**: Calculate the weighted returns of the portfolio by taking the dot product of the daily returns and the weight vector.\n\n5. **Calculate Portfolio Losses**: Compute the negative of the portfolio returns to get the losses.\n\n6. **Fit Student's t Distribution**: Fit a Student's t distribution to the portfolio losses to model the loss distribution.\n\n7. **Compute VaR at 99%**: Calculate the 99th percentile (VaR_99) of the fitted distribution using the percent point function (PPF).\n\n8. **Compute CVaR at 99%**: Calculate the Conditional Value at Risk (CVaR_99) using the expected value of losses beyond the VaR_99 threshold. The formula involves integrating the tail loss and scaling by \\( \\frac{1}{1-0.99} \\).\n\n9. **Output CVaR_99**: Print the computed CVaR_99 value.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-026"]}}], "post_process": []}
{"id": "data-sa-028", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=14", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to calculate the Black-Scholes option prices for an IBM stock using historical data. This involves computing the annualized volatility based on the stock's daily returns and then calculating the option price using the Black-Scholes model for the computed volatility and for twice the volatility. Finally, fill in the calculated results in the format specified in result.csv.\n\nStep for reference\n1. **Load Data**: Read historical stock price data for IBM from a CSV file, parse dates, and set the date column as the index.\n\n2. **Calculate Daily Returns**: Compute daily percentage changes in the 'close' prices of IBM to obtain daily returns.\n\n3. **Compute Volatility**: Annualize the standard deviation of daily returns by multiplying it with \\( \\sqrt{252} \\), where 252 is the number of trading days in a year.\n\n4. **Black-Scholes Option Pricing**: Utilize the Black-Scholes option pricing model to compute the theoretical option price for a European call option under two scenarios:\n   - **Value for \\( \\sigma \\)**: Calculate the option price using the computed volatility \\( \\sigma \\).\n   - **Value for \\( 2 \\times \\sigma \\)**: Calculate the option price using twice the computed volatility \\( 2 \\times \\sigma \\).\n\n5. **Output Comparison**: Save results in to \"result.csv\", with columns \"sigma and\"2*sigma\"\n\nThe Black-Scholes function `black_scholes` is used twice with different volatility inputs to demonstrate the sensitivity of option prices to changes in volatility.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-028"]}}], "post_process": []}
{"id": "data-sa-029", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/goal-oriented-risk-management?ex=16", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform delta hedging for an investment portfolio with IBM stock using a European put option following the instructions in step.md. After performing the delta hedging, fill in the calculated results in the format specified in result.csv.\n\nStep for reference\n1. Read data \n2. **Auxiliary Functions**: \n   - `d11(S, X, T, r, sigma)`: Calculates the risk-adjusted probability for the first term in the Black-Scholes formula.\n   - `d21(d1, T, sigma)`: Calculates the risk-adjusted probability for the second term in the Black-Scholes formula.\n\n3. **Black-Scholes Delta Function (`bs_delta`)**:\n   - Computes the delta of the Black-Scholes formula based on whether it's a call or put option.\n\n4.. **Black-Scholes Option Pricing (`black_scholes`)**:\n   - Computes the price of a European option using the Black-Scholes formula, considering the current stock price (`S`), strike price (`X`), time to maturity (`T`), risk-free interest rate (`r`), volatility (`sigma`), and option type (`option_type`).\n\n5. **Application**:\n   - Loads historical stock price data for IBM, computes daily returns, and annualizes the volatility.\n   - Calculates the Black-Scholes put option value (`value`) at a specific spot price (`S = 70`).\n   - Computes the delta (`delta`) of the put option at the same spot price.\n   - Determines the change in option value (`value_change`) when the spot price drops to `69.5`.\n   - Finally, Save the result into the \"result.csv\" with the column \"value\"\n\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-029"]}}], "post_process": []}
{"id": "data-sa-031", "source": "https://campus.datacamp.com/courses/quantitative-risk-management-in-python/estimating-and-identifying-risk?ex=10", "instruction": "You have a Quantitative Risk Management dataset task, with the description provided in README.md. In this task, you need to perform a Chow test to determine if the 2008 financial crisis caused a significant change in the relationship between mortgage delinquency rates and portfolio returns. After conducting the Chow test, write the results in the format specified in result.csv.\n\nStep for reference\n\n\n1. **Data Preparation**: \n   - Two datasets (`mort_del` and `port_q_min`) are read from CSV files and merged based on the 'Date' index.\n   - Column names are renamed for clarity to 'mort_del' (mortgage delinquency) and 'returns' (portfolio returns).\n\n2. **Time Period Segmentation**: \n   - The merged dataset (`data`) is split into two periods: 'before' June 30, 2008 and 'after' July 1, 2008 based on date indexing.\n\n3. **Regression Analysis**: \n   - Intercept terms are added to the independent variable (`mort_del`) for both the 'before' and 'after' periods.\n   - Separate Ordinary Least Squares (OLS) regressions are performed on 'returns' against 'mort_del' for each period (`before` and `after`).\n\n4. **Sum-of-Squared Residuals Calculation**: \n   - Sum-of-squared residuals (`ssr_before` and `ssr_after`) are computed for the regression models of the 'before' and 'after' periods respectively.\n   - Total sum-of-squared residuals (`ssr_total`) is calculated for the combined dataset.\n\n5. **Chow Test Statistic**: \n   - The Chow test statistic is computed using the formula involving the differences in sum-of-squared residuals between the combined model and the sum of residuals from individual models, normalized by the degrees of freedom.\n\n6. **Result Output**: \n   - The computed Chow test statistic is saved to a CSV file named \"result.csv\", with column named \"Chow test statistic\"\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-031"]}}], "post_process": []}
{"id": "data-sa-039", "source": "https://www.kaggle.com/code/mustafagerme/hypothesis-testing-with-men-women-s-soccermatches", "instruction": "You have a football match dataset, with relevant descriptions provided in the README.md file. You need to conduct a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches since 2002, testing the hypothesis that the number of goals in women's matches is significantly greater than in men's matches. If the confidence level(p-value) is less than 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\"). You need to write the calculation results and the conclusion into result.csv following the format provided.\n\nStep for reference\n1.Data Loading and Preparation:\nLoad the women’s and men's datasets.\nIdentify and handle categorical columns.\nConvert the 'date' columns to datetime format.\n2.Filter Data from 2002 Onwards:\nFilter the data to include results from FIFA World Cup matches from 2002 onwards.\n3.Calculate Total Goals Scored:\nCalculate the total goals scored per match for both women’s and men's datasets.\n4.Normality Test:\nConduct a Shapiro-Wilk test to check for normality in the goal scores of both datasets.\n5.Mann-Whitney U Test:\nUse the Mann-Whitney U test to compare the goal scores between women’s and men's datasets.\n6.Hypothesis: The number of goals in women's matches is greater than in men’s matches.\nAlternative hypothesis: alternative = \"greater\".\n7.Determine Hypothesis Result:\nCompare the p-value against the 0.01 significance level.\nIf p-value ≤ 0.01, reject the hypothesis (\"reject\"); otherwise, fail to reject the hypothesis (\"fail to reject\").\n9.Save Results", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-039"]}}], "post_process": []}
{"id": "data-sa-051", "source": null, "instruction": "You need to identify the region and source with the highest CO2 emissions and perform ANOVA tests to compare emissions across different fuel sources within each geographical region. Please fill in the results into the anova_results.csv file following the provided format.\n\nStep for reference\n\n1. **Data Loading and Preparation**:\n   - Two datasets (`energy_design_a.csv` and `energy_design_b.csv`) are loaded into pandas DataFrames (`energy_design_a` and `energy_design_b`).\n   - Based on the experimental design and column review, `energy_design_b` is selected for further analysis.\n\n2. **Statistical Analysis**:\n   - A boxplot is created using Seaborn to visualize CO2 emissions grouped by geographical region and fuel source.\n   - The region with the highest median CO2 emissions (\"South\") and the fuel source contributing to this (\"Coal\") are identified based on the boxplot.\n\n3. **ANOVA Testing**:\n   - ANOVA (Analysis of Variance) is applied to test for significant differences in CO2 emissions across different geographical regions (`Geographical_Region`) and fuel sources (`Natural_Gas`, `Biofuel`, `Coal`).\n   - The results of the ANOVA tests, including F-values and p-values, are stored in a pandas DataFrame.\n\n4. **Bonferroni Correction for Multiple Comparisons**:\n   - Pairwise t-tests are performed with Bonferroni correction to compare CO2 emissions between different fuel sources within each geographical region (`North`, `South`, `East`, `West`).\n   - The corrected p-values are computed to adjust for multiple comparisons and are stored in a list (`bonferroni_p_values`).\n\n5. **Saving Results**:\n   - Finally, the results of the ANOVA tests (`anova_results.csv`) are saved as a CSV file， ensuring the output includes columns for `Geographical_Region`, `F_Value`, and `P_Value`.\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-051"]}}], "post_process": []}
{"id": "data-sa-054", "source": "Courses - DataCamp Learn", "instruction": "You have a Statistical thinking dataset, with details described in the README.md file. Calculate 10,000 bootstrap replicates of the variance in annual rainfall at the Sheffield Weather Station. Divide the data into 50 bins, compute the bin center and corresponding probability density function (PDF) for each bin. For convenience, convert the variance to units of square centimeters. Save the results to a file named result.csv, following the template provided in sample_result.csv. (Set the random seed to 42)\n\nStep for reference\n1.Bootstrap Helper Functions:\nDefine a function bootstrap_replicate_1d(data, func) to generate a bootstrap replicate of 1D data.\nDefine another function draw_bs_reps(data, func, size=1) to draw multiple bootstrap replicates.\n2.Data Preparation:\nRead the weather station CSV file sheffield_weather_station_cleaned.csv considering it is space-delimited and does not have a header.\nAssign appropriate column names.\nRemove the first row if it contains non-numeric data.\nConvert the year column to integers and rain column to floats, handling conversion errors gracefully and dropping any resulting NaN values.\nCompute the total annual rainfall by grouping data by year and summing the rain values for each year.\nConvert the resulting annual rainfall sums to a NumPy array.\n3.Bootstrap Analysis:\nGenerate 10,000 bootstrap replicates of the variance of annual rainfall using the draw_bs_reps function.\nAdjust the variance units if needed (e.g., put the variance in units of square centimeters).\n4.Probability Density Function (PDF):\nCreate a histogram of the bootstrap replicates with 50 bins, normalized to represent a PDF.\nCalculate the center points for each bin and the corresponding PDF values.\nStore the bin centers and PDF values in a DataFrame.\n5.Save Results。", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/data-sa-054"]}}], "post_process": []}
{"id": "plot-bar-004", "source": "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy", "instruction": "This is the Titanic dataset, with relevant descriptions in the README.md file. Your task is to plot a bar chart showing the number of male and female survivors in each passenger class. The partially completed code is provided in analysis.py. Save the plotted image as 'result.jpg'. The image should have dimensions of 14 by 12, with the title 'Sex vs Pclass Survival Comparison'. The x-axis should be labeled 'Sex' with categories 'Male' and 'Female', and the y-axis should be labeled 'Survived'. Additionally, the bars representing each class (1st, 2nd, and 3rd) should be colored as follows: \"#e9d4d0\" for 1st class, \"a16f8d\" for 2nd class, and \"#2d223a\" for 3rd class.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-004"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-005", "source": "https://www.kaggle.com/code/aakashnain/is-it-better-than-2017", "instruction": "This is the 2018 Kaggle Machine Learning & Data Science Survey dataset, with relevant descriptions in the README.md file. Your task is to draw a bar chart showing the number of people in each age group who filled out the survey. The method for dividing age groups is outlined in AgeGroup.md. Save the plotted bar chart as 'result.png'. The title of the chart should be 'Age Group Distribution', with the x-axis labeled as 'Age Group' and the y-axis labeled as 'Count'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-006", "source": "https://www.kaggle.com/code/youssefismail20/international-football-matches-1872-to-2023#Host-Countries-and-Tournaments-🏟%EF%B8%8F", "instruction": "This dataset contains international football results from 1872 to 2024, with relevant descriptions in the README.md file. Based on this dataset, you need to compile the top 10 teams by overall scores from the years 2000 to 2023 and their scores. Then, create a bar chart sorted by total score in descending order. The method to calculate the total score can be found in the 'overall_score.txt' file. The generated image should be saved as 'team.png' with a height and width of 12 and 8, respectively. The bar chart should have the title 'Best Teams from 2000 to 2023', the x-axis title 'Football Team', the x-axis labels should be the names of the teams you filtered, and the y-axis title 'Overall Score'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-007", "source": "https://www.kaggle.com/code/tahaahmedt/seattle-airbnb-listings-eda", "instruction": "This is a Seattle Airbnb Open Data dataset, with relevant descriptions in the README.md file. Based on this dataset and the calculation method recorded in 'NeighborhoodCalculate.txt', create a bar chart showing the distribution of the top 10 neighborhoods by the number of listings. Save the bar chart as 'result.png' with a height and width of 16 and 8, respectively. The chart should have the title \"Top 10 Neighborhoods by Number of Listings\", the x-axis title \"Neighborhood\", with labels for the identified neighborhoods, and the y-axis title \"Number of Listings\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-007"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-011", "source": "https://www.kaggle.com/code/hamdallak/who-is-killed-by-us-police-why-how-and-where", "instruction": "This is a Fatal Police Shootings in the US dataset, with relevant descriptions in the README.md file. Following the instructions in region.txt, count the number of people killed by police gunfire in the Western, Central, Southern, and Northeastern regions of the United States who showed no signs of mental illness. Then, plot these counts in a bar chart and save the resulting image as 'result.png'. The colors of the bars should be ['#0000FF', '#008000', '#FFA500', '#FF0000'], the title should be \"Number of Victims Killed by Gunshot without Mental Illness by Region\", the x-axis title should be \"Region\", and the y-axis title should be \"Number of Victims\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-011"]}}], "post_process": ["plot_process"]}
{"id": "plot-bar-015", "source": "https://www.kaggle.com/code/csanskriti/amazon-sales-data-analysis", "instruction": "This is an E-Commerce Sales Dataset and Amazon Sales Dataset, with relevant descriptions provided in the README.md file. Your task is to compile the sales data for products of each size sold on Amazon, sort the sizes in ascending order, and plot these figures in a bar chart. The sales should be measured in units of $10,000. Save the resulting image as result.png. The image should have a size of (12, 6), with the title 'Sales by Product Size', x-axis labeled as 'Product Size', and y-axis labeled as 'Net Revenue in 10,000 dollars'.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-bar-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-006", "source": "https://www.kaggle.com/code/muzammilbaloch/global-temperature-analysis", "instruction": "This is a Climate Change: Earth Surface Temperature Data dataset, with relevant descriptions in the README.md file. Following the instructions in tips.txt, you need to plot the global seasonal average temperature changes in a line graph and save the image as result.png. The graph should have a width and height of (16, 6), with the title \"Average temperature in each season\", the x-axis titled \"Year\", and the y-axis titled \"Average temperature\". The legend title should be \"Seasons Average Temperature\", with legend labels as \"Spring\", \"Summer\", \"Autumn\", and \"Winter\". The line colors should be set to \"#ffa500\" for Spring, \"#0000ff\" for Summer, \"#008000\" for Autumn, and \"#ff0000\" for Winter.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-006"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-014", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of three different portfolio strategies (default portfolio, equal-weight portfolio, and market value-weighted portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg. The line colors should be distinct, the plot title should be “Cumulative Returns Over Time”, the x-axis title should be “Date”, and the y-axis title should be “Cumulative Returns”.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-014"]}}], "post_process": ["plot_process"]}
{"id": "plot-line-015", "source": "Statistical tests for normality | Python (datacamp.com)", "instruction": "This is a Portfolio Risk Management dataset with descriptions in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns of four different portfolio strategies (equal-weight portfolio, market value-weighted portfolio, the portfolio with the highest Sharpe ratio, and the global minimum volatility portfolio) each day, and draw a line plot. Save the resulting plot as result.jpg with a size of (12,6). The plot title should be “Cumulative Returns Over Time”, the legend labels should be “Cumulative EW”, “Cumulative MCap”, “Cumulative MSR”, and “Cumulative GMV”, the x-axis title should be “Date”, and the y-axis title should be “Cumulative Returns”.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-line-015"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-005", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, complete the analysis.py script to count the number of games for the four major genres and plot their proportions in a pie chart. Save the chart as result.png. The chart should have a size of (12, 8), with the pie colors set to Green, Orange, Blue, and Red. Include the genre names in the legend.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-005"]}}], "post_process": ["plot_process"]}
{"id": "plot-pie-008", "source": "https://www.kaggle.com/code/brunoguedesf/projeto-eda-delivery-center", "instruction": "This is a Delivery Center: Food & Goods orders in Brazil dataset with descriptions in the README.md file. You need to find the hub city with the largest Biker Average Delivery Distance, and plot the percentage of orders delivered by driver modal for that city as a pie chart. Save the resulting image as result.jpg. The pie chart colors should be [”#ffdb00”, “#ffb600”], the image size should be (8,8), and the image title should be “Percentage of orders delivered by type of driver in CURITIBA”", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-pie-008"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-002", "source": "https://www.kaggle.com/code/fahadrehman07/data-science-job-salary-prediction-glassdoor", "instruction": "This is a Data Science Jobs Salaries Dataset, with relevant descriptions provided in the README.md file. You are tasked with creating a scatter plot to visualize the relationship between observed probabilities and predicted probabilities of football match outcomes, based on parsing a CSV file compressed in gzip format following the instructions in 'guidance.txt'. Please generate the plot and save the result as 'result.png'. The x-axis should be labeled as \"estimated prob\", the y-axis as \"observed prob\", and the scatter plot labels should be \"home victory\", \"draw\", and \"away victory\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-002"]}}], "post_process": ["plot_process"]}
{"id": "plot-scatter-004", "source": "https://www.kaggle.com/code/hkhoi91/data-viz-what-to-focus-on-as-a-game-dev", "instruction": "This is a 17K Mobile Strategy Game dataset with a description in README.md. Based on the instructions in the tips.txt file, create a scatter plot showing the relationship between the time since the release date and the user rating count. Save the plot as result.png. The plot should have a size of (16, 6), with the scatter points colored \"blue\". Label the x-axis as \"Updated version date since release (days)\" and the y-axis as \"User Rating count\".", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/plot-scatter-004"]}}], "post_process": ["plot_process"]}
