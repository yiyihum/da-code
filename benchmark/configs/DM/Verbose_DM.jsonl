{"id": "dm-csv-007", "source": "https://drive.google.com/drive/folders/1wVs85eVx4KYhN0qa-eLjlNEBSLEoJGQc?usp=share_link", "instruction": "This is a TMDB 5000 Movie Dataset with a description in README.md. According to the guidance in ‘wrFormula.tex’, you should identify the top 10 most popular movies and record their name in result.csv following the template provided in sample_result.csv.\n\n[Verbose]\n\n1. **Data Loading**:\n   - Use the `pandas` library to load two CSV files: one containing movie details (`tmdb_5000_movies.csv`) and another containing movie credits (`tmdb_5000_credits.csv`).\n\n2. **Data Merging**:\n   - Merge the two datasets based on the movie ID (`id` and `movie_id`) to combine movie details with credits.\n\n3. **Weighted Rating Calculation**:\n   - Define the `weightedRating` function to calculate the weighted rating for each movie using the formula:\n     \\[\n     \\text{Weighted Rating} = \\frac{(v \\times R) + (m \\times C)}{v + m}\n     \\]\n     where `v` is the number of votes, `R` is the average rating, `m` is the vote count threshold (percentile), and `C` is the mean vote average.\n\n4. **Top-Rated Movies Retrieval**:\n   - Define the `getTopRatedMovies` function to retrieve top-rated movies based on optional filters (genre, verdict) and a specified percentile for vote count (`percentile=0.85`):\n     - **Filtering**: Apply genre and verdict filters if specified.\n     - **Mean and Quantile Calculation**: Compute the mean vote average (`C`) and the vote count threshold (`m`) at the specified percentile.\n     - **Data Selection**: Select movies with a vote count greater than or equal to `m`.\n     - **Weighted Rating Calculation**: Compute the weighted rating for each selected movie.\n     - **Sorting**: Sort movies by their weighted rating in descending order.\n     - **Return**: Return the top 10 movies based on the weighted rating.\n\n5. **Result Saving**:\n   - Obtain the top-rated movies without specifying genre or verdict.\n   - Write the results to a csv file (`result.csv`), with the column named \"Moive\"", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-007"]}}], "post_process": []}
{"id": "dm-csv-009", "source": "https://drive.google.com/drive/folders/14793kyfMJ5GRbe5djEr0uzvQwXkwr-JM?usp=share_link", "instruction": "This is an Amazon Books Dataset with a description in README.md. Following the guidance provided in guidance.txt, identify the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors. Record the results in author.csv following the template provided in sample_result.csv.\n\n[Verbose]\n1.Review the provided three datasets. You can refer to the README.md for an introduction to these datasets and their columns.\n2.Import the datasets and remove unnecessary columns. For the price column, transform its data into a handleable type.\n3.Fill the missing values in the author column with \"Unknown\".\n4.Group the data by 'Author' to calculate the following metrics:\n5.Average rating of each author's books.\n6.Average price of each author's books.\n7.Total number of ratings accumulated by each author.\n8.Average number of ratings per book for each author.\n9.Set a minimum threshold for the number of average ratings per book to ensure reliability. In this case, the threshold is set as 1000.\n10.According to the definitions of the three types of authors in README.md, count the top 10 Best-Rated Authors, Most Expensive Authors, and Most Rated Authors.\n11.Refer to the format defined in sample_result.csv and output the results to the specified file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-009"]}}], "post_process": []}
{"id": "dm-csv-010", "source": "https://drive.google.com/drive/folders/1V339AXpqR-f1DZK9wJNd0CN_svQf-I7o?usp=sharing", "instruction": "This is a Bike Store Relational Database, with related descriptions provided in the README.md file. You need to calculate the monthly average sales volume for each bike category and write the results into avg_units_sold.csv following the template of sample_result.csv.\n\n[Verbose]\n1. Read every csv files and sql.yml which indicates the relationships between different tables, which may help you to understand them.\n2. Establish a connection to a SQLite database using sqlite3.\n3. Insert the data from each DataFrame into corresponding tables in the database.\n4. Then start a SQL Query. Firstly, join tables relative with products table to get product categories.\n5. Group the sales data by year, month, and product to calculate the total units sold for each product in each month.\n6. Join the sales data with product categories data to group the information by month and category.\n7. Calculate the average units sold for each category in each month.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-010"]}}], "post_process": []}
{"id": "dm-csv-011", "source": "https://drive.google.com/drive/folders/1tO4GVH2DYJJwF2-HmSf3zTEzOUopthwH?usp=sharing", "instruction": "This is a Bike Store Relational Database, with relevant descriptions provided in the README.md file. You need to calculate and aggregate the total quantity sold and total sales revenue for each bike type from 2016 to 2018. Finally, write the results into result.csv following the template of sample_result.csv.\n\n[Verbose]\n1. Read every csv files and sql.yml which indicates the relationships between different tables, which may help you to understand them.\n2. Establish a connection to a SQLite database using sqlite3.\n3. Insert the data from each DataFrame into corresponding tables in the database.\n4. Create a temporary table , join tables relative with product names, model years, brand names, and category names.\n5. Join temporary table with order items, select fields such as `quantity`, `list_price`, `discount` and so on.\n6. Calculate the total_quantity sold and total_price for each category by multiplying list_price with (1 - discount).", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-011"]}}], "post_process": []}
{"id": "dm-csv-015", "source": "https://drive.google.com/drive/folders/1yAz4jIJ4nuBf8qKEvzPYHrzja9RBMdek?usp=share_link", "instruction": "This is a dataset of baseball games, with relevant descriptions provided in the README.md file. You need to help me calculate the player with the most games played, the player with the most runs scored, the player with the most hits, and the player with the most home runs. Each statistical data should include the player’s name and their corresponding data; for example, for the player with the most games played, record their number of games played, and for the player with the most runs scored, record their runs scored. Finally, write the results into result.csv following the template of sample_result.csv.\n\n[Verbose]\n1. **Data Loading**:\n   - Use the `sqlite3` library to connect to a SQLite database and the `pandas` library to load multiple tables from the database into DataFrame objects.\n   - The tables loaded include `all_star`, `appearances`, `batting`, `batting_postseason`, `college`, `fielding`, `fielding_outfield`, `fielding_postseason`, `hall_of_fame`, `home_game`, `manager`, `manager_award`, `manager_award_vote`, `manager_half`, `player`, and `park`.\n\n2. **SQL Query Execution**:\n   - Construct and execute a SQL query to retrieve specific batting statistics aggregated by player.\n   - The query includes:\n     - The player with the most games played.\n     - The player with the most runs.\n     - The player with the most hits.\n     - The player with the most home runs.\n   - The query uses subqueries to calculate the required statistics for each player by joining the `player` and `batting` tables and grouping the results by `player_id`.\n\n3. **Result Saving**:\n   - Convert the SQL query result into a pandas DataFrame.\n   - Save the DataFrame to a CSV file named `result.csv` without including the DataFrame index.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-015"]}}], "post_process": []}
{"id": "dm-csv-016", "source": "https://drive.google.com/drive/folders/1Nfx9mAWRZzmY6e3GPrIb2oAo2JSXfsIs?usp=share_link", "instruction": "This is a dataset for football matches, with relevant descriptions provided in the README.md file. I would like you to help me compile statistics for the seasons and the corresponding winning clubs hosted by `IT1` and `BESC`. Finally, write the results into result.csv following the template of sample_result.csv.\n\n[Verbose]\n1. **Data Loading**:\n   - Use `pandas` to read data from CSV files into DataFrame objects: `competitions`, `games`, `club_games`, `appearances`, and `clubs`.\n\n2. **Data Preparation**:\n   - **Finals DataFrame**: Filter the `games` DataFrame to include only the finals.\n   - **Matchdays DataFrame**: Filter the `games` DataFrame to include only matchdays, extract the round number, and group by `competition_id` and `season` to find the last matchday.\n   - **Finals Grouping**: Group the `finals` DataFrame by `competition_id`, `season`, and `round`, and sum the home club goals.\n   - **Combine DataFrames**: Combine `finals` and `last_matchday` DataFrames, remove duplicates, and reset the index to create a `competition` DataFrame.\n\n3. **Defining League Champions Function**:\n   - Define `league_champions` function to determine the league champion based on club position:\n     - Use `pandasql` to execute SQL queries on DataFrames.\n     - Select the club that finished in the first position at the end of the season.\n\n4. **Defining Cup Champions Function**:\n   - Define `cup_champions` function to determine the cup champion based on the final match result:\n     - Use `pandasql` to execute SQL queries on DataFrames.\n     - Select the club with more goals in the final match or mark it as a draw if goals are equal.\n\n5. **Defining General Champions Function**:\n   - Define `champions` function to determine the champion for either league or cup competitions:\n     - Identify if the competition is a league or a cup using the `competition` DataFrame.\n     - Call `league_champions` if the competition is a league.\n     - Call `cup_champions` if the competition is a cup.\n     - Return an error message if the competition ID is not found.\n\n6. **Getting Champions Data**:\n   - Get the champions for the Italian league (\"IT1\") and another league (\"BESC\").\n   - Add a column to each DataFrame indicating the league.\n\n7. **Saving Results**:\n   - Combine the champions DataFrames and save the result to a CSV file named `result.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-016"]}}], "post_process": []}
{"id": "dm-csv-025", "source": "https://drive.google.com/drive/folders/1_ZmzR5S8YNzsl9HhK1YX06y91zoX-xLR?usp=share_link", "instruction": "This is an IPL Complete Dataset (2008-2024) with relevant descriptions provided in the README.md file. Your task is to continue the development of the existing code in analys.py to calculate the winning and losing probabilities for each team according to the instructions in tips.txt. Afterwards, you need to identify the team with the highest probability of winning and the team with the highest probability of losing. Finally, write their names and corresponding (winning or losing) probabilities into team_probabilities.csv following the template provided in sample_result.csv.\n\n\n[Verbose]\n1. **Data Loading**:\n   - Read data from CSV files\n\n2. **Data Cleaning**:\n   - Rename instances of \"Rising Pune Supergiant\" to \"Rising Pune Supergiants\" in the `match_data` DataFrame for consistency.\n\n3. **Winning Probability Calculation**:\n   - Define the `winning_probability` function to calculate the winning and losing probabilities for a given team:\n     - Extract data for matches involving the given team.\n     - Calculate the total number of matches played, won, and lost by the team.\n     - Compute the winning and losing probabilities as follows:\n       \\[\n       \\text{Winning Probability} = \\frac{\\text{Total Matches Won}}{\\text{Total Matches Played}}\n       \\]\n       \\[\n       \\text{Losing Probability} = \\frac{\\text{Total Matches Lost}}{\\text{Total Matches Played}}\n       \\]\n\n4. **Probability Distribution Calculation**:\n   - Iterate over all IPL teams to calculate and store their winning and losing probabilities in dictionaries (`winning_probability_map` and `losing_probability_map`).\n\n5. **Data Sorting and Saving**:\n   - Sort the winning and losing probabilities in descending order.\n   - Convert the sorted lists to numpy arrays and save them as `.npy` files.\n\n6. **Identify Teams with Extreme Probabilities**:\n   - Find the team with the highest winning probability.\n   - Find the team with the highest losing probability.\n   - Print the results.\n\n7. **Save Results to CSV**:\n   - Create a DataFrame with the teams having the highest winning and losing probabilities along with their probabilities and types (winning/losing).\n   - Save this DataFrame to a CSV file named `team_probabilities.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-025"]}}], "post_process": []}
{"id": "dm-csv-026", "source": "https://drive.google.com/drive/folders/1MNmqFKeUQR7hpgVxc7pPJOcymQa35K9p?usp=share_link", "instruction": "This is a dataset containing information on FIFA 22 players, with detailed descriptions provided in the README.md file. Your task is to complete the analysis.py script to rank the players based on the specified score at the position level from 2015 to 2022. Relevant hints can be found in some text (txt) and Markdown (md) files. Ensure that the results are written to the result.csv file in the format demonstrated in sample_result.csv.\n\n[Verbose]\n1. **Data Loading**:\n   - Read CSV files for male and female players for the years 2016 to 2022 into separate dictionaries using pandas.\n\n2. **Database Setup**:\n   - Create an SQLite in-memory database using SQLAlchemy.\n   - Save each year's data as separate tables in the SQLite database for both male and female players.\n\n3. **Data Union**:\n   - Combine all player data into a single dataset (`raw_data`) by unioning all year tables for both genders, adding columns for gender and year.\n\n4. **Position Mapping**:\n   - Create a mapping of player positions to full position names, forming a new dataset (`player_full_position`).\n\n5. **Data Preparation**:\n   - Join the `raw_data` with `player_full_position` to get a comprehensive dataset (`raw_data2`) that includes full position names and various player attributes.\n\n6. **Role Level Score Calculation**:\n   - Compute a `role_level_score` for each player based on their position and relevant attributes, creating another dataset (`raw_data3`).\n\n7. **Role Level Percentage Calculation**:\n   - Calculate the `role_level_percentage` by normalizing the `role_level_score` against the maximum possible score for each position.\n\n8. **Filter and Export**:\n   - Filter out substitute players and export the top 20 rows of the resulting dataset to a CSV file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-026"]}}], "post_process": []}
{"id": "dm-csv-027", "source": "https://drive.google.com/drive/folders/1bzqJXG2xjnXjbzZdW8UYyH7MYVF0ZHsw?usp=share_link", "instruction": "This is a Netflix Data dataset, with detailed descriptions provided in the README.md file. Your task is to identify the top 10 countries by the number of produced shows and their corresponding counts, as well as the top 10 movie genres by the number of occurrences and their corresponding counts. Record the results in the provided templates: Top_10_countries.csv and Top_10_Movies.csv.\n\n[Verbose]\n1. Identify Top 10 Countries:\n   - Identify the top 10 countries by the number of produced shows\n   - List the corresponding counts for each country\n\n2. Identify Top 10 Movie Genres:\n   - Identify the top 10 movie genres by the number of occurrences\n   - List the corresponding counts for each genre\n\n3. Output Results to CSV:\n   - Record the top 10 countries and their counts in Top_10_countries.csv\n   - Record the top 10 movie genres and their counts in Top_10_Movies.csv\n```​⬤", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-027"]}}], "post_process": []}
{"id": "dm-csv-029", "source": "https://drive.google.com/drive/folders/1ibfAui8NvdVlZTQGW5MQab57BKPQeAH_?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the top 3 users with the most pull requests and record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template.\n\n[Verbose]\n1. Identify Top Users:\n   - Identify the top 3 users with the most pull requests\n\n2. Count Pull Requests:\n   - Count the number of pull requests made by these top 3 users\n\n3. Output Results to CSV:\n   - Record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-029"]}}], "post_process": []}
{"id": "dm-csv-030", "source": "https://drive.google.com/drive/folders/1Wyg0lPtEKqh3F_sZOnR7wNZFb2iW7yt8?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the latest 10 pull requests, merge the related file data, and find the unique files involved in these pull requests. Record the results in the unique_files.csv file using the sample_result.csv template.\n\n[Verbose]\n1. Identify Latest 10 Pull Requests:\n   - Identify the latest 10 pull requests\n\n2. Merge Related File Data:\n   - Merge the related file data for these pull requests\n\n3. Find Unique Files:\n   - Find the unique files involved in these pull requests\n\n4. Output Results to CSV:\n   - Record the results in the unique_files.csv file using the sample_result.csv template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-030"]}}], "post_process": []}
{"id": "dm-csv-031", "source": "https://drive.google.com/drive/folders/1evC6AsX-2sevVmpbE5nlpqrfO1LZgmUJ", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the users who made the last 6 pull requests for the specific file \"src/compiler/scala/reflect/reify/phases/Calculate.scala\". Record the results in the users_last_6.csv file using the provided template.\n\n[Verbose]\n1. Identify Target File:\n   - Define the file of interest\n\n2. Extract Relevant Pull Request Data:\n   - Filter the pull request data to include only entries for the specified file\n\n3. Identify Last Six Users:\n   - Identify the last six users who made pull requests for the specified file\n\n4. Output Results to CSV:\n   - Write the results to a CSV file named users_last_6.csv", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-031"]}}], "post_process": []}
{"id": "dm-csv-033", "source": "https://drive.google.com/drive/folders/13jdtj7v3Ie4Hj9aasZiNjQUc46PE9eH3?usp=sharing", "instruction": "This is a dataset titled “Mobile-Games-Testing-with-Cookie-Cats,” with detailed descriptions provided in the README.md file. Your task is to compare the 1-day retention rates between two versions of a game (gate_30 and gate_40) according to the instructions in guidance.txt. Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 and record the result in the probability_greater_retention.csv file using the provided template.\n\n[Verbose]\n1. Calculate 1-Day Retention Rates:\n   - Calculate the 1-day retention rates for both gate_30 and gate_40 versions\n\n2. Compare Retention Rates:\n   - Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 compared to gate_40\n\n3. Output Results to CSV:\n   - Write the result in the probability_greater_retention.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-033"]}}], "post_process": []}
{"id": "dm-csv-034", "source": "https://drive.google.com/drive/folders/19EUef4q8aewkLlRDndOMM55OPCCBsFmo?usp=share_link", "instruction": "This is a dataset titled “Mobile-Games-Testing-with-Cookie-Cats,” with detailed descriptions provided in the README.md file. Your task is to calculate the proportion of 7-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_7_by_version.csv file using the provided template.\n\n[Verbose]\n1. Calculate Proportion of 7-Day Retention:\n   - Calculate the proportion of 7-day retention being True for both gate_30 and gate_40 versions\n\n2. Record Results by Version:\n   - Record the results separately for the gate_30 and gate_40 versions\n\n3. Output Results to CSV:\n   - Write the results in the retention_7_by_version.csv file using the provided template", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-034"]}}], "post_process": []}
{"id": "dm-csv-036", "source": "https://drive.google.com/drive/folders/1o7QlFnyB-_RcHE9Kp7Jfa3VPu1Yf72-S?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the last six users who made pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala`. and fill in their usernames in the template file users_last_6.csv.\n\n[Verbose]\n1. Identify Target File:\n   Define the file of interest.\n\n2. Extract Relevant Pull Request Data:\n   Filter the pull request data to include only entries for the specified file.\n\n3. Identify Last Six Users:\n   Identify the last six users who made pull requests for the specified file.\n\n4. Output Results to CSV:\n   Write the results to a CSV file named `users_last_6.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-036"]}}], "post_process": []}
{"id": "dm-csv-037", "source": "https://drive.google.com/drive/folders/1G-zP7Vu07sIpX4mw598XqS2v8muQXRtf?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. Your task is to identify the top three users who made the most pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala` and fill in their usernames in the template file top_3_developers.csv.\n\n[Verbose]\n\n\n1. Identify Target File: Define the file of interest.\n2. Extract Relevant Pull Request Data: Filter the pull request data to include only entries for the specified file.\n3. Identify Top Developers: Identify the top three users who made the most pull requests for the specified file.\n4. Output Results to CSV: Write the results to a CSV file named `top_3_developers.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-037"]}}], "post_process": []}
{"id": "dm-csv-038", "source": "https://drive.google.com/drive/folders/1X9N6aLhTfXbic4dgXAt0e8G2x8TaTHir?usp=share_link", "instruction": "This is a dataset titled “The-GitHub-History-of-the-Scala-Language,” with detailed descriptions provided in the README.md file. I want to know the number of pull requests made by two users with the nicknames “soc” and “xeno-by” for each year between 2011 and 2016. Please fill in your statistical results in the provided template file pull_requests_by_year_and_author.csv.\n\n[Verbose]\n\n\n1. Identify Authors: Define the users of interest.\n2. Extract Relevant Pull Request Data: Filter the pull request data to include only entries for the specified authors and years (2011-2016).\n3. Filter Data by Year: Ensure the pull request data includes only the years 2011 to 2016.\n4. Count Pull Requests: Count the number of pull requests made by \"xeno-by\" and \"soc\" for each year from 2011 to 2016.\n5. Output Results to CSV: Write the results to a CSV file named `pull_requests_by_year_and_author_and_file.csv`.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-038"]}}], "post_process": []}
{"id": "dm-csv-039", "source": "https://drive.google.com/drive/folders/16Hk8i_1SqBwYWI3sp-hp4rMAgyynqsjk?usp=share_link", "instruction": "This is an Avocado Toast dataset, with detailed descriptions provided in the README.md file. You need to conduct a supply chain analysis of three ingredients used in an avocado toast according to the instructions in tips.md, utilizing the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file using the provided template.\n\n[Verbose]\n 1. Read in the avocado data\n- Read the avocado data from a tab-delimited CSV file.\n- Subset the DataFrame to include only a smaller number of relevant columns.\n- Read in the relevant category tags for avocados from a text file.\n\n 2. Filter avocado data using relevant category tags\n- Drop rows with null values in the `categories_tags` column.\n- Convert the `categories_tags` column from comma-separated strings to lists.\n- Filter the DataFrame to keep only rows with relevant category tags.\n\n 3. Determine the top origin country for UK avocados\n- Filter the avocado DataFrame for rows where `countries` equals \"United Kingdom.\"\n- Count and order the unique values in the `origins_tags` column.\n- Identify the top country of origin for avocados in the UK.\n- Clean up the country string to remove any leading characters or hyphens.\n\n 4. Create a user-defined function for ingredient analysis\n- Create a function called `read_and_filter_data()` that:\n  - Takes a filename and a list of relevant categories as arguments.\n  - Performs the same steps as above to read, subset, filter, and analyze the data.\n  - Returns the top country of origin for the ingredient.\n\n 5. Analyze other ingredients\n- Use the relevant categories data to determine the top origin countries for olive oil and sourdough by calling the `read_and_filter_data()` function.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-039"]}}], "post_process": []}
{"id": "dm-csv-043", "source": "https://drive.google.com/drive/folders/1izUKLoSM66ofPI6CV_l4_gzhM3uIZI5N?usp=share_link", "instruction": "You are provided with a Customer Segmation dataset, and detailed descriptions can be found in the README.md file. Your task is to calculate the retention data for each cohort according to the instructions provided in the tips.txt file. Save the calculated results to a file named retention.csv.\n\n[Verbose]\n1. Extract  year, month, and day from InvoiceDate, and store the result in a new column called InvoiceDay.\n2. Group the data by CustomerID and select the InvoiceDay value.\n3. Assign the minimum InvoiceDay value for each customer to a new column named CohortDay.\n4. Get the month of  CohortDay column, and save the result to CohortMonth.\n5. Calculate the number of months between InvoiceDay and CohortMonth and store in a new column named CohortIndex.\n6. Group by the data by CohortMonth and CohortIndex.\n7. Within each group, count the number of unique CustomerID values, reset the index of the resulting DataFrame and store it in cohort_data.\n8. Create a pivot with monthly cohort in the index, cohort index in the columns and the customer ID in the values.\n9. Select the first column and store it to cohort_sizes. Divide the cohort count by cohort sizes along the rows.Create a Pivot Table:\n", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-043"]}}], "post_process": []}
{"id": "dm-csv-044", "source": "https://drive.google.com/drive/folders/1mgjfhA3CF-Ksq7U2WHLCXspVV-RJNWvP?usp=share_link", "instruction": "This is a Customer Segmation dataset, and detailed descriptions are provided in the README.md file. Your task is to calculate the average unit price paid for items by groups of customers over time. Each group should be determined by the month of their first purchase. Save the calculated results in a file named average_price.csv, utilizing the provided template file.\n\n[Verbose]\n1. Extract  year, month, and day from InvoiceDate, and store the result in a new column called InvoiceDay.\n2. Group the data by CustomerID and select the InvoiceDay value.\n3. Assign the minimum InvoiceDay value for each customer to a new column named CohortDay.\n4. Get the month of  CohortDay column, and save the result to CohortMonth.\n5. Calculate the number of months between InvoiceDay and CohortMonth and store in a new column named CohortIndex.\n6. Group the data by CohortMonth and CohortIndex.\n7. Calculate the average UnitPrice for each cohort.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-044"]}}], "post_process": []}
{"id": "dm-csv-050", "source": "https://drive.google.com/drive/folders/1DQdtNaN-K38D6s5WMpqkGUUW1n9mdkTX?usp=share_link", "instruction": "This is a Portfolio Risk Management dataset, with detailed descriptions available in the README.md file. Using the 2017 stock return datasets for the 9 biggest companies, you need to calculate the cumulative returns of three different portfolio strategies: the default portfolio, the equal-weight portfolio, and the market value-weighted portfolio. Write the results (rounded to three significant figures) into result.csv according to the template provided in sample_result.csv.\n\n[Verbose]\n1. Load the corresponding csv files.\n2. Define portfolio weights with the given weights for each of the 9 stocks in README.md.\n3. Calculate weighted stock returns: multiply the StockReturns by portfolio weights defined in last step.\n4. Sum the WeightedReturns of same day, calculate the cumulative returns of it.\n5. Calculate the cumulative returns of equal-weight portfolio and market value-weighted portfolio, the market capitalizations of the companies is given in README.md.\n6. Calulate the ratio of market cap of the company to the total market cap of all companies, multipy weights ratios and areturns to calculate the market capitalization weighted portfolio returns.\n7. Merge the cumulative returns of three different portfolio strategies.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-050"]}}], "post_process": []}
{"id": "dm-csv-052", "source": "https://drive.google.com/drive/folders/1fLUJMT9WE5_NbHjnlMG_gvgejo8TrhlL?usp=share_link", "instruction": "This is a customer segmentation dataset, with detailed descriptions available in the README.md file. Following the instructions in tips.md, use a 1-4 scale to calculate the RFM Score and complete the segmentation of users (dividing users into three groups based on their RFM Score). Write the segmentation results into result.csv according to the format provided in sample_result.csv.\n\n[Verbose]\n1. Group the data by CustomerID and perform the following aggregations, calculate following variables:\nRecency: Calculate the number of days since the last purchase for each customer.\nFrequency: Count the number of purchases (invoices) for each customer.\nMonetary Value: Sum the total amount spent by each customer across all purchases.\n2. Define labels for Recency and Frequency to categorize customers into different groups based on their values.\n3. Group Recency and Frequency values into four equal percentiles, assign the labels defined earlier.\n4. Define labels for MonetaryValue to categorize customers into different groups based on their spending.\n5. Group MonetaryValue into four equal percentiles, assign the labels defined earlier.\n6. Calculate the RFM score by summing the values of labels of Recency, Frequency, and Monetary.\n7. Define RFM level based on its value:\n'Top' for scores 10 or higher.\n'Middle' for scores between 6 (inclusive) and 10.\n'Low' for scores less than 6.\n8. Calculate RFM level of each customer, and save the result to output file.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-052"]}}], "post_process": []}
{"id": "dm-csv-059", "source": "https://drive.google.com/drive/folders/1t1cWSI9zD7hKexuPQtIBxd2fNsXYAQlb?usp=share_link", "instruction": "This is a Customer Analytics dataset, with detailed descriptions available in the README.md file. Based on the registration dates in result.csv, you need to analyze the purchasing behavior of users within the first week of their registration date. Calculate the average number of purchases made in the first week on a daily basis, and then fill the results into result.csv.\n\n[Verbose]\n1.Read the CSV Files:\nLoad the CSV files in the dataset. If necessary, organize the data into a single DataFrame by considering the relationships between the columns.\n2.Calculate the Number of Purchases in the First Week:\nAnalyze the data based on the user's registration time and their purchase information. Count the number of purchases made by each user during their first week of registration.\n3.Calculate the Average Daily Purchases in the First Week:\nCompute the average number of purchases made by each user per day during their first week of registration.", "hints": null, "hardness": "Hard", "config": [{"type": "copy_all_subfiles", "parameters": {"dirs": ["./benchmark/source/dm-csv-059"]}}], "post_process": []}
