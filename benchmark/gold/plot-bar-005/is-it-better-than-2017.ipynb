{"cells":[{"metadata":{"_uuid":"115520005bc986a298b7650e974a5e86562d3575"},"cell_type":"markdown","source":"**Welcome** to another Data Science Survey by Kaggle.  I must say that when I was filling up the survey form, I got very excited. The questions were very much focused on different aspects as usual. Although I am expecting things to be in a better state as compared to the previous survey done in 2017, only data speaks the truth! \n \nI am going to analyze each question to the depth it demands. This is going to be a huge notebook with lots of code in it. If you are up for learning something new, sit tight and read thoroughly. Let's deep dive into the results and check what the results have to say!\n\n![yay](http://media.giphy.com/media/3o6fIZP7rywPQyEV44/giphy.gif)"},{"metadata":{"_uuid":"bfaf5a9dd4ade64d4c12c4d6533517dce2c40834"},"cell_type":"markdown","source":"<a id=\"contents\"></a>\n# Contents\n1. [Import libraries](#imports).\n2. [Load the Dataset](#dataloading) <br>\n3. [Some handy-dandy fucntions](#customfunctions)<br>\n4. Analysis <br>\n     4.1 [Gender Diversity](#diversity)<br>\n     4.2 [Age group distribution](#age)<br>\n     4.3 [Gender distribution in different age groups](#age-gender)<br>\n     4.4 [Country-wise distribution](#country)<br>\n     4.5 [Country-wise gender distribution](#country-gender)<br>\n     4.6 [Formal level of Education](#education)<br>\n     4.7 [Current Role](#job-title)<br>\n     4.8 [Experience in current role](#experience)<br>\n     4.9 [Experience in top roles](#roles-experience)<br>\n     4.10 [Yearly Compensation](#salary)<br>\n     4.11 [Do businesses really want Machine Learning?](#business)<br>\n     4.12 [Programming language preference](#proglang)<br>\n     4.13 [Which programming language beginners/aspiring data scientist should learn first?](#advise)<br>\n     4.14 [Most used ML libraries](#libraries)<br>\n     4.15 [Popular visualization libraries and tools](#vis)<br>\n     4.16 [How much data science people code actively?](#coding)<br>\n     4.17 [Writing code to do data analysis?](#data-analysis)<br>\n     4.18 [How much time people have spent on ML?](#ml_methods_hist)<br>\n     4.19 [Do people really think of themselves as data scientist?](#dataScientist)<br>\n     4.20 [What types of data do people deal with?](#datasets-type)<br>\n     4.21 [Learning platforms](#learningplatform)<br>\n     4.22 [Expertise in Data Science](#expertise_DS)<br>\n     4.23 [Do people care about bias in datasets/algorithms?](#fairness)<br>\n     4.24 [Time spent exploring model insights](#modelinsights)<br>\n     4.25 [Are ML models black boxes?](#explainableML)<br>\n     4.26 [Which IDE do people prefer for their work?](#ide)<br>\n     4.27 [Popular hosted notebooks](#notebooks)<br>\n     4.28 [Public Datasets](#datasetsfinder)<br>\n     4.29 [Time spent on different stages in a Data Science project](#timespentondiffaspects)<br>\n     4.30 [Popular Relational Databases](#databases)<br>\n     4.31 [What are all the machine learning products people have used?](#ml_products)<br>\n     4.32 [Barriers in reproudcibility](#barriersreprod)<br>\n     4.33 [Conclusion](#conclusion)<br>"},{"metadata":{"_uuid":"416675243780fc326e0f05d62aed0b88c15a00a6"},"cell_type":"markdown","source":"<a id='Imports'></a>\n### Import the required libraries"},{"metadata":{"trusted":true,"_uuid":"b0bede14e2eb34ee496c0f14e97a8003dc1cfa88"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport re\nimport math\nimport glob\nimport itertools\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected=True)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# For plotting within the notebook\n%matplotlib inline\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg'\n\ncolor = sns.color_palette()\n\n# For REPRODUCIBILITY\nseed = 111\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b547c614a409b9ccb4ba62bb7c081c09d52d034f"},"cell_type":"markdown","source":"<a id=\"dataloading\"></a>\n## 2. Load the dataset"},{"metadata":{"trusted":true,"_uuid":"e5ceff4a4db493c9f412bc5769c71a25b1de0cbb"},"cell_type":"code","source":"# Define input path\ninput_dir = Path('../input/')\n\n# Read the csvs\nsurvey_schema = pd.read_csv(input_dir/ 'SurveySchema.csv')\nfreeFormResp = pd.read_csv(input_dir/ 'freeFormResponses.csv')\nmultiChoiceResp = pd.read_csv(input_dir/'multipleChoiceResponses.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d9218c191a54318015d15f126bf0b4c78ff7b4"},"cell_type":"code","source":"print(f\"Total number of responses: {len(multiChoiceResp)}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e7c828149dbf1b1341c41500a3d03ca062c2fa8"},"cell_type":"markdown","source":"Sneak peek at the data "},{"metadata":{"trusted":true,"_uuid":"8d39ad383347bd8518716645e86c707b3c021f5d","scrolled":true},"cell_type":"code","source":"# Check the schema first\nsurvey_schema.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"590c5447d36ac206499f36135fe2f8294519706e"},"cell_type":"code","source":"multiChoiceResp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"593e2349819fe2b4b15c7e73f987cb2b58fb8335"},"cell_type":"markdown","source":"<a id=\"customfunctions\"></a>\n### 3. Some handy-dandy functions \n In this notebook, I will demonstrate:<br>\n**1)** How to visualize things in the most simple way?<br>\n**2)** How can you tweak the plots to put more information in a way you want? (This is an interesting hack.)<br>\n**3)** How can you do same thing in a better way?<br>\n**4)** How to write clean and optimized code?<br>\n\nFor example, for one question I will show that how can you make a simple bar chart. In the next question, I will demonstrate how to the percentage of bar charts in seaborn. In the next question, I will show how can you achieve the same thing with interactive visualization libraries. **Pay attention to the code if you are up for learning hundreds of new things today**\n\nLet us define some custom functions that can be used later."},{"metadata":{"trusted":true,"_uuid":"5cd1436f476560b87d6aad64c81793994f0de989"},"cell_type":"code","source":"# A handy dandy function for making a bar plot. You can make it as flexible as much as you want!!\ndef do_barplot(df, \n               figsize=(20,8), \n               plt_title=None, \n               xlabel=None, \n               ylabel=None, \n               title_fontsize=20, \n               fontsize=16, \n               orient='v', \n               clr_code=None, \n               max_counts=None,\n               print_pct=True,\n               normalize=False,\n               rotation=None):\n    \"\"\"\n    This function can be used to make a barplot from a pandas dataframe very quickly. It counts the number of instances\n    per category and plot all the values on a barchart. The barchart can be made to represent count or in terms of \n    percentages. \n    \n    Arguments:\n    df: pandas dataframe used for this plot\n    figsize: size of the plot\n    plt_title: title of the plot\n    xlabel: label on X-axis\n    ylabel: label on Y-axis\n    title_fontsize = fontsize for title\n    fontsize: fontsize for x and y labels\n    orient: orientation of the plot 'h' or 'v'\n    clr_code: color code for seaborn color paelette\n    max_counts: limit the number of labels to de displayed\n    print_pct: whether to print the count values for each category\n    normalize: whether to print percentage instead of raw counts\n    rotation: rotation value for ticks\n    \n    \"\"\"\n    \n    # Get the value counts \n    if normalize:\n        df_counts = round(df.value_counts(normalize=normalize)*100,2)\n    else:\n        df_counts = df.value_counts()\n        \n    total = df.shape[0]\n    \n    # If there are too many values, limit the amount of information for display purpose\n    if max_counts:\n        df_counts = df_counts[:max_counts]\n    \n    # Print the values along with their counts and overall %age\n    if print_pct and not normalize:\n        for i, idx in enumerate(df_counts.index):\n            val = df_counts.values[i]\n            percentage = round((val/total)*100, 2)\n            print(f\"{str(idx).ljust(25)}  {val} or roughly {percentage}%\")\n    \n    # Plot the results \n    plt.figure(figsize=figsize)\n    \n    if clr_code is None:\n        clr_code = np.random.randint(6)\n\n    if orient=='h':\n        sns.barplot(y=df_counts.index, x=df_counts.values, orient='h', color=color[clr_code])\n    else:\n        sns.barplot(x=df_counts.index, y=df_counts.values, orient='v', color=color[clr_code])\n            \n    plt.title(plt_title, fontsize=title_fontsize)\n    plt.ylabel(ylabel, fontsize=fontsize)\n    plt.xlabel(xlabel, fontsize=fontsize)\n    \n    if orient=='h':\n        plt.yticks(range(len(df_counts.index)), df_counts.index)\n    else:\n        plt.xticks(range(len(df_counts.index)), df_counts.index, rotation=rotation)\n    plt.show()\n    del df_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82ba6310290d4ffccda4251f48413e3dd42aa482"},"cell_type":"code","source":"def pct_on_bars(axs, df, offset=50, orientation='v', adjustment=2, pos='center', prec=1, fontsize=10):\n    \"\"\"\n    This function can be used to plot percentage on each bar in a barplot. The function assumes\n    that for each value on an axis, there is only one corresponding bar. So, if you have plotted something with\n    hue, then you should consider using something else\n    \n    Arguments:\n    axs: Matplotlib axis\n    df: pandas dataframe used for this plot\n    offset: Relative position of the text w.r.t the bar\n    orientation: 'h' or 'v'\n    adjustment: If the text overflows the bar on either side, you can adjust it by passing some value\n    prec: How much precision is to be used for displaying percentage?\n    fontsize: size of the font used in percentage text\n    \n    \"\"\"\n    \n    # Get all the bars\n    bars = axs.patches\n    \n    # Size of dataframe\n    items = len(df)\n    \n    assert round(prec)>-1, \"Precision value passed is wrong \"\n    \n    # Iterate over each bar and plot the percentage\n    for bar in bars:\n        width = bar.get_width()\n        height = bar.get_height()\n        precision = '{0:.' + str(prec) + '%}'\n        \n        if math.isnan(width):\n            width=0\n        if math.isnan(height):\n            height=0\n        \n        # Check orientation of the bars\n        if orientation=='h':\n            val_to_sub = height/adjustment\n            axs.text(width + offset, bar.get_y()+bar.get_height()-val_to_sub, \n                    precision.format(width/items), ha=pos, fontsize=fontsize)\n        \n        elif orientation=='v':\n            val_to_sub = width/adjustment\n            axs.text(bar.get_x()+width-val_to_sub, height + offset, \n            precision.format(height/items), ha=pos, fontsize=fontsize)\n        \n        else:\n            print(\"The orientation value you passed is wrong. It can either be horizontal 'h' or vertical 'v'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6af7449124871fb34790308b83edb6649a2cc98"},"cell_type":"code","source":"def pct_on_stacked_bars(axs, values, orientation='v', pos='center', prec=0, fontsize=10, adjustment=0.05):\n    \"\"\"\n    This function can be used to plot percentage on each bar in a stacked barplot. \n    \n    Arguments:\n    axs: Matplotlib axis\n    values: percentage values corresponding to each rectangle in a stacked plot\n    offset: Relative position of the text w.r.t the bar\n    orientation: 'h' or 'v'\n    prec: How much precision is to be used for displaying percentage?\n    fontsize: size of the font used in percentage text\n    adjustment: decimal value in case the values flow out of the rectange\n    \n    \"\"\"\n    \n    # Get all the bars\n    bars = axs.patches\n    \n    \n    # Get all the percentages\n    assert round(prec)>-1, \"Precision value passed is wrong \"\n    values = np.round(values, decimals=prec).T.flatten()\n    \n    if adjustment is not None:\n        adjustment=0.05\n    \n    # Iterate over each bar and plot the percentage\n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        height = bar.get_height()\n        x = bar.get_x()\n        y = bar.get_y()\n        label = str(values[i]) + \"%\"\n        \n        if math.isnan(width):\n            width=0\n        if math.isnan(height):\n            height=0\n        \n        axs.text(x+width/2+adjustment, y+height/2+adjustment, label, ha=pos, fontsize=fontsize) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76c227f0ca417b649dc6d2089ed35cf1fec0f85e"},"cell_type":"code","source":"def get_traces(df, \n                freq_df, \n                text=True, \n                textposition='auto', \n                opacity=0.5, \n                orientation='v', \n                prec=0,\n                stacked=False):\n    \"\"\"\n    This function can be used to generate the traces for a plotly plot. \n    \n    Arguments:\n    df: unstacked value_counts of a grouped pandas dataframe\n    freq_df: similar to freq_df but contains percentages\n    text: Whether to put text on bar or not\n    textposition: Where to put the text. Look for plotly docs for more info\n    opacity: opacity in the bars\n    orientation: horizontal or vertical bars\n    prec: how much precision to be considered for displaying text\n    stacked: whether the traces are for stacked plots or not\n    \n    \"\"\"\n    \n    # An empty list to collect traces for plotly\n    data = []\n\n    # Iterate for each group in df\n    for i, j in zip(range(freq_df.shape[0]), range(freq_df.shape[1])):\n        x = df.index\n        if stacked:\n            y = freq_df.values[:,j]\n        else:\n            y = df.values[:,j]\n        z = freq_df.values[:,j]\n        \n        if orientation=='v': \n            # define a trace for the current index\n            trace = go.Bar(\n            x=x,\n            y=y,\n            text=[str(np.round(i, decimals=prec)) + \"%\" for i in z],\n            textposition = textposition,\n            opacity=opacity,\n            orientation='v',\n            name = df.columns[j])\n        \n        elif orientation=='h':\n            # define a trace for the current index\n            trace = go.Bar(\n            y=x,\n            x=y,\n            text=[str(np.round(i, decimals=prec)) + \"%\" for i in z],\n            textposition = textposition,\n            opacity=opacity,\n            orientation='h',\n            name = df.columns[j])\n        \n        else:\n            print(\"Wrong orientation value provided\")\n            return\n        \n        # add it to the list\n        data.append(trace)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fdf75550460af115ab8446564d9ff4450e835bf"},"cell_type":"code","source":"# A handy-dany function for plottinf funnel charts\ndef draw_funnel_chart(values, \n                      phases, \n                      colors=None, \n                      plot_width=400,\n                      section_h=100,\n                      section_d=10):\n    \"\"\"\n    A function that can be used to generate funnel charts in plotly.\n    \n    \"\"\"\n    n_phase = len(phases)\n    plot_width = plot_width\n\n    # height of a section and difference between sections \n    section_h = section_h\n    section_d = section_d\n\n    # Check if the color values are given or not\n    if colors is None:\n        colors = ['rgb' + str(tuple(np.random.randint(255, size=(3)))) for i in range(n_phase)]\n    elif len(colors)!=n_phase:\n        assert len(colors)==n_phase, \"Number of color values didn't match the number of values\"\n    else:\n        colors = colors\n\n    # multiplication factor to calculate the width of other sections\n    unit_width = plot_width / max(values)\n\n    # width of each funnel section relative to the plot width\n    phase_w = [int(value * unit_width) for value in values]\n\n    # plot height based on the number of sections and the gap in between them\n    height = section_h * n_phase + section_d * (n_phase - 1)\n\n    # list containing all the plot shapes\n    shapes = []\n\n    # list containing the Y-axis location for each section's name and value text\n    label_y = []\n\n    for i in range(n_phase):\n            if (i == n_phase-1):\n                    points = [phase_w[i] / 2, height, phase_w[i] / 2, height - section_h]\n            else:\n                    points = [phase_w[i] / 2, height, phase_w[i+1] / 2, height - section_h]\n\n            path = 'M {0} {1} L {2} {3} L -{2} {3} L -{0} {1} Z'.format(*points)\n\n            shape = {\n                    'type': 'path',\n                    'path': path,\n                    'fillcolor': colors[i],\n                    'line': {\n                        'width': 1,\n                        'color': colors[i]\n                    }\n            }\n            shapes.append(shape)\n\n            # Y-axis location for this section's details (text)\n            label_y.append(height - (section_h / 2))\n\n            height = height - (section_h + section_d)\n\n    # For phase names\n    label_trace = go.Scatter(\n        x=[-100]*n_phase,\n        y=label_y,\n        mode='text',\n        hoverinfo='text',\n        text=phases,\n        textfont=dict(\n            color='rgb(200,200,200)',\n            size=12\n        )\n    )\n\n    # For phase values\n    value_trace = go.Scatter(\n        x=[70]*n_phase,\n        y=label_y,\n        mode='text',\n        hoverinfo='text',\n        text=values,\n        textfont=dict(\n            color='rgb(200,200,200)',\n            size=12\n        )\n    )\n    \n    return label_trace, value_trace, shapes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c374fafdca3338a67fe5bd75af60f02410f16e7b"},"cell_type":"markdown","source":"# 4. Analysis\n\n<a id=\"diversity\"></a>\n## 4.1 Gender Diversity: \nQ1: What is your gender? "},{"metadata":{"trusted":true,"_uuid":"64c16f6f7ca033260bb1df6dcdf29632621c9558"},"cell_type":"code","source":"# Select the column Q1 \"What is your geneder?\"\ngender_df = multiChoiceResp['Q1'][1:].dropna()\n\nf,ax=plt.subplots(figsize=(10,5))\n\n# Do countplot \nax=sns.countplot(gender_df, orient='v', color=color[2])\n\n# Plot percentage on the bars\npct_on_bars(ax, gender_df, orientation='v', offset=50,adjustment=2)\n\n    \nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Gender Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165a432e0118c7f32b9ca1d06dd54ab4c211445a"},"cell_type":"markdown","source":"**This is disastrous** I even did the analysis for [Kaggle Survey 2017](https://www.kaggle.com/aakashnain/the-world-of-stackoverflow) and I don't know why has the numbers not changed at all! I am literally in shock. Here is the comparison:\n\n| Gender       | 2017           | 2018  |\n| :-------------|:-------------|: -----|\n| Male           | 81.88%        | 81.44%|\n| Female       | 16.71%          | 16.81%|\n| Others        | <2%            |  <2%  |\n\nYou might argue that the comparison is not completely fair as the number of respondents is not equal in both the cases. The numbers have grown from 2017 but the ratio is still almost the same. To give a fair comparison and how the numbers have grown since 2017, take a look at the table below:\n\n| Gender       | 2017          | 2018  |Change(%)|\n| :-------------|:-------------|: -----|---------|\n| Male           |~14K         |~20K   | ~43%    |\n| Female         |~2.8K        |~4K    | ~43%    |\n\nHence the overall ratio is almost the same. It is good that the numbers in the case of females have increased but it is not enough. \n\n**To the World**: We need to do more. We can't remove `bias` just by saying, a collective effort is required. Because of these things, many things are Data Science and Machine Learning are biased.  \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"b8554eed0e3851b7f31f94993107b9bc82f79342"},"cell_type":"markdown","source":"<a id=\"age\"></a>\n## 4.2 Age\n\nQ2: What is your age in years?"},{"metadata":{"trusted":true,"_uuid":"0321d57ace7f0721c5c32d183cdd085bf3b17638"},"cell_type":"code","source":"# Select the column for the corresponding question\nage_df = multiChoiceResp['Q2'][1:].dropna()\norder= ['18-21', '22-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-69', '70-79', '80+']\n\nf,ax=plt.subplots(figsize=(10,5))\n\n# Do countplot \nax=sns.countplot(age_df,order=order, orient='v')\n\n# plot the percentage on bars\npct_on_bars(ax, age_df, orientation='v', offset=50, adjustment=2)\n\n    \nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.title('Age Group Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b5442ae776968fdf720778e5ef50ab97ec73d83"},"cell_type":"markdown","source":"More than **70%** of the respondents are of the age group 18-34. Given the hype around Data Science, where every other person declares it as the  **sexiest job of the 21st-century**, I don't find this odd. \n\nAlthough this is a very good sign that adoption of Data Science has become huge but there is some bad news also. Many people portray data science and machine learning as `easy` but that is very very misleading if you consider all the things that it takes to become a very good Data Scientist/ML engineer. In short, Pandas isn't Data Science, scikit-learn isn't machine learning and Tensorflow/PyTorch isn't deep learning. People start with great enthusiasm but when they deep dive into the subject they feel overwhelmed. In my opinion, if anyone can't give the right guidance, he/she shouldn't provide the wrong insights as well.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"51a011e9c6f5d41dece7fa6ed21f888042701935"},"cell_type":"markdown","source":"<a id=\"age-gender\"></a>\n## 4.3 Gender distribution in different age groups"},{"metadata":{"trusted":true,"_uuid":"e9df20db5d6adf1a3a117b8d6ff51206414cd5a0"},"cell_type":"code","source":"# Do countplot on age df\nage_df = multiChoiceResp[['Q1', 'Q2']][1:].dropna()\n\n# We will consider only three categries for brevity: Female, Male and Others. This isn't done to hurt someone\n# I am really sorry if I did hurt someone by doing this.\nage_df['Q1'] = age_df['Q1'].replace(['Prefer not to say', 'Prefer to self-describe'], 'Others')\n\nf,ax = plt.subplots(figsize=(20,5))\nax=sns.countplot(x='Q2', data=age_df, order=order, orient='v', hue='Q1')\n\n# Get all the bars and plot the percentage also\nbars = ax.patches\n\nsections = len(bars)//3\nfirst_bar = bars[:sections]\nsecond_bar = bars[sections:len(first_bar)+sections]\nthird_bar = bars[len(second_bar)+sections:]\n\n\n# Loop over the bars and put text on each bar\nfor left, middle, right in zip(first_bar, second_bar, third_bar):\n        height_l = left.get_height()\n        height_m = middle.get_height()\n        height_r = right.get_height()\n    \n        \n        if math.isnan(height_l):\n            height_l=0.0001\n        if math.isnan(height_m):\n            height_m=0.0001\n        if math.isnan(height_r):\n            height_r=0.0001\n        \n        total = height_l + height_m + height_r\n\n        ax.text(left.get_x() + left.get_width()/3., height_l + 40, '{0:.1%}'.format(height_l/total), ha=\"center\", fontsize=8)\n        ax.text(middle.get_x() + middle.get_width()/3., height_m + 40, '{0:.1%}'.format(height_m/total), ha=\"center\",fontsize=8)\n\n\n    \nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.title('Gender Distribution in different age groups')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b0da0288c1daf564b6208430a9dc3a54816d044"},"cell_type":"markdown","source":"Is this the result you expected? I know we all thought that the distribution would be highly skewed for the age groups beyond 25 and that is because of all the circumstances that were present in the last decade. But none of us would have expected that the diversity problem would be so much screwed in the youngest age group, given the facts that everyone is trying to tackle the gender bias problem. *The age group `18-21` has a much worse gender distribution as compared to the age group `25-28`* :(\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"778fd3e3362660e4d1de13cea8186d55848ae635"},"cell_type":"markdown","source":"<a id=\"country\"></a>\n## 4.4 Country of Residence\n\nQ3: In which country do you currently reside?"},{"metadata":{"_uuid":"b6a18ea446e34ab76cdfaa7524f6170ecc67a24d"},"cell_type":"markdown","source":"**First thing first**: A lot of people have shown in kernels how easy it is to plot things on a world map in languages other than Python. I assume Python people are curious to do the same. So, I am going to show how easy it is to plot things on World Map in Python too. Take a look!"},{"metadata":{"trusted":true,"_uuid":"1485be8004e258ad1ac7f589c8243a60415fe43f"},"cell_type":"code","source":"# Select the column for the corresponding question\ncountry_df = multiChoiceResp['Q3'][1:].dropna()\n\n# Drop columns where the country is mentioned as 'others' or where the respondent declined to share the name of the country\nindex_to_drop = country_df.index[(country_df=='Other') | (country_df=='I do not wish to disclose my location')]\ncountry_df = country_df.drop(index_to_drop)\n\n# Get the counts for each country\ncountry_value_counts = country_df.value_counts()\n\n# Define data for plotly interactive plot\ndata = [dict(\n        type = 'choropleth',\n        autocolorscale = True,\n        showscale = True,\n        locations = country_value_counts.index,\n        z = country_value_counts.values,\n        locationmode = 'country names',\n        reversescale = False,\n        marker = dict(\n            line = dict(color = 'rgb(180,180,180)',width = 0.5) \n            ),\n        colorbar = dict(\n            title = \"Count\"),\n            autotick = False,    \n        )]\n\n# Layout of th plot\nlayout = dict(\n    title = 'Where do data science people live?',\n    geo = dict(\n        showframe = False,\n        showcoastlines = False,\n        projection = dict(type = 'Mercator')\n        ),\n    autosize=False,\n    width=1000,\n    height=600,\n    )\n\n# Plot\nfig = dict( data=data, layout=layout )\niplot( fig, validate=False, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0bce4cadec084fb55b3a88024ad587aa02ab800"},"cell_type":"markdown","source":"You can hover over each country to check the number of data science people who participated in the Kaggle Data Science Survey 2018 from that corresponding country. \n\nThis is all good for a fancy visualization. But what about the top 10 countries and how things have changed among the top 10 players since 2017? Let's check that! "},{"metadata":{"trusted":true,"_uuid":"182c0643856e41f2af224f3507b7112602bc3ee0"},"cell_type":"code","source":"# Select the column for the corresponding question\ncountry_df = multiChoiceResp['Q3'][1:].dropna()\n\n# Drop columns where the country is mentioned as 'others' or where the respondent declined to share the name of the country\nindex_to_drop = country_df.index[(country_df=='Other') | (country_df=='I do not wish to disclose my location')]\ncountry_df = country_df.drop(index_to_drop)\ncountry_df = country_df.replace(['United States of America', 'United Kingdom of Great Britain and Northern Ireland'], ['USA', 'UK & Northern Ireland'])\n\n# Check the counts and plot the values\n# We will only consider top 10 counries only\ndo_barplot(country_df, plt_title='Country wise distribution of Data Science people(Top 10 only)', \n           xlabel='Country', ylabel='Percent', \n           figsize=(12,5), orient='v',\n           max_counts=10,\n           clr_code=5, \n           normalize=True, \n           rotation=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b21733925e904ab82fddbe09a58350f99845ca65"},"cell_type":"markdown","source":"2018 is almost the same as 2017 with USA and India as the top two leading countries. Almost 40% of the respondents are from USA and India. Also, there are some changes as compared to 2017. Let's see how much things have changed in the top 10 since 2017:\n\n| 2018           | 2017|\n|:-------------|:-------------:|\n|1.  USA        |  USA          |\n|2.  India      |  India          | \n|3. **China**  |  Russia      |\n|4.  Russia     |  UK          |\n|5.**Brazil** |  China          | \n|6.**Germany**|  Brazil      |\n|7.  UK         |  Germany          |\n|8.  France     |  France          | \n|9.  Canada     |  Canada     |\n|10**Japan** |  Australia     |\n\nSo Australia has been replaced by Japan in 2018 in the top 10 list whereas China jumped from 5th position to 3rd position. Brazil and Germany also got a jump of one position.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"013bc39e5fa083fc4ab32de4f55e10dad6d872b9"},"cell_type":"markdown","source":"<a id=\"country-gender\"></a>\n## 4.5 Country-wise gender distribution\n\nWe have already seen that the gender distribution is highly skewed. It would be interesting to check how this distribution varies in the top 10 countries."},{"metadata":{"trusted":true,"_uuid":"967d051b6637c521780596ff005872419905ef6d"},"cell_type":"code","source":"# Select the columns you are insterested in\ndf = multiChoiceResp[['Q1', 'Q3']].dropna()\n\n# Select only those indices where the top 10 countries are there\ndf = df.iloc[country_df.index]\n\n# Select countries to consider\ntop10_countries = ['United States of America', 'United Kingdom of Great Britain and Northern Ireland', \n                   'India', 'China', 'Russia','Brazil', 'Germany', 'France', 'Canada', 'Japan']\n\ndf = df[df['Q3'].isin(top10_countries)]\ndf['Q3'] = df['Q3'].replace(['United States of America', 'United Kingdom of Great Britain and Northern Ireland'], \n                            ['USA', 'UK & Northern Ireland'])\ndf['Q1'] = df['Q1'].replace(['Prefer not to say', 'Prefer to self-describe'], 'Others')\n\n\n# Group by roles and get the count for experience \nfreq_df = df.groupby(['Q3'])['Q1'].value_counts().unstack()\n\n# Convert the frequencies to percentage\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Get traces\ndata = get_traces(freq_df, pct_df, stacked=True, orientation='v', textposition='auto', opacity=0.9)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n     autosize=False,\n     width=900,\n     height=600,\n     barmode='stack',   \n     margin=go.layout.Margin(\n                            l=50,\n                            r=0,\n                            b=100,\n                            t=50,),\n    title='Country-wise gender distribution',\n    xaxis=dict(title='Country'),\n    yaxis=dict(title='Percentage'),\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7b38be82e628d4736a5e93f650e52fbb147e886"},"cell_type":"markdown","source":"Very interesting!\n* **Japan**, even after making so many technical advancements, has the worst gender bias. Totally unbelievable!\n* **Brazil** is almost on the same scale as Japan.\n* **USA** and **Canada** are much better than any other country but that's relative but I am positive that these countries will lead others in terms of removing gender bias in the nearby future. **UK and Nothern Ireland** is almost catching up with Canada.\n* **France, Germany**, and **India** are almost on a similar scale, every one of them having less than 20% of the female workforce in data science\n* **Russia** is ahead of Japan in gender diversity in data science but still way too behind than **USA** and **Canada**\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"59a4079e971f8af0cf68010e58e20ce7b1ce52c6"},"cell_type":"markdown","source":"<a id=\"education\"></a>\n## 4.6 Level of Education\n\nQ4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?"},{"metadata":{"trusted":true,"_uuid":"2f5276d3403dd5cea9654995c697928da4b83bb4"},"cell_type":"code","source":"# Select the column corresponding to Q4, the education level\nedu_df = multiChoiceResp['Q4'][1:].dropna()\n\n# Get the value counts\nedu_df_counts = edu_df.value_counts()\n\n# Get the labels and the corresponding counts \ncounts = edu_df_counts.values\nlabels = edu_df_counts.index\n\n\n# Function to show percentage in pie plot\ndef show_autopct(values):\n    def my_autopct(pct):\n        total = len(edu_df)\n        val = int(round(pct*total/100.0))\n        return '{p:.2f}%'.format(p=pct)\n    return my_autopct\n\n\n# Plot a pie chart showing level of formal education\nplt.figure(figsize=(10,8))\npatches, text, autotext = plt.pie(counts, labels=labels, autopct=show_autopct(counts), )\nplt.title(\"Highest level of formal education\", fontsize=16)\nplt.show()\n\n# Delete variables that aren't going to be used further in order to save memory\ndel counts,labels, patches, text, autotext, edu_df_counts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96282fbda8105e8aa4ebca3ed831f076f3dbdc1e"},"cell_type":"markdown","source":"Almost 50% of the respondents are having Master's degree while 30% of them are having a Bachelor's degree. In the 2017 survey, I predicted that the percentage of `Bachelor's degree` will increase while that of `Master's degree` will decrease because of the on-going political situations as well as the huge demand of DS people in the market.\n\n**My predictions went completely wrong** There is a whopping increased interest in getting a  `Master's degree`. Is this because of the fact that most of the data science jobs, especially including Machine learning, put a minimum of Master's degree in requirements? Maybe or maybe not. \n\nAnywyas, let's see how things have changed overall since 2017:\n\n| Degree                             | 2018(%)  | 2017(%)   |\n|:-------------|-------------:|---------------:|\n|Master's                            |46        |41         |  \n|Bachelor's                          |30        |32         |  \n|Doctoral                            |14        |15         |\n|Professional                        |3         |3          |\n|College/Univ without earnings       |4         |5          |\n|No formal edu past high school      |~1.5      |~2         |\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"438704ad00e48263765d9b7d7dde5aca20d40649"},"cell_type":"markdown","source":"<a id=\"job-title\"></a>\n## 4.7 Current Role\n\nQ6: Select the title most similar to your current role (or most recent title if retired)"},{"metadata":{"trusted":true,"_uuid":"baba0bd3378c7c95220575cd6f03019acb355479"},"cell_type":"code","source":"# Select column Q6\nrole_df = multiChoiceResp['Q6'][1:].dropna()\n\n# Get the value counts\nrole_df_counts = role_df.value_counts()\n\n# Perecntage values\nrole_df_pct = (round(role_df.value_counts(normalize=True)*100,1)).values\nrole_df_pct = [str(x)+'%' for x in role_df_pct]\n\n# Visualize\ntrace0 = go.Bar(\n                x=role_df_counts.values,\n                y=role_df_counts.index,\n                orientation='h',\n                text = role_df_pct,\n                textposition='outside',\n                marker=dict(\n                color='rgb(200,2,55)',\n                line=dict(color='rgb(125,75,55)',\n                         )),\n                opacity=0.7\n               )\n\nlayout = go.Layout(title='<b>Job titles</b>',\n                  autosize=False,\n                  width=1000,\n                  height=600,\n                  xaxis=dict(title='Count'),\n                  margin=go.layout.Margin(\n                                        l=200,\n                                        r=50,\n                                        b=100,\n                                        t=100, pad=10)\n                   )\n\nfig = go.Figure(data=[trace0], \n                layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fd5ca052601f297492d2832b5aaffbce06eeb55"},"cell_type":"markdown","source":"The above plot is an interactive plot. You can hover over the bars to see the actual counts and the percentage count for each category separately.\n\nOut of all the respondents, the top four roles are:\n* **Student** -> ~23% of the respondents are students. Is there any chance that some people who have filled up the form might be working professionals as well as a students preparing for a doctoral degree? Maybe or maybe not.\n* **Data Scientist** -> 18% of the respondents are working/worked recently are/as Data Scientist\n* **Software Engineer** -> ~14% of the total respondents are software engineer by profession\n* **Data Analyst** -> Data analyst is fourth on the list leaving others (business analysts and data engineers) behind. Although all three are different, I didn't expect such a huge gap between them. If you a little attebtion to the above plot, the difference between a business analyst and a Data Engineer is negligible but data analyst is leading by a huge margin than the other two.\n\nThere are two things worth noting down:\n* What all roles are included in `Software Engineer`? For example, in India, a software engineer but can be working as a Machine Learning Engineer, or a web developer or a mobile app developer. The same goes for `Research Scientist`. A research engineer in India can be a Machine Learning Engineer or someone who is just looking for more papers. It would have been better if there were more explicit optiond in the form.\n\n* The term `Data Scientist` means data scientist only or is it a broad term used to combine data scientist with ML engineers? Although the work is very much similar in some aspects, in my opinion, a machine learning engineer is also worried about putting things in production, that too under certain constraints. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"7c7c04e77d74d0f88a2d7b32cdc74e9e085a45e9"},"cell_type":"markdown","source":"<a id=\"experience\"></a>\n## 4.8 Experience in current role\n\nQ8: How many years of experience do you have in your current role?"},{"metadata":{"trusted":true,"_uuid":"90e47760e530c3ae0f7dff5c147119dc663d55be"},"cell_type":"code","source":"# Select the column for the corresponding question\nexp_df = multiChoiceResp['Q8'][1:].dropna()\n\nf,ax=plt.subplots(figsize=(10,5))\n\n# Order in which we want the plot to ensure readability \norder= ['0-1', '1-2', '2-3', '3-4', '4-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30+']\n\n\n# Do countplot \nax=sns.countplot(exp_df,order=order, orient='v', color=color[4])\n\n# Plot percentage on the bars\npct_on_bars(ax, exp_df, orientation='v', offset=50,adjustment=2)\n\n    \nplt.xlabel('Experience (in years)')\nplt.ylabel('Respondents count')\nplt.title('Years of experience in current role')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d1cb489ee93735d2fc545c8beb79ee4738dbf72"},"cell_type":"markdown","source":"Given the fact that the maximum number of respondents are students, the first column doesn't give much of insight as in some way it was expected that the percentage of `0-1` years of experience would also be on a similar scale. In order to actually get better insights from this column, let us choose some top roles of interest and check the amount of experience people are having in those roles. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"af48c6b2a950c540aa135e1ccda3027640fb9877"},"cell_type":"markdown","source":"<a id=\"roles-experience\"></a>\n## 4.9 Experience in top roles"},{"metadata":{"trusted":true,"_uuid":"6efca43b35b103bcdf17fbc15136e41c4a89a4ef"},"cell_type":"code","source":"# Select the columns we are insterested in\ndf = multiChoiceResp[['Q6', 'Q8']][1:].dropna()\n\n# Select top roles to consider\nroles_to_consider = ['Data Scientist', 'Data Analyst', 'Software Engineer', 'Research Scientist'] \n                   \n# Select only those rows that are of our interest\ndf = df[df['Q6'].isin(roles_to_consider)]\n\n# Map the column values to numeric values because sorting the columsn then would become easy\ncol_dict = dict([(x,i)  for i,x in enumerate(order)])\ndf['Q8'] = df['Q8'].replace(col_dict)\n\n# Group by roles and get the count for experience \nfreq_df = df.groupby(['Q8'])['Q6'].value_counts().unstack()\n\n# Convert the frequencies to percentage\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Do a percentage plot\nf,ax=plt.subplots(figsize=(15,6))\nax=pct_df.plot(kind='bar', stacked=True, ax=ax, rot=30, use_index=True);\n\n# Use our handy dandy function to plot percentage on the stacked bars\npct_on_stacked_bars(axs=ax,orientation='v',values=pct_df.values)\n\n# Map the xticks back to original column values \nax.set_xticklabels(order)\n\nplt.xlabel('Experience in years');\nplt.ylabel('Percentage')\nplt.title('How much of experience respondents are having in top roles?')\nplt.legend(loc=(1.02,0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76940ccef065e923572f973eaea0640c30703279"},"cell_type":"markdown","source":"**Note**: If you add up the percentages on the stacked bars, it might happen that the sum of percentages is either slightly lower/greater than 100. This is because of the **rounding error** of the float values and if you increase the precision, then everything will be fine except that visualization will be cluttered by the extra float values in text.\n\nThis is a much better plot than the previous one and we can extract a lot of information from this. \n* The percentage of **Data Analyst** decreases as the experience increases which is natural. Data Analyst is often considered an entry-level job in maximum cases, although this is not true for all cases. \n* A similar trend can be observed for **Data Scientists**. As the experience increases, people start to look for higher positions, for example, the position of a **Research Scientist**. We can clearly see that the percentage of Research Scientist increases as experience(in years) increases.\n\nThe only weird trend is about `Software Engineering`. As I said earlier too, a software engineer can be related to too many profiles. If you look at the bar corresponding to `5-10` years of experience, then it is clear that the percentage of software engineers increased and that of Data Scientists has decreased. Of course, some data scientists might have upgraded/switched to Research Scientist but would anyone switch from Data Scientist to Software engineer until unless the position involves Machine Learning/Deep Learning? Who knows!\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"ce01278f1fc5aa158e4cd915f8e1747d90e0ff87"},"cell_type":"markdown","source":"<a id=\"salary\"></a>\n## 4.10 Yearly compensation\n\nQ9. What is your current yearly compensation (approximate $USD)?"},{"metadata":{"trusted":true,"_uuid":"5c9b3bc3ee13f371de330265e0c2e2d42b2388d4"},"cell_type":"code","source":"# Define a function to clean data and convert the salaries to desired type\ndef cleanup_salary(salary):\n    '''The salaries are represented as 0-10,000 10-20,000...We will clean up this column \n       and convert the salary to numeric. Also for a range we will just take the upper value \n       as the representative salary. For example, if the salary in range 0-10,000 we will consider \n       the salary to be 10,000 as the representative. \n     '''\n    \n    # Replace the unwanted characters. PAY ATTENTION TO THE CHAINING\n    salary = salary.str.replace(',', '').str.replace('+','')\n    \n    # Split the salaries on '-' and choose the last value\n    # P.S: See the pandas usage here. This is the most elegant way to do such thigs in pandas\n    salary = salary.str.split('-').str[-1]\n    \n    # Convert to numeric type\n    salary = salary.astype(np.float64)\n    \n    return salary","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237f346a468a6134a651c346ad25c203004ddaf7"},"cell_type":"markdown","source":"Looking at the plain salary distribution won't give much of insight but there are two things which are more interesting to look into as compared to plain salary:\n* Distribution of salary according to years of experience\n* Distribution of salary according to the current role\n\nYou might be wondering if we can also include the `level of education` as another level to consider for salary distribution? It is very common that the salary of a person with a Doctoral degree/Master's degree is more than a person having only Bachelor's degree. There might be a few exceptions to this but this is how the trend is in general.\n\n#### 4.10.1 Salary distribution w.r.t to years of experience"},{"metadata":{"trusted":true,"_uuid":"21790d601ecddfbc6c02fc82e7887c61ee024a85"},"cell_type":"code","source":"# Select the desired columns\nsalary_df = multiChoiceResp[['Q6', 'Q8', 'Q9']][1:].dropna()\n\n# Remove all rows where the respondent declined to disclose the salary\nsalary_df = salary_df[~(salary_df['Q9']=='I do not wish to disclose my approximate yearly compensation')]\n\n# Clean the salary column\nsalary_df['Q9'] = cleanup_salary(salary_df['Q9'])\n\n# Order in which we want the plot to ensure readability \norder= ['0-1', '1-2', '2-3', '3-4', '4-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30+']\n\n# A list to store box plot data for each category\nsalary_data= []\n\n# Create a box plot trace\nfor exp in reversed(order):\n    salary_data.append(go.Box(x=salary_df[salary_df['Q8'] == exp]['Q9'], name=exp, orientation='h'))\n\nlayout = go.Layout(title='Salary distribution w.r.t years of experience',\n                   autosize=False,\n                   height=500,\n                   width=1000,\n                   xaxis=dict(title='Salary'),\n                   yaxis=dict(title='Experience(in years)',),)\n\nfig = go.Figure(data=salary_data, layout=layout)\n# Visualize\niplot(fig, show_link=False)\n\ndel salary_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b2680d927272eae82cf333923bb011b908c80bb"},"cell_type":"markdown","source":"One interesting thing to note here is that the number of outliers is much more for the groups having lesser experience, for example, groups with `0-1` and `1-2` years of experience as compared to the groups with more than 10 years of experience. Pause reading this kernel for a moment and think of all the reasons that can be a potential driver for this thing. (I will provide my thoughts too but just write down all the things that you can think of as of now.)\n\n\n#### 4.10.2 Salary distribution w.r.t to current role\nWe will consider only the top four roles only(discussed in the previous section). **There is no intention behind this selection except for brevity in plots.**"},{"metadata":{"trusted":true,"_uuid":"9802d65d13761dd7d4444b4ba017b13e49a82599"},"cell_type":"code","source":"# Select only those data points that are of interest\nsalary_df = salary_df[salary_df['Q6'].isin(roles_to_consider)] # roles_to_consider is defined in previous section\n\n# A list to store box plot data for each category\nsalary_data= []\n\n# Create a box plot trace\nfor role in roles_to_consider:\n    salary_data.append(go.Box(y=salary_df[salary_df['Q6']==role]['Q9'], name=role, orientation='v'))\n\nlayout = go.Layout(title='Salary distribution w.r.t current role',\n                   xaxis=dict(title='Role'),\n                   yaxis=dict(title='Salary',),)\n\nfig = go.Figure(data=salary_data, layout=layout)\n# Visualize\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90502331a1164bd7175eaf61dccb6a20c26589ca"},"cell_type":"markdown","source":"This is something really really interesting. \n* **Data Scientist** earn more than a **Research Scientist** in general. I suspect this is because of the fact that we didn't consider the 500K plus salaries in the data. \n* Analayze the plot for Data Analyst and Software engineer. Now you know why there are more outliers in the `0-1 and 1-2 years of experience` groups as compared to the more experienced groups.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"64796bb66b5faa75c7e782b67a249111e5317834"},"cell_type":"markdown","source":"<a id='business'></a>\n## 4.11 Do businesses really want Machine Learning? \n\nQ10. Does your current employer incorporate machine learning methods into their business?"},{"metadata":{"trusted":true,"_uuid":"8789d38747100fb1f3e354e6d5542c5200f20dc3"},"cell_type":"code","source":"# Select the question \nbus_df = multiChoiceResp['Q10'][1:].dropna()\n\n# Check percentage\nbus_df_counts = round(bus_df.value_counts(normalize=True)*100)\nbus_df_counts = bus_df_counts.to_dict()\n\nfor k,v in bus_df_counts.items():\n    print(k.ljust(100), v,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e830f3ada0c1e0022acf31d7c133fff2c478415"},"cell_type":"markdown","source":"(**Jaw dropped...literally!**) This is completely shocking, at least for me.\n* Using ML in production -> 31%\n* Either non-production ML or not using it at all -> 69%\n\nWOW! How can it be? Only 30% of the business is running on ML. Why are the companies not leveraging machine learning?\n* Is it because ML is an overkill for their work?\n* Is it because they don't have enough data?\n* Is it because they don't have the right people to work on it?\n* Or is it something completely different?\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"0bd39d36df7f90f6c749005eae5d123ea80cd16b"},"cell_type":"markdown","source":"<a id='proglang'></a>\n## 4.12 Programming language preference\n\nQ17. What specific programming language do you use most often?"},{"metadata":{"trusted":true,"_uuid":"0d69888f2937e550af63f8f2d7a28d5d3579948d"},"cell_type":"code","source":"# Select the desired columns\nprog_df = multiChoiceResp['Q17'][1:].dropna()\norder = prog_df.value_counts().index\n\nf,ax=plt.subplots(figsize=(15,7))\n\n# Do countplot \nax=sns.countplot(y=prog_df, orient='h', order=order)\n\n# plot the percentage also\npct_on_bars(ax, prog_df, orientation='h', offset=170, adjustment=3)\n    \nplt.ylabel('Programming language')\nplt.xlabel('Count')\nplt.title('Which programming language is preferred how much?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c532c1132c2e8ae68d01bc90c9ab9169c91b1e4a"},"cell_type":"markdown","source":"Ha! **Python** is taking over the world. Second place is secured by **R language** as usual, but the difference between the number of people using python and the number of people using R is quite high **(54% Vs 13%)**. The amount of flexibility python provides is literally awesome. One of the best things about Python is that it helps to save **developers time** to experiment with different things as everything is much easier to code in Python.\n\nMost surprising is that **Julia** usage is almost 0%. I expected it to be bad but I didn't expect the count to be this bad. Last year we had a debate on Julia usage in our **[KaggleNoobs](https://kagglenoobs.herokuapp.com/)** slack and most of us agreed that if Julia keeps going at this rate, it would die a miserable death. \n\nAnyways let's look at what the people in top roles prefer most. We will consider top 4 roles and top 10 languages for visualization purpose. And I think that's all which matters actually in the real world too!"},{"metadata":{"_uuid":"da9030a201aeac8c7eb02d872151e25e5f711931"},"cell_type":"markdown","source":"#### 4.12.1 Programming languages used by people in different roles"},{"metadata":{"trusted":true,"_uuid":"24c837bc6107f00435e0981dd54a82f893b59795"},"cell_type":"code","source":"# Select the columns corresponding to current role(Q6) and programming language(Q17)\nprog_df = multiChoiceResp[['Q6', 'Q17']][1:].dropna()\n\n# Filter only top four roles and top 10 programming languages for brevity \nprog_df = prog_df[prog_df['Q6'].isin(roles_to_consider)]\nprog_to_consider = prog_df['Q17'].value_counts().index[:10]\nprog_df = prog_df[prog_df['Q17'].isin(prog_to_consider)]\n\n\n#col_dict = dict([(x,i)  for i,x in enumerate(prog_to_consider)])\n#prog_df['Q17'] = prog_df['Q17'].replace(col_dict)\n\n# Group by roles and get the count for experience \nfreq_df = prog_df.groupby(['Q17'])['Q6'].value_counts().unstack()\n\n# Convert the frequencies to percentage\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Get traces\ndata = get_traces(freq_df, pct_df, stacked=True, orientation='v', textposition='auto', opacity=0.7)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n     autosize=False,\n     width=1000,\n     height=600,\n     barmode='stack',   \n     margin=go.layout.Margin(\n                            l=50,\n                            r=0,\n                            b=100,\n                            t=50,),\n    title='<b>Programming language preference in top roles</b>',\n    xaxis=dict(title='Programming language'),\n    yaxis=dict(title='Percentage'),\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9f9336366424ba707d77a554b5d6ca70b16ae1d"},"cell_type":"markdown","source":"**Summary:**\n* Data Analyst uses SQL, SAS/STATA and R more.\n* For Data Scientist, Python and R are more favorable\n* Research Scientist use MATLAB heavily. I know MATLAB is good and before python evolved, it was the only thing out there for researchers but I don't see any point in using MATLAB today. The world should switch to Python(3 not 2)\n* Software Engineers use Java, Javascript, and PHP. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"4e09d0241440eb4efa0ee8b746f5fc84a4ee41b3"},"cell_type":"markdown","source":"<a id='advise'></a>\n## 4.13 Which language should beginners learn first?\n\nQ18. What programming language would you recommend an aspiring data scientist to learn first?"},{"metadata":{"trusted":true,"_uuid":"2fd91de468cd583bf50afbaece814286b780f84f"},"cell_type":"code","source":"# Select the desired columns\nprog_df = multiChoiceResp['Q18'][1:].dropna()\n\n# Consider only top 5 languages\norder = prog_df.value_counts().index[:5]\n\nf,ax=plt.subplots(figsize=(10,5))\n\n# Do countplot \nax=sns.countplot(x=prog_df, order=order)\n\n# Plot percentage on the bars\npct_on_bars(ax, prog_df, orientation='v', offset=50, adjustment=2)\n    \nplt.ylabel('Programming language')\nplt.xlabel('Count')\nplt.title('Which programming language should you learn first?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad79632363ef3383e53671d2521a39e71bfc5e61"},"cell_type":"markdown","source":"Of course, you should learn **Python** first. But there is one thing that I really want to point out and make it clear as much as I can. Learning how to write **Python** code isn't a big deal but learning how to code in **Pythonic** way is a steep path and it can take quite some time to learn how to write efficient python code \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"3290a94c3da953f7c05e631feedf4c33041ab31c"},"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n## 4.14 Most used ML libraries\nQ20. Which ML library have you used the most?"},{"metadata":{"trusted":true,"_uuid":"5bd27237e204fccfdeb535b3abf25f6dbd592fae"},"cell_type":"code","source":"# Pick the data we are interested in\nlibraries_df = multiChoiceResp['Q20'][1:].dropna()\n\nf,ax=plt.subplots(figsize=(15,5))\n\n# Order in which we want the plot to ensure readability \norder= libraries_df.value_counts().index\n\n\n# Do countplot \nax=sns.countplot(y=libraries_df,order=order, orient='h', color=color[2])\n\n# Plot percentage on the bars\npct_on_bars(ax, libraries_df, orientation='h', offset=150, adjustment=3)\n    \nplt.ylabel('Library/Framework')\nplt.xlabel('Count')\nplt.title('Which library is used by how much?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76cbcfce4720d40961c3ba09b70e576638951499"},"cell_type":"markdown","source":"**scikit-learn**: 46% of the respondents use scikit-learn. This shouldn't come as surprise. In fact, I expected it more. Even if you are not using scikit-learn for using any ML algorithm implementation, people still use it for doing five fold splits for validation, or the most basic train/test split, standardizing data, etc. Then there are algorithms like RandomForest in the library that is also used widely used.\n\n\n**Tensorflow and Keras**: Keras has always been the easiest path(in most of the cases) if you want to build things quickly. Tensorlfow has always been a pain for simple tasks whereas for production usage, tensorflow still dominates the market. One important thing to observe is that the number of Tensorflow users has dropped from **24% in 2017** to **15% in 2018**. No surprise why it has dropped, PyTorch!.The ease and flexibility of doing things in **PyTorch** are much more as compared to Tensorflow.\n\n* **A note to TF developers:** From the above stats, you should learn a lesson. A developers' responsibility is to build things in the way the end users want. What is important from your perspective may not be useful at all from an end users' perspective. Although with the ongoing `RFCs` for `TF2.0`, I expect things to improve, that doesn't mean that the developers should forget why someone switched from Keras/Tensorflow to PyTorch\n\n**PyToch**: PyTorch is catching up quickly. Let's see how it performs next year once TF2.0 comes out\n\n**Xgboost**: Xgboost has been one of the favorite libraries especially for almost all Kagglers. It is very powerful, easy to use and still unbeatable when it comes to tabular data.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"878208ecce8bab0e347b4a1f4c8b3a39fbc69c2e"},"cell_type":"markdown","source":"<a id=\"vis\"></a>\n## 4.15 Popular Visualization libraries and tools\n\nQ22. Which specific data visualization library or tool have you used the most?"},{"metadata":{"trusted":true,"_uuid":"848465a698ae77202424c3c25233694c22afa05a"},"cell_type":"code","source":"# Select the column corresponding to Q22\nvis_df = multiChoiceResp['Q22'][1:].dropna()\n\nf,ax=plt.subplots(figsize=(15,5))\n\n# Order in which we want the plot to ensure readability \norder= vis_df.value_counts().index\n\n\n# Do countplot \nax=sns.countplot(y=vis_df,order=order)\n\n# Plot percentage on the bars\npct_on_bars(ax, vis_df, orientation='h', offset=150, adjustment=3)\n    \nplt.ylabel('Visualization Library/tool')\nplt.xlabel('Count')\nplt.title('Which libraries/tools are most popular for visualizations?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12e46b9a809252c177d247e38f3620ca9b96660c"},"cell_type":"markdown","source":"**Matplotlib** is the backbone of almost all the visualization libraries in Python. 55% of the respondents use matplotlib for visualization. The usage of Matplotlib is still dominant compared to other high-level libraries for graph objects in research papers. There is one thing that needs to be made crystal clear though. The respondent may not be using matplotlib directly for high-level objects but might be using it to tweak the basic graph objects. For example, in the above plot, I am not using matplotlib directly for creating the bar plot but I am using it to change the labels and title of the matplotlib figure object.\n\n**ggplot2** ggplot is among those libraries that actually follow the `Grammar of Graphics` concept. It is neat and the visualizations created using it are always eye-catching.\n\n**Seaborn** and **Plolty** are very useful high-level libraries for creating awesome visualizations. With plotly you can create some of the best interactive visualizations and if you look at the home page of that library, you will find that it supports multiple languages apart from Python. Seaborn is one of my favorite libraries when it comes to doing quick visualizations. With few lines of code, you can produce good visualizations. The only con with seaborn is that it doesn't support interactive visualization(**..yet**). \n\nApart from them, **D3.js** is also very good in terms of flexibility and complex visualizations. **Altair** is another good interactive visualization library. The downside with Altair is that the documentation show its usage only with tiny and clean examples which are good but there is no documentation for dealing with complex and big datasets. If **[Jake](https://github.com/jakevdp)** can provide a tutorial(in a single jupyter notebook) with a dataset like this one or any other competition involving tabular data from Kaggle, I think Altair can be in top five within a few months. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"0f2f231013a11375258704461351961d5cebe38d"},"cell_type":"markdown","source":"<a id=\"coding\"></a>\n## 4.16 How much data science people code actively?\n\nQ23. Approximately what percent of your time at work or school is spent actively coding?"},{"metadata":{"trusted":true,"_uuid":"fc2ee8b3f7af9184c57cc1ae32d2636997b20012"},"cell_type":"code","source":"# Select all the columns that we are interested in\ncoding_df = multiChoiceResp[['Q6','Q23']][1:].dropna()\n\n# Some cleanup\ncoding_df['Q23'] = coding_df['Q23'].str.replace(\"of my time\", '').str.replace(\" to \", '-').str.strip()\n\nf,axes=plt.subplots(2,1, figsize=(10,10))\n\n# Order in which we want the plot to ensure readability \norder= ['0%', '1%-25%', '25%-49%', '50%-74%', '75%-99%', '100%']\n\n\n# Do countplot \nax=sns.countplot(x='Q23', data=coding_df,order=order, color=color[1], ax=axes[0])\n\n# Plot percentage on the bars\npct_on_bars(ax, vis_df, orientation='v', offset=50, adjustment=3)\naxes[0].set_ylabel('Count')\naxes[0].set_xlabel('Time spent coding')\n\n# Another interesting thing to do\nroles = ['Data Scientist', 'Data Analyst', 'Research Scientist']\ncoding_df = coding_df[coding_df['Q6'].isin(roles)]\nax=sns.countplot(x='Q23', data=coding_df,order=order, ax=axes[1], hue='Q6')\n\n# Get all the bars and plot the percentage also\nbars = ax.patches\nsections = len(bars)//3\nfirst_bar = bars[:sections]\nsecond_bar = bars[sections:len(first_bar)+sections]\nthird_bar = bars[len(second_bar)+sections:]\n\n# Loop over the bars and put text on each bar\nfor left, middle, right in zip(first_bar, second_bar, third_bar):\n        height_l = left.get_height()\n        height_m = middle.get_height()\n        height_r = right.get_height()\n    \n        \n        if math.isnan(height_l):\n            height_l=0\n        if math.isnan(height_m):\n            height_m=0\n        if math.isnan(height_r):\n            height_r=0\n        \n        total = height_l + height_m + height_r\n\n        ax.text(left.get_x() + left.get_width()/2., height_l + 40, '{0:.1%}'.format(height_l/total), ha=\"center\", fontsize=7)\n        ax.text(middle.get_x() + middle.get_width()/2., height_m + 40, '{0:.1%}'.format(height_m/total), ha=\"center\",fontsize=7)\n        ax.text(right.get_x() + right.get_width()/2., height_r + 40, '{0:.1%}'.format(height_r/total), ha=\"center\",fontsize=7)\n\n\naxes[1].set_ylabel('Count')\naxes[1].set_xlabel('Time spent coding')\n\nplt.suptitle('How much do data science people code?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbea9f5befd69e0dea9e9ed450520352504657a4"},"cell_type":"markdown","source":"Very interesting insights!\n\nOnly **45%** of the respondents spend 50-75% of their time in coding. Out of these,\n* 63% of the respondents are data scientist\n* 23% of the respondents are data analyst\n* 13% of the respondents are research scientist\n\n**40%** of the respondents spend 25-49% of their time in coding. Out of these,\n* 55% of the respondents are data scientist\n* 27% of the respondents are data analyst\n* 18% of the respondents are research scientist\n\nOverall, we can conclude that **Data Scientist** spend maximum time coding things while **Research Scientist** spend a minimum amount of time in coding. To be honest, this thing literally irritates me. When you become a Research Scientist, it doesn't mean you should stop coding and tell others to code something for you. I don't know what it is like in other countries but in India, if a person is Ph.D. and he is working with Data Science team, most of the time, he/she will be giving telling others what to experiment with. A Ph.D. in India hardly writes any code and most of the time they exploit the teammates for getting their things done.\n\nDon't believe me? I will show you another plot right now to support my argument."},{"metadata":{"trusted":true,"_uuid":"6b4500cf0eb5d647b8777bcdf62043662d2c6189"},"cell_type":"code","source":"# In order to support my above argument, we will plot profiles and their education level\n# Select rows from educational dataframe corresponding to the current indices\nedu_level = edu_df[edu_df.index.isin(coding_df.index)]\n\n# Create a new column\ncoding_df['Q4'] = edu_level.values\n\n# Group by roles and get the count for experience \nfreq_df = coding_df.groupby(['Q4'])['Q6'].value_counts().unstack()\n\n# Convert the frequencies to percentage\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Get traces\ndata = get_traces(freq_df, pct_df, stacked=True, orientation='h', textposition='outside', opacity=0.7)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n     autosize=False,\n     width=1300,\n     height=500,\n     barmode='stack',   \n     margin=go.layout.Margin(\n                            l=400,\n                            r=0,\n                            b=100,\n                            t=50,),\n    title='How much do data science people code?',\n    yaxis=dict(title='Level of Education'),\n    xaxis=dict(title='Percentage'),\n    \n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3044d86f1e8f0f33d53edb97afa311f016129cba"},"cell_type":"markdown","source":"There you go. Did you see that? Most of the **Data Scientist** are having **Master's** or **Bachelor's** degree while most of the **Research Scientist** are having **Doctoral** degree. \n\nI am not saying all the research scientist are to be blamed and I am quite biased here because I have only seen the conditions in my country. The situation might be different in different countries. I would love to hear your views in the comments section\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"297832edc37d4b712a5f90885a3c0afcbd6daae0"},"cell_type":"markdown","source":"<a id=\"data-analysis\"></a>\n\n## 4.17 Writing code to do data analysis?\n\nQ24. How long have you been writing code to analyze data?"},{"metadata":{"trusted":true,"_uuid":"cff73f594677dac30f5681436e655fc65682797b"},"cell_type":"code","source":"# Selcting the columns of interest\neda_df = multiChoiceResp[['Q6', 'Q23', 'Q24']][1:].dropna()\n\n# Get the value counts\neda_df_counts = eda_df['Q24'].value_counts()\n\n# Perecntage values\neda_df_pct = (round(eda_df['Q24'].value_counts(normalize=True)*100,1)).values\neda_df_pct = [str(x)+'%' for x in eda_df_pct]\n\n# Visualize\ntrace0 = go.Bar(\n                y=eda_df_counts.values,\n                x=eda_df_counts.index,\n                orientation='v',\n                text = eda_df_pct,\n                textposition = 'outside',\n                marker=dict(\n                color='rgb(100,223,225)',\n                line=dict(color='rgb(5,4,150)',width=2.0,\n                         )),\n                opacity=0.5\n               )\n\nlayout = go.Layout(title='<b>How long have people been writing code to analyze data?</b>',\n                  autosize=False,\n                  width=800,\n                  height=500,\n                  margin=go.layout.Margin(\n                                        l=50,\n                                        r=200,\n                                        b=200,\n                                        t=50,),\n                  yaxis=dict(title='Count'),\n                \n                )\n\nfig = go.Figure(data=[trace0], \n                layout=layout)\niplot(fig, filename='eda_df-hover-bar', show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522345693b8209b2b854435686326b11b400dc46"},"cell_type":"markdown","source":"Given the fact that Data Science became popular only in the last few years, it is obvious that most of the people spending time on data analysis would fall in the category `1-5 years`.\n\nA more interesting thing to look at would be checking who among the top roles have been writing code for how long to analyze the datasets."},{"metadata":{"trusted":true,"_uuid":"2f305acd370fe8c91ce5846a565ba9f78b8db246"},"cell_type":"code","source":"# Selecting only top profiles only\nroles = ['Data Scientist', 'Data Analyst', 'Research Scientist']\nyears_to_consider = ['< 1 year', '1-2 years', '3-5 years', '5-10 years', '10-20 years']\neda_df = eda_df[eda_df['Q6'].isin(roles) & eda_df['Q24'].isin(years_to_consider)]\n\n# Groupby and get the normalize counts\ndf = eda_df.groupby('Q24')['Q6'].value_counts().unstack()\n\n# Reindex in the order we want\ndf = df.reindex(reversed(years_to_consider))\n\n# Get the percentage values for each bar in each group on X-axis\nfreq_df = np.round(df.divide(df.sum(axis=0), axis=1)*100)\n\n# Get traces\ndata = get_traces(df, freq_df, stacked=False, orientation='h', textposition='outside', opacity=0.6)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n    title='Who among DS,DA and RS have been writing code to analyze data for how long?',\n    xaxis=dict(\n        title='Count',\n        titlefont=dict(\n            family='Times New Roman',\n            size=18\n        )\n    ),\n    yaxis=dict(\n        title='Years',\n        titlefont=dict(\n            family='Times New Roman',\n            size=18,\n            )\n    )\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99e9dfd41881da5eb5801b52516e13393cb86199"},"cell_type":"markdown","source":"**Note:** The percentage on the bars represents the percent of all the respondents of that particular category(DA/DS/RS). Plus DA is short for Data Analyst, DS for Data Scientist and RS for Research Scientist.\n\nIn any case, a data scientist has spent maximum time doing analysis. Of course, this will consist of all those Data Analysts also who got promoted to Data Scientist position but forget about it for now.\n\n**The more you care about the data, the more you shine in data science**\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"8121477e5cceb99e3106bc48d945e84dd02a6433"},"cell_type":"markdown","source":"<a id=\"ml_methods_hist\"></a>\n## 4.18 How much time people have spent on ML?\nQ25. For how many years have you used machine learning methods (at work or in school)?"},{"metadata":{"trusted":true,"_uuid":"e30214e26745cde85e97cae58dc98644aa6a7f83"},"cell_type":"code","source":"# Select the question column\nml_df = multiChoiceResp['Q25'][1:].dropna()\n\n# Some cleansing\nml_df = ml_df.str.replace('I have never studied machine learning but plan to learn in the future', \n                          'Never studied but planned')\nml_df = ml_df.str.replace('I have never studied machine learning and I do not plan to', \n                          'Never studied and no plan')\n\n# order in which we want to display the categoricals\norder = ['< 1 year', '1-2 years', '2-3 years', '3-4 years', \n         '4-5 years', '5-10 years', '10-15 years', '20+years',\n         'Never studied but planned', 'Never studied and no plans']\n\n# visualize\nf,ax=plt.subplots(figsize=(12,5))\nax=sns.countplot(y=ml_df,order=order, color=color[3])\n\n# Plot percentage on the bars\npct_on_bars(ax, ml_df, orientation='h', offset=170, adjustment=2)\n    \nplt.ylabel('')\nplt.xlabel('Count')\nplt.title('For how many years people have used ML methods at work/school?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"602a79f21f25bab05e21ac283646e50cb3a4297e"},"cell_type":"markdown","source":"[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"4bf49aafc30a3a3bf10d7bbae3843b0f5a50c437"},"cell_type":"markdown","source":"<a id=\"dataScientist\"></a>\n## 4.19 Do people really think of themselves as data scientist?\n\nQ26. Do you consider yourself to be a data scientist?"},{"metadata":{"trusted":true,"_uuid":"8c0a201cb6862440b3ae67d2215c8b7bad204063"},"cell_type":"code","source":"# Select the question column\nds_df = multiChoiceResp['Q26'][1:].dropna()\n\n# Get the counts\nds_df_counts = ds_df.value_counts()\n\n# visualize\nf,ax=plt.subplots(figsize=(10,5))\nax=sns.countplot(x=ds_df, color=color[6], order=ds_df_counts.index)\n\n# Plot percentage on the bars\npct_on_bars(ax, ds_df, orientation='v', offset=70, adjustment=2, prec=1)\n    \nplt.ylabel('Count')\nplt.xlabel('Response')\nplt.title('Do you consider yourself to be a data scientist?')\nplt.xticks(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d4163085905cc03603aabdc15bc2441b0eb941f"},"cell_type":"markdown","source":"* **25%** of the respondents **definitely** consider themselves as data scientists.\n* Almost **57%** of the respondents are not fully confident but with some probability consider themselves as data scientists. This represents both categories: **Probably yes** category as well as **Maybe** category.\n* **17%** have a doubt on being considered as a data scientist while **8%** of the respondents are sure that they are not data scientists at all.\n\nThe above plot doesn't give us an idea about how many **true data scientist** are present in our dataset. Suppose a tech recruiter of some Muli-National Company(MNC for short) is also looking at this dataset in order to make sure that they are looking at the right country for hiring the right talent. How should the recruiter take a decision where to look for? Hence, we will divide the above plot among the top 5 countries(countries with the respondent count higher than the other countries)  "},{"metadata":{"trusted":true,"_uuid":"9fba6847a82fa85af4c0e919a22654ffb6f81181"},"cell_type":"code","source":"# Select the question column\nds_df = multiChoiceResp[['Q3', 'Q26']][1:].dropna()\n\n# select top 5 countries\ncountries_to_consider = top10_countries[:5]\n\n# Some cleansing\nds_df = ds_df[ds_df['Q3'].isin(countries_to_consider)]\nds_df['Q3'] = ds_df['Q3'].replace(['United States of America', 'United Kingdom of Great Britain and Northern Ireland'], \n                            ['USA', 'UK & Northern Ireland'])\n\n# Groupby and get the counts\ndf = ds_df.groupby('Q26')['Q3'].value_counts().unstack()\n\n# Get the percentage values for each bar in each group on X-axis\nfreq_df = np.round(df.divide(df.sum(axis=0), axis=1)*100)\n\n# Get traces\ndata = get_traces(df, freq_df, stacked=False, orientation='v', textposition='outside', opacity=0.8)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n    title='<b>Do you consider yourself to be a Data Scientist?</b>',\n    xaxis=dict(\n        title='Response',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Count',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='data-scientist-in-top-countries', show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b05ee5b9690f991c6c4163174ee10b258a970ef9"},"cell_type":"markdown","source":"*Beautiful plot, isn't it?*\n\n**Note:** The percentage on the bars in the above plot shows how much percent of the total respondents from a country falls in this category. **It is not the percent of all the respondents (irrespective of the country) falling into a particular category**. For example, if there were 1000 respondents from US, out of those 1000 how many belongs to the different categories `Definietly not`, `Definitely yes`,etc.\n\n* **Russia** tops the list in **Definitely yes** category with a whopping 35% followed by **USA(27%)** and **India(26%)**.\n* Even if you consider the **Probably yes** category, **Russia** still tops the list followed by **China** and **United Kingdoms and Northern Ireland**.\n* **China** and **India** are the places where the maximum number of **Maybe** cases are present.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"182be131084699904dd943b2b5295a324b829ec6"},"cell_type":"markdown","source":"Moving on, let's see what kind of data people are dealing with at work/school? What do you think? Given that most of the business is still running on tabular data, will `structured data` be at the top of the list, or with the hype that deep learning has created since past few years, is it going to be `unstructured data`? "},{"metadata":{"_uuid":"c6ce6def63714b3852ec5aec1df083417d43bee1"},"cell_type":"markdown","source":"<a id=\"datasets-type\"></a>\n## 4.20 What types of data do people deal with?\n\nQ32. What is the type of data that you currently interact with most often at work or school?"},{"metadata":{"trusted":true,"_uuid":"e323b92fbcc1339e63b2561cda9efb20b40a35eb"},"cell_type":"code","source":"# Select the question column\ndataset_df = multiChoiceResp['Q32'][1:].dropna()\n\n# Get the counts\ndataset_df_counts = dataset_df.value_counts()\n\n# visualize\nf,ax=plt.subplots(figsize=(10,5))\nax=sns.countplot(x=dataset_df, order=dataset_df_counts.index)\n\n# Plot percentage on the bars\npct_on_bars(ax, dataset_df, orientation='v', offset=50, adjustment=2, prec=1)\n    \nplt.ylabel('Count')\nplt.xlabel('Dataset type')\nplt.title('What is the type of data that you currently interact with most often at work or school?')\nplt.xticks(rotation=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f551832fcb983691791387819bad5a627209e692"},"cell_type":"markdown","source":"* 26% of the people deal with numerical data. I expected it more because most of the people I see are dealing with numbers only.\n\n* 20% of the data is tabular. This shouldn't come as a surprise as  huge amount of data is still stored in relational databases and in many companies, Excel is also prominent which is also tabular data.\n\n* Time Series data is also an important type of dataset. Finance sector deals primarily with time series data. Apart from this, Image data and Text data are the datasets which people generally deal with most of the time. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"72846b0f74af0ba75b02711d468f3b24e585fa4c"},"cell_type":"markdown","source":"Data Science and Machine Learning are very fast moving fields. The number of papers that are uploaded to [arXiv](https://arxiv.org/) on a daily basis is crazy. It is no surprise that the last 5 years has been the `era of deep learning` and the amount of progress we have made there is incredible. \n\nIn order to keep with the latest innovations and technologies, one needs to be an independent life-long learner. Learning is very very important, for personal as well as professional growth. The question is where do people spend most of their time learning these skills? Let's find out."},{"metadata":{"_uuid":"92abedac577fc47d8cf51a34054faa8d781be62c"},"cell_type":"markdown","source":"<a id=\"learningplatform\"></a>\n## 4.21 Learning platforms\n\nQ37. On which online platform have you spent the most amount of time?"},{"metadata":{"trusted":true,"_uuid":"90a6f84142f89024829de3cab03a494a364b662e"},"cell_type":"code","source":"# Select the question column\nplatform_df = multiChoiceResp['Q37'][1:].dropna()\n\n# Get the counts\nplatform_df_counts = platform_df.value_counts()\n\n# visualize\nf,ax=plt.subplots(figsize=(15,5))\nax=sns.countplot(y=platform_df, order=platform_df_counts.index, color=color[2])\n\n# Plot percentage on the bars\npct_on_bars(ax, platform_df, orientation='h', offset=80, adjustment=2, prec=1)\n    \nplt.xlabel('Count')\nplt.ylabel('Platform')\nplt.title('On which online platform have you spent the most amount of time?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43890da2ed9003d38810026bb6d8d76df75b7dfe"},"cell_type":"markdown","source":"* **Coursera** is the most popular learning platform among the respondents. Given that Coursera has some of the best courses and all the courses are cheap (comparatively), it shouldn't be surprising. Also, if you consider Machine learning, Andrew Ng ML course on Coursera is still one of the best courses. It is one of those courses that is recommended to everyone starting in the field of ML.\n\n* **DataCamp** is also cheap compared to others and has a wide variety of courses to learn from. Same goes for Udemy.\n* **Udacity** is another great platform to learn from. It has some of the best courses, popularly known as Nanodegrees, designed by industry guys working on those things in production. But the fact that those Nanodegrees are very very expensive, the number of learners enrolled for these courses are expected to be small.\n* **Kaggle learn** provides all the basic courses needed to get an overview of the baisc things and start Kaggling. \n* **FastAI** is great but currently it offers only two courses, one for Machine Learning and another for Deep Learning. Plus, I feel that the lectures are too long to concentrate on in a single go. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"8a42244215c5550ea8685724b9e75c3999005a10"},"cell_type":"markdown","source":"Next question is a million dollar question. As we all know that Data Science is a vast field and gaining expertise in this field is no easy task at all. It is more complex and much broad than software engineering. In fact, software engineering is an aspect of it. I am not going to discuss that aspect here as it requires lengthy explanation (but we can do it in comments if you want to). \n\nSo what makes you better at data science? Is it your academic achievements, independent projects or industrial experience? Let's find out."},{"metadata":{"_uuid":"1deb33fa7cbf5add8a13b5aa894766cf97b12075"},"cell_type":"markdown","source":"<a id=\"expertise_DS\"></a>\n## 4.22 Expertise in Data Science\n\nQ40. Which better demonstrates expertise in data science: academic achievements or independent projects?"},{"metadata":{"trusted":true,"_uuid":"7d02b552eac0045bc68409ec01548ad223cbd8ec"},"cell_type":"code","source":"# Selcting the columns of interest\nexpertise_df = multiChoiceResp[['Q4', 'Q40']][1:].dropna()\n\n# Get the value counts\nexpertise_df_counts = expertise_df['Q40'].value_counts()\n\n# Perecntage values\nexpertise_df_pct = (round(expertise_df['Q40'].value_counts(normalize=True)*100,1)).values\nexpertise_df_pct = [str(np.round(x).astype(int))+'%' for x in expertise_df_pct]\n\n# Visualize\ntrace0 = go.Bar(\n                x=expertise_df_counts.values,\n                y=expertise_df_counts.index,\n                orientation='h',\n                text = expertise_df_pct,\n                textposition = 'outside',\n                marker=dict(\n                color='rgb(300,100,25)',\n                line=dict(color='rgb(5,4,150)',width=2,\n                         )),\n                opacity=0.6\n               )\n\nlayout = go.Layout(title='<b>Which better demonstrates expertise in Data Science?</b>',\n                  autosize=False,\n                  width=900,\n                  height=500,\n                  margin=go.layout.Margin(\n                                        l=450,\n                                        r=0,\n                                        b=100,\n                                        t=100,),\n                   xaxis=dict(title='Count')\n                  )\n\nfig = go.Figure(data=[trace0], \n                layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dad6fed50f10da41620b1ee3cecea0a26f00a46b"},"cell_type":"markdown","source":"Almost **86%** of the respondents believe that personal projects are more or equally important compared to academic achievements to gain expertise in Data Science. Personally, I also feel that independent projects are more important. I am not saying academia doesn't play an important role. It surely does but until unless you are developing something, academia knowledge can't take you to the next level. **Actual learning comes when you implement something that you learned in theory**.\n\nLet us take this analysis to one more level deep. If 86% respondents think that independent projects are more/equally important as academia, it would be very interesting to know the level of education of these respondents and by the level of education, I mean the degree they hold. Let's plot that one as well.  "},{"metadata":{"trusted":true,"_uuid":"4c3a0480639d4e5704b834322d4d0e4c2201d73d"},"cell_type":"code","source":"# We will consider only three levels of education here\nedu_level_to_consider=[\"Doctoral degree\", \"Masters degree\", \"Bachelors degree\"]\nexpertise_df = expertise_df[expertise_df['Q4'].isin(edu_level_to_consider)]\n\n# Groupby and get the value counts\ndf = expertise_df.groupby('Q40')['Q4'].value_counts().unstack()\n\n# Get the percentage values for each bar in each group on X-axis\nfreq_df = np.round(df.divide(df.sum(axis=1), axis=0)*100)\n\n# Get traces\ndata = get_traces(df, freq_df, stacked=False, orientation='h', textposition='outside', opacity=0.6)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n    title='Which better demonstrates expertise in Data Science?',\n    xaxis=dict(title='Response'),\n    margin=go.layout.Margin(\n                            l=450,\n                            r=0,\n                            b=100,\n                            t=50,),\n    \n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e181c4b9005145acfd524be851292f2009359da3"},"cell_type":"markdown","source":"**53%** of the respondents who consider independent projects equally important as academic, hold Master's degree while only **22%** of the respondents who are PhDs think the same.\n\nAlmost **50%** of the respondents who think independent projects are much more important, hold a Master's degree while only **12%** of them are PhD\n\n**35%** of the respondents who think that independent projects are much less important, hold a PhD degree while only **48%** of them hold a Master's degree.\n\n\nAnother interesting thing to know is that how many respondents from a group with a certain level of education believes in what. For example, it is good to know that **22%** of the respondents who consider independent projects equally important hold a Doctoral degree. But what about the overall percentage of such respondents? Out of all the respondents having a Doctoral degree, how many of them believe that independent projects are equally important as academics?\n\nLet's find out this percentage as well."},{"metadata":{"trusted":true,"_uuid":"c2de95ca22734cb44d16ed684b3beeb51f6986e8"},"cell_type":"code","source":"# Get the percentage values for each bar in each group on X-axis\nfreq_df = np.round(df.divide(df.sum(axis=0), axis=1)*100)\n\n# Get traces\ndata = get_traces(df, freq_df, stacked=False, orientation='h', textposition='outside', opacity=0.6)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n    title='Which better demonstrates expertise in Data Science?',\n    xaxis=dict(title='Response'),\n    margin=go.layout.Margin(\n                            l=450,\n                            r=0,\n                            b=100,\n                            t=50,),\n    \n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03d88e432dd623223292547d45a1229839d22d8d"},"cell_type":"markdown","source":"Now we can actually look at the plot and tell how much percentage of a group is lean to which opinion.\n\n[(Click here to go to the contents section at the top)](#contents)\n\n\n\nAnyways moving on, let us take a look on one of the most important questions in this survey. "},{"metadata":{"_uuid":"1fe2c8d4753f4eb7ed9f0ebdc492361d2bcd7ff2"},"cell_type":"markdown","source":"<a id=\"fairness\"></a>\n## 4.23 Do people care about bias in datasets/algorithms?\n\nQ43. Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?"},{"metadata":{"trusted":true,"_uuid":"2b8aeb11e75c9a185786bcc8ba05ca0f88867229"},"cell_type":"code","source":"# Select the question column\nbias_df = multiChoiceResp['Q43'][1:].dropna()\n\n# Get the counts\nbias_df_counts = bias_df.value_counts()\n\n# visualize\nf,ax=plt.subplots(figsize=(10,7))\nax=sns.countplot(x=bias_df, order=sorted(bias_df_counts.index), color=color[4])\n\n# Plot percentage on the bars\npct_on_bars(ax, bias_df, orientation='v', offset=50, adjustment=2, prec=0)\n    \nplt.ylabel('Count')\nplt.xlabel('Time spent(%)')\nplt.title('What percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc2e02a7e2aa18c1aa839d8ead83f2f3f5bb4cbd"},"cell_type":"markdown","source":"**~53%** of the respondents spend less than 10% of their time in investigating unfair bias in dataset or algorithm. This is certainly bad. I am not saying that you need to spend 50% or more time to analyze potential unfair bias but you should at least spend 20-30% of the time in analyzing all these things, especially when you are developing things for production. Bias in datasets is very common!\n\nMost of the beginners/ novice machine learning engineers take the data and run different algorithms on it without giving much thought about the dataset or even about the algorithm. This is a bad practice and you should learn looking at your dataset carefully before making any assumption. Time spent here will be worth in the long run.\n\n#### 4.22.1 Who among the DA,DS and RS spends how much time on finding bias?"},{"metadata":{"trusted":true,"_uuid":"ef2b911096f6d725301301a38248af3c9238f102"},"cell_type":"code","source":"# Select the question column\nbias_df = multiChoiceResp[['Q6','Q43']][1:].dropna()\n\n# We will consider only three roles here : Data Analyst, Data Scientist, Research Scientist\nbias_df = bias_df[bias_df['Q6'].isin(roles)]\n\n# Get the frequency counts for each category\nfreq_df = bias_df.groupby(['Q43'])['Q6'].value_counts().unstack()\n\n# Get the percentages across each category for each role\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Get traces\ndata = get_traces(freq_df, pct_df, stacked=True, orientation='v', textposition='auto', opacity=0.7)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n     autosize=False,\n     width=900,\n     height=500,\n     barmode='stack',   \n     margin=go.layout.Margin(\n                            l=50,\n                            r=0,\n                            b=50,\n                            t=50,),\n    title='What % of your data projects involved exploring unfair bias in the dataset and/or algorithm?',\n    xaxis=dict(title='Time(%)'),\n    yaxis=dict(title='Percentage'),\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f54b6ba5ac56dfdd6d528c9323ae3c515443fcd"},"cell_type":"markdown","source":"I expected the Research Scientist to spend more time on finding bias as compared to other roles but to my surprise it isn't the case. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"3d429cc72e4e5e3377514ac85da15b3c572d9180"},"cell_type":"markdown","source":"<a id=\"modelinsights\"></a>\n## 4.24 Time spent exploring model insights\n\nQ46. Approximately what percent of your data projects involve exploring model insights?"},{"metadata":{"trusted":true,"_uuid":"d519699e112b65c8118ceecea327f03ce9db07b0"},"cell_type":"code","source":"# Select the question column\nmodel_insights_df = multiChoiceResp['Q46'][1:].dropna()\n\n# Get the counts\nmodel_insights_df_counts = model_insights_df.value_counts()\n\n# visualize\nf,ax=plt.subplots(figsize=(10,7))\nax=sns.countplot(x=model_insights_df, order=sorted(model_insights_df_counts.index), color=color[2])\n\n# Plot percentage on the bars\npct_on_bars(ax, model_insights_df, orientation='v', offset=30, adjustment=2, prec=0)\n    \nplt.ylabel('Count')\nplt.xlabel('Time spent(%)')\nplt.title('Approximately what percent of your data projects involve exploring model insights?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"784b36bf73de6b324b25ae1c1e7cb65a170b2e9c"},"cell_type":"markdown","source":"This is a tricky question. Exploring model insights is important for making progress in building better and efficient models but there is a catch. For example, It also depends on how much time does the project manager/company allows spending time on it. They want the same model to improve or rhey want 70 models and pick the best out of them? In any case, people should spend time exploring model insights. Also, I am not saying that you should dedicate 70-80% time on it but you should at least spend 20-25% of the time analyzing what is happening behind the scene\n\n* **45%** of the respondents spend less than 30% of the time exploring model insights.\n* Only **7%** of the respondents spend 50% of the time exploring model insights\n* Almost **28%** of the respondents spend more than 50% of the time exploring model insights. \n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"f5674d4571d9e778e54881b2ad65045ab2081b87"},"cell_type":"markdown","source":"<a id=\"explainableML\"></a>\n## 4.25 Are ML models black boxes?\n\nQ48. Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?\n\nI have heard many people saying that they don't want to apply machine learning to business because they don't know how to interpret the algorithms and how to explain the final outcome. A majority of them considers Machine Learning models as pure black box. \nLet's see what the respondents think about these machine learning models."},{"metadata":{"trusted":true,"_uuid":"9ca62c499b53363ce269c5eae92345f7561cd47b"},"cell_type":"code","source":"# Select the column of interest\ndf = multiChoiceResp['Q48'][1:].dropna()\n\n# Get a frequency count\nfreq_counts = df.value_counts()\n\n# Define a trace for the plotly pie chart\ntrace = go.Pie(labels=freq_counts.index, values=freq_counts.values,\n               hoverinfo='label+value', \n               textfont=dict(size=20),\n               marker=dict( line=dict(color='#000000', width=2)))\n\n# Define a layout for the figure on which you are going to plot your data\nlayout = go.Layout(\n            autosize=True,\n            margin=go.layout.Margin(\n                            l=10,\n                            r=100,\n                            b=10,\n                            t=50,),\n            legend=dict(\n                x=1,\n                y=1,\n                traceorder='normal',\n                font=dict(\n                    family='sans-serif',\n                    size=12,\n                    color='#000'\n                ),\n                bgcolor='#E2E2E2',\n                bordercolor='#FFFFFF',\n                borderwidth=2\n            )\n)    \n\n# Define the figure object\nfig = go.Figure(data=[trace], layout=layout)\n\n# Plot\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa27785b5864c5961529c8419c244279bc6211b1"},"cell_type":"markdown","source":"**48%** of the respondents think that they understand the outputs of **many but not all** ML models and that they can explain it to someone else also. This is actually good. I expected this number to be low but surprisingly this is good. It is acceptable that not everyone understands every other ML model.\n\n**12%** of the respondents treat ML models as **pure black box** while **22%** of them consider ML models **black boxes that can be interpreted by experts only**. This shows that we need to provide better courses, tutorials, etc. along with a good amount of mathematics, if not in depth so that people can get a better understanding of what a model is doing.\n\nOnly **9%** of the respondents can explain outputs of most of the ML models.\n\n\nLet's check if we can find out how many respondents among `Data Analysts`, `Data Scientist` and `Research Scientist` consider ML models as black box. For this thing we need to make some changes to the data.\n\nIn order to simplify things, here are all the steps that we are gonna apply:\n* Selct the columns corresponding to Q6(roles) and Q48(current one).\n* Combine different categories in Q48 that we saw above into two groups: Black boxes and Not black boxes\n* Select only DA,DS and RS as the roles in Q6\n* Get the value counts after doing a groupby on roles\n* Plot the values"},{"metadata":{"trusted":true,"_uuid":"c3b92acb293beefa8c69e0c048c539682496a160"},"cell_type":"code","source":"def divide_categories(x):\n    if x in ['I am confident that I can understand and explain the outputs of many but not all ML models',\n            'I am confident that I can explain the outputs of most if not all ML models']:\n        x = 'Not black boxes'\n    else:\n        x = 'Black boxes'\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49cfd23e65eb283008b4113eef1b1b21114d0618"},"cell_type":"code","source":"# Select the question column\ndf = multiChoiceResp[['Q6','Q48']][1:].dropna()\n\n# We will consider only three roles here : Data Analyst, Data Scientist, Research Scientist\ndf = df[df['Q6'].isin(roles)]\n\n# Drop the rows where people didn't provide opinion on explainable ML models\ndf = df[~df['Q48'].isin(['I do not know; I have no opinion on the matter'])]\n\n# divide into two categories\ndf['Q48'] = df['Q48'].apply(divide_categories)\n\n# Get the frequency counts for each category\nfreq_df = df.groupby(['Q6'])['Q48'].value_counts().unstack()\n\n# Get the percentages across each category for each role\npct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)*100\n\n# Get traces\ndata = get_traces(freq_df, pct_df, stacked=False, orientation='v', textposition='auto', opacity=0.8)\n\n# Define the layout for plotly figure\nlayout = go.Layout(\n     autosize=False,\n     width=900,\n     height=500,   \n     margin=go.layout.Margin(\n                            l=70,\n                            r=0,\n                            b=50,\n                            t=50,),\n    title='Do you consider ML models as black boxes?',\n    xaxis=dict(title='Role'),\n    yaxis=dict(title='Count'),\n)\n\n# Visualize\nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7278b79866bb2b307ef252a29ce188ed15523d5"},"cell_type":"markdown","source":"* **74%** of the Data Scientists and **66%** of the Research Scientists **don't consider ML models as black boxes**, which is great.\n\n* **45%** of the Data Analysts consider ML models as **black boxes** which is somewhat expected. Given the fact that most of the time a Data Analyst is either wrangling the data or cleaning the data in order to find insights, a data analyst hardly deals with ML models actually.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"510dba93ff526e824577cb4d21bf9ab31f8f1e8a"},"cell_type":"markdown","source":"Let's move to those parts of the survey where a single question was split into many parts"},{"metadata":{"trusted":true,"_uuid":"e3eaa875da201561a5fe7698ff26402d4e5d3a57"},"cell_type":"code","source":"## Checking all the columns that are split into multiple parts\ncols = multiChoiceResp.columns.tolist()\nques = set()\n#cols = [x for x in cols if 'Part' in x]\nfor col in cols:\n    col = col.split('_')[0]\n    ques.add(col)\nprint(\"Number of questions split into different parts \", len(ques))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65492d0925d07aa7cd727c49b77b21d0397524f5"},"cell_type":"markdown","source":"<a id=\"ide\"></a>\n## 4.26 Which IDE do people prefer for their work?"},{"metadata":{"trusted":true,"_uuid":"0ac6d83c9a9456a32c92d70b5103cac641567567"},"cell_type":"code","source":"# A handy danyd function to rename columns which are parts of a single question\ndef rename_columns(df, column_names, custom_name=None, index=None, exclude_others=True):\n    column_names = column_names.apply(lambda x: x.split(\"-\", 2)[-1]).tolist()\n    column_names = [x.strip() for x in column_names]\n    if custom_name:\n        if index!= None:\n            column_names[index] = custom_name\n        else:\n            return \"Index value must be passed to use a custom name for a column\"\n    \n    # Rename columns\n    df.columns = column_names \n    \n    # Whether to remove \"None\" and \"Other\" categories\n    if exclude_others:\n        columns_to_consider = [x for x in column_names if x not in ['None', 'Other']]\n    else:\n        columns_to_consider = column_names\n    \n    return columns_to_consider","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8fc5d1179dc9553eb49038f5e62baa38d4d8878"},"cell_type":"code","source":"# We will select the columns corresponding to roles and the question in which IDE pref was asked in the survey\ndf = multiChoiceResp[[x for x in cols if x=='Q6' or 'Q13_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, custom_name=\"Current Role\", index=0, exclude_others=True)\n\n\ndf = df[columns_to_consider][1:] \n\n# Do a groupby on current role and get count for each column corresponding to each group\nfreq_df = df.groupby('Current Role')[columns_to_consider[1:]].count()\npct_df = np.round(freq_df.divide(freq_df.sum(axis=1), axis=0)*100)\n\n\n# Sort the dataframe values\n#pct_df.values.sort()\n\n# values for annotations\npct_df_text = pct_df.applymap(lambda x: str(int(x)) + \"%\") \n\n\n# Define a annotated figure object\nfig = ff.create_annotated_heatmap(z=pct_df.values.tolist(), \n                                  y=pct_df.index.tolist(), \n                                  x=pct_df.columns.tolist(), \n                                  annotation_text=pct_df_text.values.tolist(), \n                                  colorscale='Viridis')\n\n\n# Define the layout for the fig created above\nfig.layout.title = \"Which IDE people prefer to work with?\"\nfig.layout.height = 800\nfig.layout.width = 1000\nfig.layout.margin.l = 200\nfig.layout.margin.t = 150\n\n# Visualize\niplot(fig,show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c5bc160aef733f9908e0abdf3fee6958e246465"},"cell_type":"markdown","source":"It shouldn't come as a surprise that the most widely used IDE is `Jupyter/Ipython`. For each category the amount of usage of a particular IDE will be different, hence I plotted this heatmap, so that you look for a particular group and check what is the usage of each IDE in that group.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"054e8e2d68f0618dec8119a0d673eae4d58ecf99"},"cell_type":"markdown","source":"<a id=\"notebooks\"></a>\n## 4.27 Popular hosted notebooks\n\nQ14. Which of the following hosted notebooks have you used at work or school in the last 5 years?"},{"metadata":{"trusted":true,"_uuid":"1724ef2abaf33d21eb05174e870b8c58d1b6bb99"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q14_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=True)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:]\n\n# Get the count and plot them\ndf = df.count().sort_values()\n\n# Get percentages\npct = np.round(df.divide(df.sum())*100,2).values\npct = [str(x) + \"%\" for x in pct]\n\n\n# Visualize\ntrace = go.Bar(\n                x=df.values,\n                y=df.index,\n                orientation='h',\n                text = pct,\n                textposition = 'outside',\n                marker=dict(\n                color='rgb(100,225,25)',\n                line=dict(color='rgb(5,4,150)',width=2,\n                         )),\n                opacity=0.6\n               )\n\nlayout = go.Layout(title='<b>Popular hosted notebooks</b>',\n                  autosize=False,\n                  width=900,\n                  height=500,\n                  margin=go.layout.Margin(\n                                        l=150,\n                                        r=0,\n                                        b=100,\n                                        t=50,),\n                   xaxis=dict(title='Count')\n                  )\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1303eb5ebb6222bd170eced16c2951983c17c4c"},"cell_type":"markdown","source":"**Kaggle Kernels** are the most popular hosted notebooks. In fact, kaggle kernels are the best in class. They come up with pre-installed packages(almost all of them) and within minutes you can start working on the problem sets. One of the best things is that they come with GPU support as well. So, you don't have to worry about choosing the type of instances on cloud services for your use case. Just fire up a Kaggle kernel and you are good to go.\n\nApart from Kaggle Kernels, the other two platforms that are more famous are **JupyterHub/Binder** and **Google Colab**. No doubt, JupyterHub is a great tool, especially for collaboration purposes. **Colab** is a powerful and handy tool. You can directly access data from your drive, you can reference notebooks hosted on GitHub and open it up in Colab within a single click. Colab can be used by researchers to open source their code along with the papers. **Talk less, code more**. This step can help us handling the **reproducibility issues**. The downside with Colab is that the reading data from drive is very slow.\n\n**Google Datalab** and **Azure Notebooks** share the same percentage of taste among the users while **Paperspace, Floydhub, Crestle** and **Domino Datalab** all are having a very small share among the users. Why? Maybe because of the limited functionalities...maybe!\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"2765b9943e1a4bfc72294dff4a9f534cd71cb37b"},"cell_type":"markdown","source":"<a id=\"datasetsfinder\"></a>\n## 4.28 Public Datasets\n\nQ33. Where do you find public datasets?"},{"metadata":{"trusted":true,"_uuid":"24f0976c4df20661f7ec68214a7a30198bd45488"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q33_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=True)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:]\n\n# Get the count and plot them\ndf = df.count().sort_values()\n\n# Define trace for plotly chart\ntrace = go.Pie(labels=df.index, values=df.values, showlegend=True)\n\n# Layout of the fig\nlayout = go.Layout(title='Where do you find public datasets?',\n        margin=go.layout.Margin(\n                                l=10,\n                                r=200,\n                                b=10,\n                                t=50,),\n                legend=dict(\n                    x=1,\n                    y=1,\n                    traceorder='normal',\n                    font=dict(\n                        family='sans-serif',\n                        size=12,\n                        color='#000'\n                    ),\n                    bgcolor='#E2E2E2',\n                    bordercolor='#FFFFFF',\n                    borderwidth=2\n                )\n    )\n\n# Plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4917270e2459bc63498fd3817bcea4643d74a9bf"},"cell_type":"markdown","source":"* Data Aggregation platforms, like **Kaggle Datasets**, are the biggest source for finding public datasets.\n* **Google Search** on the other hand, is the source for searching datasets for 15% of the respondents.\n* **Google Dataset Search**, which was launched recently, is another good source for finding datasets. The good thing about Google Dataset Search platform is that you get all the details related to a dataset, like license info, author, etc. Since Kaggle is part of Google now, we can say that almost 30-35% of the respondents use these platforms for dataset search. Regarding the **Kaggle Datasets**, I would say this is the best place to start with. Why? You not only get the dataset, but you can also start a kernel from the Dataset page itself and start working on it privately/publicly within seconds.\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"044ac8317fa9e95a56b47ae581dc8ce3c55331f1"},"cell_type":"markdown","source":"<a id=\"timespentondiffaspects\"></a>\n## 4.29 Time spent on different stages in a Data Science project\n\nQ34. During a typical data science project at work or school, approximately what proportion of your time is devoted to the following?"},{"metadata":{"_uuid":"aa07c005a3811eda1c471275a1863a0d7fc3c4e1"},"cell_type":"markdown","source":"This is an important question. Although we all know that gathering and cleaning data are the two most consuming parts of any data science project, it will still be good to know how much time on an average people spend on each stage.<br> \n**Note**: There can be multiple ways to view this data and depends on what you are looking for. For example:\n* You can take mean value for each stage to represent the time spent on that stage.\n* Mean isn't a good measure when dealing with outliers. So, you can replace the mean with the median for each stage.\n* For each respondent, get the percentage of time spent on each stage. Do this for all the respondents and then take the mean for each column correspondingly. "},{"metadata":{"trusted":true,"_uuid":"1406c28f390ea30168dc09e4d923d8d11570c5b1"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q34_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=False)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:].dropna().astype(np.float32)\n\n# Get the median values for each column. The median value will be our representative value for that particular stage\nmean_df = np.round(df.mean().sort_values(ascending=False))\n\n# Get the phases and values(time spent) corresponding to each phase\nvalues = mean_df.values.tolist()\nphases = mean_df.index.tolist()\n\n# Get the traces for plotly funnel chart\nlabel_trace, value_trace, shapes = draw_funnel_chart(values, phases, colors=None, \n                                                     plot_width=50, section_d=10, section_h=70)\n\n\ndata = [label_trace, value_trace]\n\n# Define layout\nlayout = go.Layout(\n    title=\"Average time (%) spent on different stages in a typical Data Science project\",\n    titlefont=dict(\n        size=15,\n        color='rgb(203,203,203)'),\n    margin=go.layout.Margin(l=50,\n                            r=0,\n                            b=10,\n                            t=50,),\n    shapes=shapes,\n    height=500,\n    width=1000,\n    showlegend=False,\n    hovermode='closest',\n    paper_bgcolor='rgba(44,58,71,1)',\n    plot_bgcolor='rgba(44,58,71,1)',\n    xaxis=dict(\n        showticklabels=False,\n        zeroline=False,\n    ),\n    yaxis=dict(\n        showticklabels=False,\n        zeroline=False)\n)\n \nfig = go.Figure(data=data, layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dac1b49714610aab6d12b7708020cad15ce8648"},"cell_type":"markdown","source":"If we combine **gathering** and **cleaning** processes into a single stage, which is how we count it in the real world (the amount of time required to prepare the data from the process of gathering data to cleaning it for usage), then these two stages consumes almost **40%** of a typical data science project.\n\n*Some of the labels aren't visible fully in the above funnel chart but you can hover over the labels to see them.*\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"b3248d67a02eca84d388262222f95c776fdc91b9"},"cell_type":"markdown","source":"<a id=\"databases\"></a>\n## 4.30 Popular Relational Databases\n\nQ29. Which of the following relational database products have you used at work or school in the last 5 years? "},{"metadata":{"trusted":true,"_uuid":"320ddaf2dfc6a5099ade2fd15a0f484dd655ae14"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q29_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=True)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:]\n\n# Get the count and plot them\ndf = df.count().sort_values()\n\n# Get percentages\npct = np.round(df.divide(df.sum())*100,2).values\npct = [str(x) + \"%\" for x in pct]\n\n\n# Visualize\ntrace = go.Bar(\n                x=df.values,\n                y=df.index,\n                orientation='h',\n                text = pct,\n                textposition = 'auto',\n                marker=dict(\n                color='rgb(35,35,65)',\n                line=dict(color='rgb(0,0,0)',width=2,\n                         )),\n                opacity=0.6\n               )\n\nlayout = go.Layout(title='Databases used by people over last 5 years',\n                  autosize=True,\n                  width=1500,\n                  height=800,\n                  margin=go.layout.Margin(\n                                        l=200,\n                                        r=0,\n                                        b=100,\n                                        t=50, pad=10),\n                   xaxis=dict(title='Count'),\n                  )\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cb963fd65adc306556b4fa1b2a95dc9428c33e6"},"cell_type":"markdown","source":"* No doubt that even today, **MySQL** is the most popular relational database. With a whopping 23% overall usage, MySQL is at the top position.\n* **PostgresSQL** and **SQLite** are at the second and third position respectively. Anyone who has done web development, especially the backend, must have used either of these three once in their life.\n* **Microsoft SQL server** is also good and I am surprised a little bit by the huge difference in the usage of MySQL and MS SQL server.\n\n[(Click here to go to the contents section at the top)](#contents)\n"},{"metadata":{"_uuid":"73a5aa75390b908417af114a50b0fab602be6e0d"},"cell_type":"markdown","source":"<a id=\"ml_products\"></a>\n## 4.31 What are all the machine learning products people have used?\n    \nQ19. Which of the following machine learning products have you used at work or school in the last 5 years? "},{"metadata":{"trusted":true,"_uuid":"7a12f8340a4e06dc964f38da255b185ac25e92ed"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q28_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=True)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:]\n\n# Get the count and plot them\ndf = df.count().sort_values()\n\n# Get percentages\npct = np.round(df.divide(df.sum())*100,2).values\npct = [str(x) + \"%\" for x in pct]\n\n\n# Visualize\ntrace = go.Bar(\n                x=df.values,\n                y=df.index,\n                orientation='h',\n                text = pct,\n                textposition = 'outside',\n                marker=dict(\n                color='rgb(10,20,225)',\n                line=dict(color='rgb(0,0,0)',width=2,\n                         )),\n                opacity=0.6\n               )\n\nlayout = go.Layout(title='Machine Learning Products',\n                  autosize=False,\n                  width=1500,\n                  height=800,\n                  margin=go.layout.Margin(\n                                        l=300,\n                                        r=0,\n                                        b=10,\n                                        t=50,\n                                        pad=10),\n                   xaxis=dict(title='Count'),\n                  )\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac8994b80591fb5692830fbfed2e439cbf09a27a"},"cell_type":"markdown","source":"<a id=\"barriersreprod\"></a>\n## 4.32 Barriers in reproudcibility\n\nQ50. What barriers prevent you from making your work even easier to reuse and reproduce?"},{"metadata":{"trusted":true,"_uuid":"f4e53b79ffeace46b3ae319a7d6507d686581c9a"},"cell_type":"code","source":"df = multiChoiceResp[[x for x in cols  if 'Q50_Part' in x]]\n\n# Rename the columns and remove \"None\" and \"Other\"\ncolumn_names = df.iloc[0, :]\ncolumns_to_consider = rename_columns(df, column_names, exclude_others=True)\n\n# select only relevant columns\ndf = df[columns_to_consider][1:]\n\n# Get the count and plot them\ndf = df.count().sort_values()\n\n# Define trace for plotly chart\ntrace = go.Pie(labels=df.index, \n               values=df.values, \n               showlegend=True,\n               marker=dict(\n                           line=dict(color='#000000', width=2))\n              )\n\n# Layout of the fig\nlayout = go.Layout(title='<b>What barriers prevent you from making your work even easier to reuse and reproduce?</b>',\n                   margin=go.layout.Margin(\n                                l=10,\n                                r=200,\n                                b=10,\n                                t=100,),\n                   legend=dict(\n                                x=1,\n                                y=1,\n                                traceorder='normal',\n                                borderwidth=5\n                            )\n                )\n\n# Plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58a75592d32cfeadd396f190b2e050a86b7a2fc1"},"cell_type":"markdown","source":"**~33%** of the respondents think that reproducibility is **too time consuming**. I think this is a very lame excuse. If your work is important, how the world is going to believe you until unless your results are reproducible? Also, if you can't reproduce it, how is it different than just being **lucky**? I personally don't think that reproducibility is hard. If you make sure that you are taking care of things like `seed`, `rng` in your code, it is no big deal. I agree that the results can't be exactly the same but minute differences doesn't matter much (until unless it is SOTA). For example, in the real world, if a paper/code claims that they can achieve 90% accuracy with it and previous works were around 87-88%, even if I am getting 89% accuracy, I would be fine with it. Even 88.5% would work.\n\n**~19** of the respondents say that they **don't get enough incentives to share their work**. I understand that when you are underpaid and still you do amazing work, it gets frustrating sharing your work. It feels like that you are throwing away your work for nothing. But not everything has to be related to money. **If you feel, you have done good work, go and say it out loud.**\n\n**~14%** of the respondents think that reproducibility **requires too much of technical knowledge**. No, it doesn't require too much of technical knowledge but it requires you to know about all the things that could have biased your results. **Validation isn't just science, it's an art**.\n\n**10%** of the respondents are afraid that **others will use their work without giving proper credits**. *This one is a legitimate concern.* Almost all the time people talk about reproducibility but I have hardly seen anyone discussing acknowledgement. This is a reflection of your bad character as a data science person. **Acknowledging someone's work doesn't make you less capable, rather it shows that you respect the great work done by others**\n\n\n[(Click here to go to the contents section at the top)](#contents)"},{"metadata":{"_uuid":"4cf19ecb006cfd1fcdcb05e87ef561289345034d"},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n## 4.33 Conclusion"},{"metadata":{"_uuid":"cb508cb2833ef83d4f447f5b79e383d8833e8927"},"cell_type":"markdown","source":"That's all folks! I think we have analyzed all the important questions asked in this survey. I will conclude the main points here that we already discussed and analyzed in detail in the notebook:\n\n* Number of women working in Data Science has increased in 2018 but the growth is slow and we need to do more to remove the gender bias from this niche field.\n* Gender distribution in younger age-group is more pronounced and we need to address this before things get bad again.\n* Each country has the gender bias problem and the ratio is screwed in each and every country.\n* Number of respondents interested in getting Masters' degree has increased since 2017. \n* Most of the businesses have just started to explore Machine Learning and only a handful of them have deployed Machine Learning in production.\n* Python is the topmost preferred language for doing data science and machine learning and it is the most recommended one as well.\n* In terms of ML libraries, scikit-learn, Tensorflow, Keras, Xgboost and PyTorch are dominating over others\n* Russia is the country where most of the respondents are confident about considering themselves as a data scientist.\n* Numerical data, text data, tabular data, time-series data and image data are the kind of datasets most of the people deal with.\n* Coursera is the most popular online learning platform among the respondents.\n* Most of the people spend less than 10% of their project time in finding bias in dataset/algorithm. This is a bad habit and we need to teach people the importance of it.\n* More than 50% of the respondents believe that ML models aren't black boxes and their outputs can be interpreted.\n* Jupyter is the most popular IDE among all the respondents.\n* Kaggle Kernels are the most popular hosted notebooks.\n* Kaggle Datasets, Google Search and Google Dataset Search are the best sources to search for any kind of dataset.\n* Gathering, cleaning and visualizing data are the most expensive stages of a data science project.\n* Tabular databases like MySQL, PostgresSQL, etc dominate the database market.\n* A large group of respondents think that reproducibility is too time-consuming while others think that it requires too much of knowledge. Not giving proper credits to someone's work is another factor which drives people away from reproducibility.\n\n\nWe, as a data science community, are responsible(directly or indirectly) for all the things that are happening in this field and only we have the power to make things right. It is acceptable that not everyone can bring a big change but you can always raise your voice against wrong things. \n\n\nI hope you enjoyed the analysis and might have learnt something new about all the libraries I have used. There is a lot of code in there and if you find any mistake in it or if you have any suggestion/feedback, please do let me know in the comments section. **Happy Kaggling!**"},{"metadata":{"trusted":true,"_uuid":"8c41cdabe980794a7f30bcfd99d85a7dd3996411"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}