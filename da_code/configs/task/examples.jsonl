{"id": "di-text-001", "type": "Data Insight", "instruction": "Use the mean to fill in missing values, then identify the countries with the highest and lowest population densities in the dataset. Please output the answers below in JSON format by filling in the placeholders in:\n'''\n{\n  \"highest country\": [...],\n  \"lowest country\": [...]\n}\n'''", "hardness": "Medium", "post_process": []}
{"id": "di-text-002", "type": "Data Insight", "instruction": "Use the mean to fill in missing values and identify which country allocates the highest percentage of their land for agricultural purposes. Please directly provide the answer below as text in the format:\n'''\n{\n  \"highest country\": [...],\n  \"Agricultural Land %\": [...]\n}\n'''", "hardness": "Medium", "post_process": []}
{"id": "di-text-003", "type": "Data Insight", "instruction": "Use the mean to fill in missing values and identify the top 5 countries with the highest and lowest birth rates. Sort them from highest to lowest. Please output the answers below in JSON format by filling in the placeholders in:\n\"\"\"\n{\n  \"Top 5 countries with High Birth Rate\": [...],\n  \"Top 5 countries with Lowest Birth Rate\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-004", "type": "Data Insight", "instruction": "Use the label mapping from tips.md to transform the labels and calculate the most frequent experience level and its corresponding ratio (as a decimal). Please directly provide the answers below as text in the following format:\n\n\"\"\"\n{\n  \"The most frequent experience level\": [...],\n  \"ratio\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-005", "type": "Data Insight", "instruction": "Compile the total gold medals won by each country in the Olympics. List the top three countries by gold medal count in descending order using this format:\n\"\"\"\n{\n  \"Regions\": [...],\n  \"Medal\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "di-text-006", "type": "Data Insight", "instruction": "Find out which three sports China has won the most gold medals in. Work out what percentage of their total golds each sport makes up. List the sports from highest to lowest percentage and format it like this:\n\"\"\"\n{\n    \"Sports\": [...],\n    \"Ratio\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-007", "type": "Data Insight", "instruction": "Look at the student salary data and figure out what salary 75% of the students make less than. The options are A: 9k, B: 10K, C: 8k, D: 5k. Pick the right answer and format it like this:\n\n\"\"\"\n{\n\"answer\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-008", "type": "Data Insight", "instruction": "Determine the top interests or activities for men and women. Present your findings in the following format:\n\n\"\"\"\n{\n    \"men\": [...],\n    \"women\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-009", "type": "Data Insight", "instruction": "Could you please analyze the dataset to calculate the correlations between different features? Determine if the common criticism of \"The World Happiness Report\" having a high focus on GDP and strongly correlated features like family and life expectancy is valid. Present your response in the following format(Yes or No):\n\n\"\"\"\n{\n\"answer\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-010", "type": "Data Insight", "instruction": "Examine the dataset to find out the year when India's economy was at its highest, the year it declined, and the year it started to recover. Format your findings as follows:\n\n\"\"\"\n{\n    \"peak\": [...],\n    \"dropped\": [...],\n    \"recovered\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-011", "type": "Data Insight", "instruction": "Could you please analyze the matches between Argentina and Brazil? Record the total number of matches, the number of wins for each team, and the number of draws. Indicate if Brazil has won more matches (\"Yes\" or \"No\"). Present your findings in the following format:\n\n\"\"\"\n{\n    \"Total Matches between Argentina and Brazil\": [...],\n    \"Arg Wins\": [...],\n    \"Brazil Wins\": [...],\n    \"Draws\": [...],\n    \"Result\": [...]\n}\n\"\"\"", "hardness": "Hard", "post_process": []}
{"id": "di-text-012", "type": "Data Insight", "instruction": "Analyze the dataset to determine which main category (‘category_1’) contains the highest-selling product, and identify the sub-categories (‘category_2’) of the top two highest-selling products. Please directly provide the answers below in the following format:\n\n\"\"\"\n{\n    \"Most Products by main category\": [...],\n    \"Top 2 Most Products by Sub-Category\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-013", "type": "Data Insight", "instruction": "You are working on a project for a music analytics company. Verify the statement \"more energetic tracks tend to be more loud\" using the given dataset. Additionally, assess the correlation between loudness and energy levels of tracks. Answer with \"Y\" if the statement is true, otherwise \"N\". For the correlation, respond with \"Positive\" if it is positive or \"Negative\" if it is negative. Format your response in the following way:\n\n\"\"\"\n{\n    \"answer\": [\"Y\"/\"N\"],\n    \"relationship\": [\"Positive\"/\"Negative\"]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-014", "type": "Data Insight", "instruction": "You are a data analyst at a music streaming company. Your task is to determine whether the statement \"as the energy of a track increases, the amount of acoustic instruments increase\" is accurate. Additionally, analyze the correlation between the energy and acousticness of a track. Respond with \"Y\" if the statement is true, otherwise respond with \"N\". For the correlation, respond with \"Positive\" if it is positive, otherwise respond with \"Negative\". Format your response as follows:\n\n\"\"\"\n{\n    \"answer\": [\"Y\"/\"N\"],\n    \"relationship\": [\"Positive\"/\"Negative\"]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-015", "type": "Data Insight", "instruction": "You are working on a project for a market analysis firm. Your task is to identify the seasonal trends in the Superstore sales data. Based on your findings, select the most fitting description for each period from the following: \"Generally stable trend\", \"Sharp downward trend\", \"Sales increase\", \"Sales drop\", \"Sales rise\". Format your response as follows:\n\n\"\"\"\n{\n\"November-December\": [...],\n\"January\": [...],\n\"February-March\": [...],\n\"April-August\": [...],\n\"October\": \"Sharp [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-016", "type": "Data Insight", "instruction": "As a data scientist for a fashion retailer, you need to examine the sales data to identify the top five states with the highest clothing spending. Your response should be formatted like this:\n\"\"\"\n{\n    \"Top five states\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-017", "type": "Data Insight", "instruction": "Analyze the data to determine the popularity of different categories among various age groups. Based on your findings, select the most fitting description for each category from the following: ‘Most famous in 45-55 age group’, ‘Most popular among all age groups’, ‘Equally famous in all age groups except 15-25 and 65-75’, or ‘Almost equally famous in all age groups’. Please directly provide the answers below in the following format:\n\"\"\"\n{\n    \"Clothing\": [...],\n    \"Accessories\": [...],\n    \"Footwear\": [...],\n    \"Outerwear\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-018", "type": "Data Insight", "instruction": "You are a data analyst for a retail chain. Your task is to analyze the data to determine the order of store count, size, and weekly sales for stores A, B, and C from highest to lowest. Format your response as follows:\n\"\"\"\n {\n\"Store count\": [...],\n\"Size\": [...],\n\"Weekly sales\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-019", "type": "Data Insight", "instruction": "You are working on a project for a sports analytics firm. Your task is to determine the number of wins and losses Chennai Super Kings have had in Mumbai Stadium by analyzing the provided data. Format your response as follows:\n\"\"\"\n{\n    \"Wins\": [...],\n    \"Losses\": [...]\n}\n\"\"\"\n\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-020", "type": "Data Insight", "instruction": "You are working on a project for a sports analytics firm. Your task is to determine the average number of matches won and lost by Chennai Super Kings in Chennai by analyzing the provided data. Format your response as follows:\n{\n\"Average matches won\": [...],\n\"Average matches lost\": [...]\n}", "hardness": "Medium", "post_process": []}
{"id": "di-text-021", "type": "Data Insight", "instruction": "As a data scientist for a restaurant chain, examine the data to identify the city with the highest concentration of restaurants. Please directly provide the answers below in the following format:\n\n\"\"\"\n{\n\"City\": [...],\n\"State\": [...],\n\"Number of Restaurants\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "di-text-022", "type": "Data Insight", "instruction": "As a data scientist for a financial services company, you need to examine the salary data to determine the patterns of change from 2020 to 2021, 2021 to 2022, and 2022 to 2023. Select the appropriate description from these options: \"Similar pattern of progress\", \"Not much change\", \"Continual changes\". Your response should be formatted like this:\n\"\"\"\n{\n    \"2020-2021\": [...],\n    \"2021-2022\": [...],\n    \"2022-2023\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-023", "type": "Data Insight", "instruction": "You are a data analyst for a human resources consulting firm. Your task is to analyze the data to determine the average salaries for Experienced professionals, Seniors, Mid-Level employees, and Entry-Level positions, rounded to the nearest integer. Format your response as follows:\n\"\"\"\n{\n    \"Experienced professionals\": [...],\n    \"Seniors\": [...],\n    \"Mid-Level employees\": [...],\n    \"Entry-Level positions\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-024", "type": "Data Insight", "instruction": "You are working on a project for an economic research institute. Your task is to analyze the data to determine the average salaries for Full-Time employees, Contractors, Freelancers, and Part-Time workers, rounded to the nearest integer. Format your response as follows:\n\"\"\"\n{\n    \"Full-Time employees\": [...],\n    \"Contractors\": [...],\n    \"Freelancers\": [...],\n    \"Part-Time workers\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-025", "type": "Data Insight", "instruction": "As a data scientist for a global consultancy, examine the salary data to find the average data science salaries in Illinois (IL), Puerto Rico (PR), the United States (US), Russia (RU), and Canada (CA). Round the results to the nearest integer. Please directly provide the answers below in the following format:\n\n\"\"\"\n{\n    \"Illinois (IL)\": [...],\n    \"Puerto Rico (PR)\": [...],\n    \"United States (US)\": [...],\n    \"Russia (RU)\": [...],\n    \"Canada (CA)\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-026", "type": "Data Insight", "instruction": "As a data scientist for a video game analytics company, examine the data to find the top 3 years with the highest number of game releases, and also provide the number of games released each year from 2008 to 2010. Please directly provide the answers below in the following format:\n\n\n\"\"\"\n{\n    \"Top 3 year had the most game release\": [...],\n    \"2008-2010\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "di-text-027", "type": "Data Insight", "instruction": "As part of an epidemiology research project, identify the counties with the highest and lowest average annual diagnosed cases. Provide the county names in the format ‘Bullock County, Alabama’. Please directly provide the answers below in the following format:\n\n\"\"\"\n{\n    \"Fewest average annual cases diagnosed County\": [...],\n    \"Most average annual cases diagnosed County\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-028", "type": "Data Insight", "instruction": "You are working on a project for a national cancer research center. Your task is to analyze the provided data to determine the rough estimate of the average number of cancer cases cured/recovered annually and the chance of recovery from all-cause cancer in any given year in the US. Format your response as follows:\n\n\"\"\"\n{\n    \"Average cases cured/recovered in a year\": [...],\n    \"Chance of recovery from all-cause cancer in any given year\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-029", "type": "Data Insight", "instruction": "You are tasked with analyzing economic data for a government advisory body. Your objective is to identify strong positive and negative correlations (absolute value > 0.5) between economic indicators. Replace missing values with np.nan. Report the correlated variable pairs in the format [\"Indicator 1\", \"Indicator 2\"]. Format your response as follows:\n\n{\n    \"Strong Positive Correlation (>=0.5)\": [...],\n    \"Strong Negative Correlation (<=-0.5)\": [...]\n}\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-030", "type": "Data Insight", "instruction": "Examine the dataset to find out how many female athletes competed in the 1900 Summer Olympics. Report the count in the following format:\n\"\"\"\n{\n    \"record\": [number_of_female_athletes]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "di-text-031", "type": "Data Insight", "instruction": "Examine the data to identify the trend in the count of paid apps with increasing price, answering with \"increase\" or \"decrease.\" Choose an appropriate upper price limit for apps from 10, 30, and 60 dollars that reflects the majority of app prices and a reasonable maximum. Present your findings in this format:\n\n\"\"\"\n{\n    \"correlation\": [...],\n    \"upper bound($)\": [selected_price]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-032", "type": "Data Insight", "instruction": "Analyze the data to select the most appropriate upper price limit for paid gaming apps from 10, 25, and 40 dollars. Additionally, compare the price ranges of various app categories to determine whether the price range for paid entertainment apps is ‘lower’ or ‘higher.’ Please directly provide the answers below as text in the following format:\n\n“”“\n{\n    \"upper bound($)\": [...],\n    \"Paid entertainment apps' price range\": [...]\n}\n”“”\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-033", "type": "Data Insight", "instruction": "Analyze the data to determine whether apps get pricier as their size increases. Respond with \"Yes\" or \"No.\" Format your response as follows:\n\n\"\"\"\n{\n    \"answer\": [\"Yes\"/\"No\"]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-034", "type": "Data Insight", "instruction": "Analyze the provided data to identify the top 2 main genres for both PS3 and XB360 platforms. Format your response as follows:\n\n\"\"\"\n{\n    \"Top 2 main genre game\": [genre1, genre2]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-035", "type": "Data Insight", "instruction": "Analyze the dataset to identify variables that are strongly correlated with the \"satisfaction\" variable (absolute correlation > 0.4). Report the variables with strong positive and negative correlations in the following format:\n\n\"\"\"\n{\n    \"Strong Positively Correlated\": [...],\n    \"Strong Negatively Correlated\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-036", "type": "Data Insight", "instruction": "Examine the data to identify the most frequent starting point and stop destination for drivers. Present your findings in the specified format:\n\n\"\"\"\n{\n    \"most popular starting point\": [...],\n    \"most popular stop destination\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-037", "type": "Data Insight", "instruction": "For the given dataset, analyze which distance category—[\"short distanced\", \"middle distanced\", \"long distanced\"]—is most frequently associated with Uber trips. Report your results in the following format:\n\n\"\"\"\n{\n    \"most number of distanced trip\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-038", "type": "Data Insight", "instruction": "Analyze the distribution on goal scoring trends for Defenders, Forwards, and Midfielders in 2021. Identify the correct insights from the options below:\n\n- Higher density in the 0 to 20 mark, indicating many capable of scoring a goal or two.\n- Likeliest to not score a goal throughout the year.\n- Most likely to score a lot of goals.\n\nFormat your response as:\n\n{\n    \"Defenders\": [...],\n    \"Forwards\": [...],\n    \"Midfielders\": [...]\n}", "hardness": "Medium", "post_process": []}
{"id": "di-text-039", "type": "Data Insight", "instruction": "Determine the Top 4 programming languages most commonly asked for in Google Jobs. Count the number of times each appears in the dataset. Format your response like this:\n\n\"\"\"\n{\n    \"Top 4 Popular Program Language\": [...],\n    \"Count\": [...]\n}\n\"\"\"\n", "hardness": "Hard", "post_process": []}
{"id": "di-text-040", "type": "Data Insight", "instruction": "Identify the Top 3 years of experience most commonly required for Google Jobs. Count how often each appears in the dataset. Format your response like this:\n\n\"\"\"\n{\n    \"Top 3 Years of experiences are needed\": [...],\n    \"Count\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "di-text-041", "type": "Data Insight", "instruction": "Determine if there are outliers in the student study duration and scores data, responding with \"Yes\" or \"No\". Also, identify if there is a strong linear relationship between study duration and scores, responding with \"Yes\" or \"No\". Present your answer in this format:\n\n\"\"\"\n{\n    \"outlier\": [...],\n    \"linear relation\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-text-042", "type": "Data Insight", "instruction": "Examine the data from 2020 to 2022 to find the specific month and year when the cost was at its highest. Additionally, determine when the cost started rising in 2021 and reached its first peak. Use the format \"Jun 2021\" for your answers. Present your findings in this format:\n\n\"\"\"\n{\n    \"Lowest price\": [...],\n    \"Peak\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "di-csv-001", "type": "Data Insight", "instruction": "“Retrieve the year-to-date performance of the Magnificent 7 stocks by calculating the percentage change in their prices from the beginning of the current year to the most recent available data. Use the provided template for result.csv and ensure that the results are filled in according to the specified column names", "hardness": "Hard", "post_process": []}
{"id": "di-csv-002", "type": "Data Insight", "instruction": "Review the book data and identify the protagonist for each book. Input this information into result.csv using the provided template, ensuring the results follow the specified format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-003", "type": "Data Insight", "instruction": "Pull the most recent stock performance data for Amazon, Facebook, and the S&P 500. Calculate the average daily return differences between Amazon and the S&P 500, as well as between Facebook and the S&P 500. Fill the results into result.csv following the provided template.”", "hardness": "Medium", "post_process": []}
{"id": "di-csv-004", "type": "Data Insight", "instruction": "Retrieve the latest Fortune 500 list and rank the companies. Highlight those that operate under large conglomerates and fill the ranked list in result.csv, following the provided template.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-005", "type": "Data Insight", "instruction": "Analyze which sectors have had the highest average company valuations over the past three years. Identify any standout new companies within those sectors and fill the results into result.csv, ensuring the data follows the provided template.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-006", "type": "Data Insight", "instruction": "Analyze the impact of handwashing, which began on 1847-06-01, on the monthly proportion of deaths. Calculate the average reduction in monthly deaths and fill the results into result.csv according to the provided template.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-007", "type": "Data Insight", "instruction": "Identify the match with the largest score difference in the Spanish league. Save the details of that match in max_score_difference.csv, following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-008", "type": "Data Insight", "instruction": "Which country's league performs the best in terms of average home team scores? Fill the country and its average home team scores to 'result.csv'.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-009", "type": "Data Insight", "instruction": "Calculate the distribution of players across different height ranges. Identify the specific height with the highest number of players and its corresponding count. Fill this information in result.csv, following the provided format.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-010", "type": "Data Insight", "instruction": "Determine the number of individual wrestlers and tag teams in the Wrestlers table. Also, check if there are any three-man teams in the table. Fill the results into result.csv using the provided format", "hardness": "Medium", "post_process": []}
{"id": "di-csv-011", "type": "Data Insight", "instruction": "Count the total number of title belts, excluding any titles that contain the phrase ‘title change.’ Do not count the Money in the Bank Briefcase, King of the Ring, spots in the Royal Rumble, interim championships, or the Dusty Rhodes Tag Team Classic Cup as titles. Fill the results into result.csv following the provided template.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-012", "type": "Data Insight", "instruction": "Count the number of title belts for each of the following promotions: WWE, WWF, WCW, NXT, and ECW. Save the results in result.csv using the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-013", "type": "Data Insight", "instruction": "Count the number of title belts for each promotion and gender combination, including ECW (Male), NXT (Female and Male), WCW (Female and Male), WWE (Female and Male), and WWF (Female and Male). Fill the results into result.csv following the provided template.", "hardness": "Hard", "post_process": []}
{"id": "di-csv-014", "type": "Data Insight", "instruction": "Count the number of title belts for tag teams in each promotion: NXT, WCW, WWE, and WWF. Save the results in result.csv using the provided format", "hardness": "Hard", "post_process": []}
{"id": "di-csv-015", "type": "Data Insight", "instruction": "Identify the location that held the most wrestling events for each promotion. Save the results in result.csv using the provided format.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-016", "type": "Data Insight", "instruction": "Identify the wrestlers involved in the shortest title match for each title in NXT. Provide details about the card, including title, match duration, match type, win type, location, and event. Save the results in result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-017", "type": "Data Insight", "instruction": "Identify the wrestlers with the highest win rate at Elimination Chamber. Provide the wrestler’s name and number of wins. Save the results in result.csv, following the provided format.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-018", "type": "Data Insight", "instruction": "Calculate the number of motorcycle riders killed in motorcycle accidents, grouped by age. Save the results in result.csv following the provided template.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-019", "type": "Data Insight", "instruction": "Explore the safety of motorcycle riders wearing helmets by analyzing the data. Include case ID, rider age, and helmet usage statistics in the query results. Save the results in result.csv following the provided template", "hardness": "Medium", "post_process": []}
{"id": "di-csv-020", "type": "Data Insight", "instruction": "Calculate the age distribution and determine if the age distribution of movies and TV shows in the catalog is similar. Answer with \"Yes\" or \"No\" and save your response in answer.csv with the column name \"answer.\"", "hardness": "Medium", "post_process": []}
{"id": "di-csv-021", "type": "Data Insight", "instruction": "Identify the top 5 movies with the most days on the catalog and provide their details, including title, country, date added, and duration. Fill in the information according to the provided template in top_5_movies.csv.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-022", "type": "Data Insight", "instruction": "Identify the top 5 most recently added TV shows on the catalog and provide their details, including title, country, date added, and duration. Fill in the information according to the provided template in top_5_movies.csv.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-023", "type": "Data Insight", "instruction": "Examine the dataset to determine the top 5 genres among the best 100 K-Dramas. Fill the results into result.csv, following the provided template.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-024", "type": "Data Insight", "instruction": "Please evaluate the Students' Academic Performance Dataset to identify whether the variables in `sample_relation.csv` have a positive or negative effect on student success. Mark 1 under the respective column for a positive impact and 0 for a negative impact, following the format in `sample_relation.csv`. Fill your results into `Relation.csv`.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-025", "type": "Data Insight", "instruction": "Calculate the total balance for each customer associated with accounts, where the customer resides in Los Angeles. Save the results in result.csv, following the provided template.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-026", "type": "Data Insight", "instruction": "Identify the branch with the highest average account balance and record its details, including BRANCHID, BRANCHNAME, CITY, STATE, and AVG_ACCOUNT_BALANCE. Fill the results into result.csv using the provided format.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-027", "type": "Data Insight", "instruction": "Identify the customer with the most transactions in the Transactions table and record their customerID. Save the result in result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-028", "type": "Data Insight", "instruction": "Identify the branch with the highest total balance across all of its accounts. Record its BranchID, BranchName, City, State, and Total_Balance. Save the results in result.csv following the provided format", "hardness": "Easy", "post_process": []}
{"id": "di-csv-029", "type": "Data Insight", "instruction": "Which customer has the highest total balance across all of their accounts, including savings and checking accounts? Save the results to 'result.csv'.", "hardness": "Easy", "post_process": []}
{"id": "di-csv-030", "type": "Data Insight", "instruction": "Identify the customer with the highest total balance across all of their accounts, including savings and checking accounts. Record their BranchID, BranchName, and City. Save the results in result.csv following the provided format", "hardness": "Medium", "post_process": []}
{"id": "di-csv-031", "type": "Data Insight", "instruction": "Find the orders placed in 1998 with a total amount exceeding 10,000. Record the customerID, companyName, orderID, and total_amount. Sort the results in descending order by total_amount and save them to result.csv following the provided format", "hardness": "Medium", "post_process": []}
{"id": "di-csv-032", "type": "Data Insight", "instruction": "Identify the customers who placed orders in 1998 with a total amount exceeding $15,000. Record their customerID, companyName, and total_amount. Sort the results in descending order based on total_amount and save them to a file named result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-033", "type": "Data Insight", "instruction": "Identify the companies with a total order amount exceeding $10,000 in 1998. Record their customerID, companyName, and total_amount, and save the results to result.csv following the provided format", "hardness": "Medium", "post_process": []}
{"id": "di-csv-034", "type": "Data Insight", "instruction": "Filter the orders where the quantity is greater than or equal to 60 and has multiple repeated quantities within the same order. Record their orderID, productID, unitPrice, quantity, and discount. Output the results to result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-035", "type": "Data Insight", "instruction": "Calculate the total number of orders and the number of late orders for each employee. Record their employeeID, lastName, total orders, and late_orders, and save the results to result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-036", "type": "Data Insight", "instruction": "Retrieve the total sales for each customer in the year 1998 and categorize them into different groups. Record their Customer ID, Company Name, Total, and Group, and save the results to result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "di-csv-037", "type": "Data Insight", "instruction": "Filter the data to include only supplier country and customer country information. Record the supplier_country and customer_country, and save the results to result.csv following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-001", "type": "Data Manipulation", "instruction": "Analyze the UFC Fighters’ Statistics 2024 dataset to identify the top fighters in each weight class and count how many remain undefeated. Please ensure the results are entered into the provided undefeated.csv file, adhering strictly to its format.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-002", "type": "Data Manipulation", "instruction": "Determine how many of the students have been married before and how many have never been married. Please enter the corresponding results in the appropriate sections of the provided result.csv file, ensuring the format is followed precisely.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-003", "type": "Data Manipulation", "instruction": "Using the dataset, identify the top five most frequent educational qualifications for both mothers and fathers. For each qualification, calculate its frequency and sort the results in descending order. Save the results in top_qualifications.csv, ensuring the file includes the following columns: ‘Mother’s Qualification’, ‘Mother’s Frequency’, ‘Father’s Qualification’, and ‘Father’s Frequency’. Ensure qualifications are formatted like ‘10th Year of Schooling’.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-004", "type": "Data Manipulation", "instruction": "Analyze the dataset to determine the total number of Games held for both the Summer and Winter Olympics. Record the results in allGames.csv, adhering to the template provided in sample_allGames.csv. Additionally, for each edition of both the Summer and Winter Olympics, identify the host city and the total number of events conducted. Compile this information in allEvents.csv, ensuring it matches the structure of the provided sample_allEvents.csv template", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-005", "type": "Data Manipulation", "instruction": "Using the instructions provided in age.txt, count the number of people in each age group. If any values in the ‘Age’ column are missing, fill them with 30. Record the results in result.csv, following the structure and format outlined in the sample_result.csv template.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-006", "type": "Data Manipulation", "instruction": "Identify which products are frequently purchased together in the same order. Specifically, find the different product pairs and calculate the number of times they are purchased together. Record the information for the top two product pairs with the highest co-purchase count in result.csv, following the format in sample_result.csv. The column names should be ‘product_a’ for the first product in the pair, ‘product_b’ for the second product, and ‘co_purchase_count’ for the number of times the two products are purchased together. Ensure product names follow the format ‘Heller Shagamaw Frame - 2016’.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-007", "type": "Data Manipulation", "instruction": "Calculate the top 10 most popular movies by considering only those whose number of votes falls within the top 15%. Use the formula provided in wrFormula.tex to perform the calculation. Once the calculations are complete, write the names of the top 10 movies into result.csv, following the format specified in sample_result.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-008", "type": "Data Manipulation", "instruction": "Calculate the correlation coefficient between leverage_ratio and profitability_ratio. Based on the value of the correlation coefficient, determine whether the relationship is positive, negative, or shows no correlation. Write the results into correlation_results.csv, following the format provided in sample_correlation_results.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-009", "type": "Data Manipulation", "instruction": "Identify the top 10 best-rated authors, the most expensive authors, and the authors with the most ratings. Save the results in author.csv, following the format provided in sample_result.csv.”", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-010", "type": "Data Manipulation", "instruction": "Calculate the monthly total sales volume for each bike category. Then compute the average monthly sales volume for each category across all months. Ensure that the average is calculated as the mean of monthly totals, and save the results in avg_units_sold.csv, matching the format of sample_result.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-011", "type": "Data Manipulation", "instruction": "Calculate the total quantity sold and total sales revenue for each bike category from 2016 to 2018. Ensure that total sales revenue accounts for any discounts applied to each order. Write the results into result.csv, following the exact structure and formatting of sample_result.csv.\n", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-012", "type": "Data Manipulation", "instruction": "Identify the top five users based on their total expenditure and provide their full names (first name and last name) along with their corresponding total expenditure. Ensure that the results are sorted in descending order by total expenditure and written into result.csv, following the structure and format of sample_result.csv.\n", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-013", "type": "Data Manipulation", "instruction": "From the top 20 clubs with the highest average overall scores in FIFA 20, identify the club with the best average physicality based on the BMI calculation from the players’ heights and weights. Using the chosen club, find the player with the highest shooting score based on data from BMI.txt. Record both the name of the club and the full name of this player in result.csv, ensuring the result matches the format of sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-014", "type": "Data Manipulation", "instruction": "Calculate the Wilson score for each review using the formula provided in Wilson.tex. Then, extract the summaries of the top 3 reviews based on the highest Wilson scores. Write the results into result.csv, ensuring that the format matches the template provided in sample_result.csv", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-015", "type": "Data Manipulation", "instruction": "Identify the top player in terms of games played, runs scored, hits, and home runs from the baseball dataset. Ensure that you save the player’s name and the respective category (e.g., “Most Games Played”) in result.csv, following the format provided in sample_result.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-016", "type": "Data Manipulation", "instruction": "Compile season statistics for both Italian Serie A (IT1) and the Bundesliga (BESC), focusing on identifying the top-performing clubs in terms of wins for each season. Ensure that the results include the competition ID, season, and top-performing club for each competition, and write the results into a CSV file named result.csv, following the format provided in sample_result.csv.\n", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-017", "type": "Data Manipulation", "instruction": "Follow the instructions in data_standard.md to calculate the Net Promoter Score (NPS) for the dataset. After calculating the Net_Promoter_Score, save the results in result.csv, following the format and structure provided in the file template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-018", "type": "Data Manipulation", "instruction": "Please review the Data Science Programs list dataset to determine the top 10 universities with the most affordable data science programs. Ensure that tuition fees are normalized to a per-year basis. Record the university names and their corresponding normalized tuition fees, sorted from lowest to highest, in result.csv. Ensure the output is formatted according to the structure specified in sample_result.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-019", "type": "Data Manipulation", "instruction": "Analyze the dataset to identify the top 20 teams based on their average score. For each of these teams, record the number of draws they have had. Arrange the results in result.csv, sorting by the number of draws from highest to lowest. Ensure that the format follows the structure outlined in sample_result.csv", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-020", "type": "Data Manipulation", "instruction": "Please review the Telco customer churn dataset to determine the top 10 cities with the highest churn rates. Ensure that the churn rate is calculated as the proportion of customers who churned in each city. Save the findings in result.csv, ensuring the output is formatted according to the template in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-021", "type": "Data Manipulation", "instruction": "You need to identify the top 3 most popular product categories among families with 0, 1, and 2 children. Use the total amount spent on each product category to determine popularity. Write the results into result.csv, following the format specified in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-022", "type": "Data Manipulation", "instruction": "Assuming the current year is 2024, calculate the Mean Campaign Acceptance Rate by Age for each country, based on the age groups specified in result.csv. Use the total number of accepted campaigns to compute the acceptance rate. Fill in the results in result.csv, ensuring the format is consistent with the provided file.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-023", "type": "Data Manipulation", "instruction": "Calculate the average number of campaigns it takes for customers with each education level to accept their first offer. Record the results in average_campaign_accepted_by_education.csv, following the format specified in sample_result.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-024", "type": "Data Manipulation", "instruction": "Using the guidelines provided in tips.txt, calculate the total number of online purchases (NumWebPurchases) and in-store purchases (NumStorePurchases) for different income groups. The income groups should be categorized into the intervals specified in tips.txt. Once the calculations are completed, fill in the results in purchase_by_income.csv, following the exact format of the provided template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-025", "type": "Data Manipulation", "instruction": "Enhance the existing code in analys.py to calculate the winning and losing probabilities for each team, as detailed in tips.txt. Determine which team has the highest winning probability and which team has the highest losing probability. Once identified, record their names along with their corresponding probabilities in team_probabilities.csv, ensuring the results follow the format provided in sample_result.csv.\n", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-026", "type": "Data Manipulation", "instruction": "Complete the analysis.py script to rank FIFA 22 players across various positions based on a specified score, covering the years 2015 to 2022. The score calculation for each position is outlined in the provided text and Markdown files. Ensure that both male and female player data are included in the analysis, and output the top 20 ranked players per position. Write the final results to result.csv, following the format of sample_result.csv.\n", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-027", "type": "Data Manipulation", "instruction": " Identify the top 10 countries by the number of produced shows and their corresponding counts, as well as the top 10 movie genres by the number of occurrences and their corresponding counts. Record the results in the provided templates: Top_10_countries.csv and Top_10_Movies.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-028", "type": "Data Manipulation", "instruction": "Calculate the total profit for each state by subtracting the Cost from the Revenue. Then, sort the states by their total profits in descending order. Finally, save the results in state_profit.csv, ensuring the format matches the provided template.\n", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-029", "type": "Data Manipulation", "instruction": "Identify the top 3 users with the most pull requests and record their usernames and the number of pull requests in the top3_pull_requests.csv file using the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-030", "type": "Data Manipulation", "instruction": "Identify the latest 10 pull requests, merge the related file data, and find the unique files involved in these pull requests. Record the results in the unique_files.csv file using the sample_result.csv template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-031", "type": "Data Manipulation", "instruction": " Identify the users who made the last 6 pull requests for the specific file \"src/compiler/scala/reflect/reify/phases/Calculate.scala\". Record the results in the users_last_6.csv file using the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-032", "type": "Data Manipulation", "instruction": "Calculate the proportion of 1-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_by_version.csv file using the provided template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-033", "type": "Data Manipulation", "instruction": "Compare the 1-day retention rates between two versions of a game (gate_30 and gate_40) according to the instructions in guidance.txt. Calculate the probability (expressed as a decimal) that the retention rate is higher for gate_30 and record the result in the probability_greater_retention.csv file using the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-034", "type": "Data Manipulation", "instruction": "Calculate the proportion of 7-day retention being True for the gate_30 and gate_40 versions, respectively. Record the results in the retention_7_by_version.csv file using the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-035", "type": "Data Manipulation", "instruction": "Convert all time columns to Coordinated Universal Time (UTC) and record each user’s pulled files based on their pid. If a user pulled multiple files within one pid, you need to record these files separately. Write the results in the output.csv file using the format provided in sample_output.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-036", "type": "Data Manipulation", "instruction": "Identify the last six users who made pull requests for `src/compiler/scala/reflect/reify/phases/Calculate.scala`. and fill in their usernames in the template file users_last_6.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-037", "type": "Data Manipulation", "instruction": "Identify the top three users who made the most pull requests for src/compiler/scala/reflect/reify/phases/Calculate.scala. Fill in their usernames in the provided template file top_3_developers.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-038", "type": "Data Manipulation", "instruction": "I want to know the number of pull requests made by two users with the nicknames “soc” and “xeno-by” for each year between 2011 and 2016. Please fill in your statistical results in the provided template file pull_requests_by_year_and_author.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-039", "type": "Data Manipulation", "instruction": "conduct a supply chain analysis of three ingredients used in an avocado toast according to the instructions in tips.md, utilizing the Open Food Facts database. Identify the list of ingredients and their countries of origin, and record the results in the ingredient_origins.csv file using the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-040", "type": "Data Manipulation", "instruction": "Categorize the salary scale into four levels according to the salaries of all employees. For the company size with the most top-level employees, determine the designations and salaries of the top 10 paid employees. Save the results in result.csv, following the format in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-041", "type": "Data Manipulation", "instruction": "Count the stop outcomes for male and female drivers when they are pulled over for speeding, and express these outcomes as proportions. Save the results in the stop_outcomes_by_gender.csv file, ensuring that the format matches the provided template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-042", "type": "Data Manipulation", "instruction": "Calculate the total number of days between the InvoiceDate and the CohortDate (the date of a customer’s first purchase) for each entry in the dataset. Save the calculated difference as CohortIndex in the result.csv file, ensuring the output matches the provided format", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-043", "type": "Data Manipulation", "instruction": "Calculate the retention rate for each cohort by comparing the number of unique customers who made a purchase in subsequent months to the initial cohort size. Save the calculated retention data to retension.csv, ensuring the format matches the provided template.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-044", "type": "Data Manipulation", "instruction": "Calculate the average unit price paid for items by customer groups, where each group is defined by the month of their first purchase. Track how the average unit price changes over time for each cohort. Save the results in average_price.csv, ensuring the format matches the provided template file.\n", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-045", "type": "Data Manipulation", "instruction": "According to the information in tips.txt, you need to use the RFM method to calculate values for each CustomerID in the datasmart.csv for the recent 12 months. Save the results in datasmart.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-046", "type": "Data Manipulation", "instruction": "Calculate the annualized volatility and annualized variance for Microsoft trading data, using daily returns based on the ‘Close’ prices. Assume 252 trading days in a year for the annualization calculations. Save the calculated results in result.csv, ensuring the format matches the template provided in sample_result.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-047", "type": "Data Manipulation", "instruction": "Calculate the historical beta of FamaFrenchData using the Capital Asset Pricing Model (CAPM). Write the calculated results into result.csv, with the column named “result”.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-048", "type": "Data Manipulation", "instruction": "Your task is to calculate the drawdown of USO, an ETF that tracks oil prices. Fill in the results into result.csv, as provided in the corresponding file.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-049", "type": "Data Manipulation", "instruction": "Use the mean to fill in any missing values in the dataset. Then, calculate the correlation between Population Density and Life Expectancy, as well as the correlation between Population Density and GDP. Please output the answers below in JSON format by filling in the placeholders in:\n\n\"\"\"\n{\n  \"Correlation between Population Density and Life Expectancy\": [...],\n  \"Correlation between Population Density and GDP\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-050", "type": "Data Manipulation", "instruction": "Using the 2017 stock return datasets for the 9 biggest companies, calculate the cumulative returns for three different portfolio strategies: the default portfolio, the equal-weight portfolio, and the market value-weighted portfolio. Save the results in result.csv, ensuring the output follows the required format.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-051", "type": "Data Manipulation", "instruction": "Calculate the portfolio volatility of the default portfolio and express the result as a percentage. Write the result into output.csv, with the column labeled as result", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-052", "type": "Data Manipulation", "instruction": "Calculate the RFM (Recency, Frequency, Monetary) score for each customer, build segmentation based on these scores, and assign a level to each customer. Save the results, including the customer segmentation and levels, in result.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-053", "type": "Data Manipulation", "instruction": "Using a 1-3 scale, calculate the RFM score for each CustomerID in RFM_Score.csv. Fill in the corresponding RFM scores for each customer, ensuring the results are saved in RFM_Score.csv as per the provided file format.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-054", "type": "Data Manipulation", "instruction": "Calculate the Sharpe Ratio for each asset and identify both the minimum and maximum Sharpe ratios. Save the results in result.csv, ensuring the output follows the format specified in sample_result.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-055", "type": "Data Manipulation", "instruction": "Calculate the cumulative returns of the portfolio (Portfolio) and the excess returns over the risk-free rate (Portfolio_Excess) based on FamaFrenchData over time. After completing the calculations, fill in the results in result.csv, ensuring the provided Date column is maintained.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-056", "type": "Data Manipulation", "instruction": "Using the Portfolio Risk Management dataset, select the equal-weighted portfolio and calculate its historical beta with the Fama-French data. Save the results in result.csv, ensuring the output follows the format provided in sample_result.csv.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-057", "type": "Data Manipulation", "instruction": "Calculate a set of summary statistics on the purchase data, broken down by ‘device’ (Android or iOS) and ‘gender’ (Male or Female). These summary statistics should include the total purchases, average purchase amount, and purchase count for each group. Fill the corresponding blanks in purchase_summary.csv with these summary statistics, ensuring the format matches the provided template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-058", "type": "Data Manipulation", "instruction": "Calculate and print the confidence interval for the difference in conversion rates (lift) between a test group and a control group. After performing the calculation, write the results into result.csv, ensuring the format matches the provided sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-059", "type": "Data Manipulation", "instruction": "Analyze the purchasing behavior of users during the first week after their registration date. Calculate the average number of purchases made each day within the first week, and fill the results into result.csv.", "hardness": "Hard", "post_process": []}
{"id": "dm-csv-060", "type": "Data Manipulation", "instruction": "Calculate the average daily purchases and paywall views for the entire population. After calculating the results, write them into result.csv, ensuring the format matches the structure provided in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-061", "type": "Data Manipulation", "instruction": "Calculate the dates of the earliest and most recent reviews, the total number of private rooms, and the average listing price. Fill in the results into result.csv, ensuring the format matches the template provided in result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-062", "type": "Data Manipulation", "instruction": "Analyze the handwashing history dataset to calculate the average monthly reduction in the proportion of deaths after handwashing practices began on June 1, 1847. Record your findings in result.csv, ensuring the format matches the required template.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-063", "type": "Data Manipulation", "instruction": "Identify the oldest business on each continent and fill in the relevant information in result.csv, ensuring the format follows the specifications provided in result.csv.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-064", "type": "Data Manipulation", "instruction": "Identify the restaurants in the dataset that have been operating since before the year 1800. Fill in their relevant information into result.csv, ensuring the format matches the specifications provided in result.csv", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-065", "type": "Data Manipulation", "instruction": "Retrieve the necessary data to calculate the regression line for each category, with ‘days’ as the independent variable. Save the results to daily_sales_per_category.csv, ensuring the data is properly formatted.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-066", "type": "Data Manipulation", "instruction": "For each of the top 10 cities with the most orders, retrieve the necessary data for each timestamp of the order stages. Save the results to order_stage_times_top_10_cities.csv, ensuring the data is properly formatted.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-067", "type": "Data Manipulation", "instruction": "Calculate the average delivery time for orders placed each day between June 1, 2017, and June 30, 2018. Save the results in a file named daily_avg_shipping_time.csv, ensuring the data is saved in chronological order.", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-068", "type": "Data Manipulation", "instruction": "Calculate the average occupancy per aircraft and save the result to occupancy_rate.csv", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-069", "type": "Data Manipulation", "instruction": "Identify the top 5 airports that attract the most business class passengers. Save the results in top_airports_business.csv, ensuring the data is properly formatted.", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-070", "type": "Data Manipulation", "instruction": "Calculate the number of individual wrestlers, tag teams, triple teams, and teams with more than 3 members from the Wrestlers table. Save the results in a file named wrestlers_team_count.csv, with the columns named: \"No_of_Individual_Wrestlers\", \"No_of_Tag_Teams\", \"No_of_Triple_Teams\", and \"No_of_Teams_with_More_Than_3_Members\"", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-071", "type": "Data Manipulation", "instruction": "Identify wrestlers in the dataset whose names contain more than 2 ampersands (&). Save the corresponding names in a file named wrestlers_with_more_than_2_ampersands.csv, with the column \"name.”", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-072", "type": "Data Manipulation", "instruction": "Analyze the dataset to determine the total number of title belts. Save the result in a file named number_of_titles.csv, with the column named \"Number_of_Titles\".", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-073", "type": "Data Manipulation", "instruction": "Calculate the number of title belts for each promotion (WE, WWF, WCW, NXT, ECW) in the dataset. Fill the results into promotions_title_counts.csv, ensuring the data is properly formatted", "hardness": "Easy", "post_process": []}
{"id": "dm-csv-074", "type": "Data Manipulation", "instruction": "Analyze the dataset to find the number of title belts for each promotion and gender. Fill the results into  promotions_gender_title_counts.csv, ensuring the data includes both the promotions and gender columns with the format provided", "hardness": "Medium", "post_process": []}
{"id": "dm-csv-075", "type": "Data Manipulation", "instruction": "Using the formula from tips.md, calculate Algeria's World Population Percentage for 2030, 2040, and 2050, starting from the year 2022. Please output the answers below in JSON format by filling in the placeholders in:\n\n\"\"\"\n{\n  \"Algeria 2030 World Population Percentage\": [...],\n  \"Algeria 2040 World Population Percentage\": [...],\n  \"Algeria 2050 World Population Percentage\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-001", "type": "Data Wrangling", "instruction": "Clean the dataset by deleting records with null values or a total_gross of 0. Preserve the columns genre, release_year, total_gross, and inflation_adjusted_gross. Organize the data by grouping the same genres together and sorting them by release_year in ascending order. Save the cleaned dataset as \"disney_movies_total_gross_cleaned.csv\".", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-002", "type": "Data Wrangling", "instruction": "Compute the is_arrested values for various ratings and store the results strictly according to the provided format in \"result.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-003", "type": "Data Wrangling", "instruction": "Clean the data based on the requirements specified in the README file. Save the cleaned dataset to 'cleaned_cars_details_merges.csv'.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-004", "type": "Data Wrangling", "instruction": "Follow the data schema to add new columns. Save the result in 'cleaned_cars_details_merges.csv'", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-005", "type": "Data Wrangling", "instruction": "Standardize the terms and descriptions in the 'brake type' and 'tyre type' fields. Save the result to 'cleaned_cars_details_merges.csv'.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-006", "type": "Data Wrangling", "instruction": "Transform the data according to the provided schema. Save the cleaned data to 'cleaned_ebola.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-007", "type": "Data Wrangling", "instruction": "Ensure consistency by standardizing the terms and descriptions in the 'Gear Box' and 'Drive Type' fields, so that different terminologies referring to the same types of gearboxes and drive systems are unified. Save the final dataset to 'cleaned_cars_details_merges.csv'.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-008", "type": "Data Wrangling", "instruction": "Transform the weather data based on the provided data schema to prepare it for further analysis. Save the cleaned dataset as 'cleaned_weather.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-009", "type": "Data Wrangling", "instruction": "Standardize the terminology and descriptions in the 'Fuel Supply System' field to ensure consistency. Save the updated data to 'cleaned_cars_details_merges.csv'.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-010", "type": "Data Wrangling", "instruction": "Compute the arrest rates for each type of violation across different weather ratings. Save the results to result.csv.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-011", "type": "Data Wrangling", "instruction": "Based on the requirements specified in the README file, generate a new data information table. Save the final table to result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-012", "type": "Data Wrangling", "instruction": "Merge and organize data, then save the source and Net Promoter Score group in the predefined format. Fill the results into 2020Q4.csv following its existing format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-013", "type": "Data Wrangling", "instruction": "Transform the 'Male Risk Development Percentage' and 'Female Risk Development Percentage' columns from percentages to decimal numbers. Calculate the difference in risk development between genders and save it in the 'Male-Female Dev. Diff.' column. Format the data according to 'result.csv' and save it.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-014", "type": "Data Wrangling", "instruction": "Using the data standards from the local document, clean the NYC_Open_Data_Parking_Violations dataset in the database by removing any noisy data. Save the cleaned dataset to 'cleaned_parking_violation.db'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-015", "type": "Data Wrangling", "instruction": "Process the dataset directly in the original file, converting fuel consumption to L/100km and normalizing the car dimensions (length, width, height) to meet local standards. Save the cleaned data to 'cleaned_data.csv'.\n", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-016", "type": "Data Wrangling", "instruction": "Manage missing data in the NYC_Open_Data_Parking_Violations dataset by either dropping or replacing it. Drop rows if necessary, and drop columns only if most entries are empty. Replace missing values with the mean, mode, or other appropriate methods. Save the cleaned data to 'cleaned_data.csv'.\n", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-017", "type": "Data Wrangling", "instruction": "Identify and remove duplicate entries in the NYC_Open_Data_Parking_Violations dataset where residents received multiple tickets for the same violation. Save the cleaned data to 'cleaned_parking_violation.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-018", "type": "Data Wrangling", "instruction": "Follow the instructions in 'tips.md' to clean the dataset using the IQR method. Save the cleaned data according to the format provided in 'cleaned_data.csv'.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-019", "type": "Data Wrangling", "instruction": "For the 'Units Sold (million)' column, identify missing values and fill them using the average from the respective 'Manufacturer' group. After completing the imputation, format the cleaned data to match 'sales_data_imputed.csv' and save it accordingly.\n", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-020", "type": "Data Wrangling", "instruction": "Identify the top five days of the month with the highest number of earthquakes across all data. Save these days (as numbers, e.g., 11 for the 11th) along with their earthquake counts in 'result.csv' according to the specified format.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-021", "type": "Data Wrangling", "instruction": "Follow the data cleanup steps outlined in the provided Python file, which involve processing missing values and normalizing the data. Save the final cleaned and normalized dataset to result.csv.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-022", "type": "Data Wrangling", "instruction": "Transform the airquality DataFrame from a wide to long format. Then, generate a pivot table with 'Month' and 'Day' as indexes, with different air quality metrics as columns and their readings as values. Save the result in 'cleaned_airquality.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-023", "type": "Data Wrangling", "instruction": "Follow the data schema to add or delete some columns, and refine some columns. Save the result in 'cleaned_cars_details_merges.csv'", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-024", "type": "Data Wrangling", "instruction": "Follow the data schema to change the type of columns. Save the result in 'cleaned_cars_details_merges.csv'", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-025", "type": "Data Wrangling", "instruction": "Please refer to the schema document to modify the data. Save the result in 'cleaned_cars_details_merges.csv'", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-026", "type": "Data Wrangling", "instruction": "Identify and extract job-related information such as job titles, technical skills, and educational degrees from the dataset. Save this information in \"result.csv\".", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-027", "type": "Data Wrangling", "instruction": "Follow the requirements in the standard document to retain only the specified columns and eliminate duplicate rows. Save the processed data in 'cleaned_cars_details_merges.csv'.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-028", "type": "Data Wrangling", "instruction": "Remove duplicate entries in the dataset by focusing on the 'Lifter Name' and 'Lift Type' columns, keeping just the first row for each duplicate group. Save the resulting data in the same format as 'cleaned_data.csv'.\n", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-029", "type": "Data Wrangling", "instruction": "Remove the following columns: 'Friend's Tagged', 'Route Name', 'GPX File', 'Activity Id', 'Calories Burned', and 'Notes'. Replace \"Other\" activity type with \"Unicycling\" and perform mean imputation for missing heart rate values within each activity type. Save the cleaned data into 'cleaned_cycle.csv', 'cleaned_run.csv', and 'cleaned_walk.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-030", "type": "Data Wrangling", "instruction": "Clean the data according to the provided schema. Save the cleaned data to 'cleaned_RI-clean.csv'.\n", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-031", "type": "Data Wrangling", "instruction": "Adhere to the data standard to clean the dataset. Fill any missing values by using the most frequent value (mode) for each group of records sharing the same Street Name and Block. Save the final cleaned dataset to \"Building_Permits.csv\".", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-032", "type": "Data Wrangling", "instruction": "Follow the data standards provided in the local document to clean the NYC_Open_Data_Parking_Violations dataset. Correct or remove any noisy data, and save the resulting dataset to 'cleaned_parking_violation.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-033", "type": "Data Wrangling", "instruction": "Filter the database to keep only the information related to airports and aircraft. Ensure all data is in English. Make these modifications directly in the original file and save the changes.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-034", "type": "Data Wrangling", "instruction": "Refer to 'tips.md' for detailed instructions on cleaning the dataset. Once cleaned, ensure the data is saved in the original format as 'laptop_cleaned.csv'.\n", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-035", "type": "Data Wrangling", "instruction": "Merge the provided datasets and save the combined data to RI-clean.csv.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-036", "type": "Data Wrangling", "instruction": "Refer to 'tips.md' for instructions on cleaning the dataset by eliminating outliers with the IQR method. Make sure the data is ready for further analysis, then save it in its original format as 'cleaned_data.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-037", "type": "Data Wrangling", "instruction": "Under the direction of the Head Data Scientist at Training Data Ltd., create a more efficient DataFrame from the customer_train.csv file. Follow the guidelines provided in README.md and save the optimized DataFrame to result.csv.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-038", "type": "Data Wrangling", "instruction": "load the data into the sqlite database 'database.db' as specified in the schema.yml.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-039", "type": "Data Wrangling", "instruction": "Complete data wrangling according to the predefined data schema and generate five new CSV files, named as specified in the schema.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-040", "type": "Data Wrangling", "instruction": "Load the data into the SQLite database 'database.db' as specified in the schema.yml.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-041", "type": "Data Wrangling", "instruction": "Extract the database from the provided archive. Count the number of wrestling championships related to WWE, WWF, WWWF, WCW, NXT, and ECW. Group the results by brand and sort by the number of championships from most to least. Fill in the result according to the format provided in 'result.csv'.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-042", "type": "Data Wrangling", "instruction": "From the `fortune500` dataset, calculate the profits for each sector using the columns `sector` and `pct80`. Save these calculations to profit.csv. Additionally, find the first occurrence date for each tag in the `stackoverflow` dataset using the columns `tag` and `mindate`, and save these dates to startdates.csv.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-043", "type": "Data Wrangling", "instruction": "Unzip the database file to access the data. Calculate the flight counts for each departure airport and sort them by number of flights. Select the top 5 airports with the most flights and the bottom 5 with the fewest. Save these results in 'departure.csv', following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-044", "type": "Data Wrangling", "instruction": "Unzip the provided database file. Read the data from the tables and sort it in ascending order based on the 'range' field. Extract the top 10 rows and save the results according to the format in 'airRange.csv'.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-045", "type": "Data Wrangling", "instruction": "Extract the database from the archive. Query the data to find the wrestlers with the most wins in \"Elimination Chamber\" events. Rank the wrestlers by their number of wins and extract the top three, along with their win counts. Fill in the results according to the format in 'result.csv'.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-046", "type": "Data Wrangling", "instruction": "Unzip the provided database file. Extract the data including flight ID, departure city with its coordinates, and arrival city with its coordinates. Save the results in 'result.csv' according to the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-047", "type": "Data Wrangling", "instruction": "Load these csv into the wwe.db sqlite database according to the schema", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-048", "type": "Data Wrangling", "instruction": "Combine the columns from all CSV files into a single large table and load the data into the SQLite database named 'database.db'.\n", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-049", "type": "Data Wrangling", "instruction": "Organize the newly arrived player data. Check for duplicates and add the unique entries to the sport.db database.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-050", "type": "Data Wrangling", "instruction": "Load these csv into the sport.db sqlite database according to the schema", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-051", "type": "Data Wrangling", "instruction": "Unzip the database to access the wrestling data. Count the championship belts for male and female wrestlers across WWE, WWF, WWWF, WCW, NXT, and ECW brands, excluding entries related to 'title change', Briefcase, King of the Ring, and Royal Rumble. Group the counts by brand and gender, then record the results in 'result.csv', ensuring the format matches the provided structure.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-052", "type": "Data Wrangling", "instruction": "Identify directors who have released at least three movies in consecutive years. Save the names of these directors strictly according to the provided format in 'result.csv'. ", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-053", "type": "Data Wrangling", "instruction": "Apply the DENSE_RANK window function to rank each movie within its director's collection based on 'vote_average'. Rank all movies by their ratings among the films directed by the same person. Save the formatted data to 'result.csv' according to the provided structure.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-054", "type": "Data Wrangling", "instruction": "Extract the database from the archive. For each championship title in NXT, find the shortest title match. List the wrestlers involved, match duration, victory type, location, and event name. Group the results by championship title, ranking by match duration, and fill in the data according to the format in 'result.csv'.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-055", "type": "Data Wrangling", "instruction": "Find the players with the longest and shortest lifespans, calculate their ages, and ensure the results are formatted according to \"result.csv\". Save the results accordingly.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-056", "type": "Data Wrangling", "instruction": "Identify each batsman's number of ducks (zero scores) and count the matches where they scored zero runs. Sort the results by the number of ducks in descending order and save them in \"ducks.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-057", "type": "Data Wrangling", "instruction": "Count the number of movies directed by each director and sort the results in descending order by the number of movies. Save the results in \"movies_all.csv\" according to the provided format.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-058", "type": "Data Wrangling", "instruction": "Unzip and load the database from the provided archive. Filter the data to include matches from Spain, Germany, France, Italy, and England. For each combination of country, league, and season, compute the number of stages, number of teams, average goals scored by home and away teams, goal difference, and total goals. Only include records where the number of stages is greater than 10. Sort the results by country, league, and season, and populate \"leages_by_season.csv\" following the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-059", "type": "Data Wrangling", "instruction": "Identify the top 10 actors whose films have the highest rental counts, sorted in descending order. Fill in their details strictly according to the provided format in the file, \"result.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-060", "type": "Data Wrangling", "instruction": "Identify the total number of movies available in inventory for each category. Ensure the results are entered strictly according to the given format in the file, \"films_in_inv.csv\".", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-061", "type": "Data Wrangling", "instruction": "Determine the percentage of individuals with a family history of mental health issues across all records to explore the connection between family mental health history and overall mental health. Populate the results into \"result.csv\" following the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-062", "type": "Data Wrangling", "instruction": "Extract the list of actors for every movie from the database. Ensure the data is formatted strictly according to the provided format in \"actors.csv\" and load it into this file. ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-063", "type": "Data Wrangling", "instruction": "Find the highest batting averages and strike rates for batsmen who have participated in at least 50 matches. Sort the results by batting average and strike rate in descending order. Save the results in \"highest_avg.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-064", "type": "Data Wrangling", "instruction": "After replacing missing values with 'NaN', compute the average base salary of employees. Filter out those employees whose base salary falls within ±10,000 of the average salary. Populate the results into \"medium_employees.csv\" following the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-065", "type": "Data Wrangling", "instruction": "Compile the rental count and total revenue for each movie category from the database. Calculate the number of rentals and total payment for each category, ensuring the results are sorted by rental count in descending order. Format the data strictly according to the provided format in \"result.csv\" and save the results in this file. ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-066", "type": "Data Wrangling", "instruction": "Compute the overdue rental percentage for each customer. Generate a list that includes the customer’s name, email, and overdue percentage, sorted by overdue percentage in descending order. Populate the data strictly according to the specified format in the file, \"result.csv\".", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-067", "type": "Data Wrangling", "instruction": "Count the number of non-self-employed employees working in tech companies who participated in the survey, broken down by year and state. Save the results in \"result.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-068", "type": "Data Wrangling", "instruction": "Generate a count of films for each category, sorted from highest to lowest. Include the corresponding category ID and number of films, and populate the data into \"result.csv\" as per the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-069", "type": "Data Wrangling", "instruction": "Calculate the percentage of male and female employees who discussed mental health issues between 2017 and 2019, grouped by discussion type (with employer or with colleagues). Save the results in \"result.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-070", "type": "Data Wrangling", "instruction": "Group and adjust player heights (set heights below 165 to 165, and heights above 195 to 195). For each adjusted height group, calculate the number of players, average rating, potential, and weight. Save the results strictly according to the provided format in the file, \"players_height.csv\".", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-071", "type": "Data Wrangling", "instruction": "Find the director who generated the highest revenue and list the information of the movies they directed. Populate the data into \"moviesByHighestRevenue.csv\" following the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-072", "type": "Data Wrangling", "instruction": "Extract the names, birthdates, and total wins of each driver from the Formula 1 database, and order the results by the number of wins from highest to lowest. Populate the data into \"total_wins_by_driver.csv\" following the specified format.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-073", "type": "Data Wrangling", "instruction": "Calculate the rolling averages and standard deviations for height measurements in manufacturing. Establish control limits based on these statistics and flag any measurements that fall outside these limits. Fill the results into the provided file, 'result.csv', using the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-074", "type": "Data Wrangling", "instruction": "I want to know the best fighters of every UFC weight class. Tell me in each class, how many fighters have never been defeated. Please fill in the results into the provided file, \"undefeated.csv\", using the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-075", "type": "Data Wrangling", "instruction": "Identify the most common categories for the oldest businesses on each continent. For categories with more than 5 businesses, provide the continent, category, and count. Ensure to fill your findings into the provided file, \"result.csv\", using the given format. ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-076", "type": "Data Wrangling", "instruction": "Generate a report listing the top five assignments by total donation amounts. For each assignment, include the assignment name, region, total rounded donation amount, and donor type. Fill the results in the provided file, \"result.csv\", using the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-077", "type": "Data Wrangling", "instruction": "Query the SQLite database for movie and director information, sorted by box office revenue in descending order.  Fill the results into 'mostProfit.csv' according to the provided template.\n", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-078", "type": "Data Wrangling", "instruction": "List the names of directors along with the number of movies they've directed and their total revenue. Save the results in \"mostProfitableDirector.csv\" according to the provided format.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-079", "type": "Data Wrangling", "instruction": "Find the film category with the largest total expenditure. Record its name and the total amount spent, ensuring the results are entered into \"result.csv\" according to the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-080", "type": "Data Wrangling", "instruction": "Extract and load the data from the compressed database file. Filter the records to include only those for matches in Spain. Sort these records by date and select the top 10 entries. Save the filtered and sorted data in \"result.csv\" according to the given format.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-081", "type": "Data Wrangling", "instruction": "Retrieve the best performance for each bowler in every IPL match, including the maximum wickets taken and runs conceded. Sort the results by the number of wickets in descending order, and display the bowler's name, nationality, and bowling skill. Save the results strictly according to the provided format in \"highest_wicket.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-082", "type": "Data Wrangling", "instruction": "Retrieve the best performance for each bowler in every IPL match, including the maximum wickets taken and runs conceded. Sort the results by the number of wickets in descending order, and display the bowler's name, nationality, and bowling skill. Save the results strictly according to the provided format in \"result.csv\".", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-083", "type": "Data Wrangling", "instruction": "For each bowler who has bowled at least 50 overs, determine their total runs conceded, total wickets taken, best performance, and number of matches. Sort the data primarily by economy rate in ascending order, and secondarily by the number of wickets taken in descending order. Populate the results into \"best_bowler.csv\" following the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-084", "type": "Data Wrangling", "instruction": "Identify the baseball players who excel in each metric (e.g., Games Played, Runs, Hits, Home Runs). Save the top performer for each metric in \"result.csv\" according to the provided format.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-085", "type": "Data Wrangling", "instruction": "Generate a comprehensive report on each cricket player's performance, including batting average, highest score, number of thirties, fifties, bowling wickets, and economy rate. Ensure these metrics are matched with their bowling skills and format the data strictly according to the provided format in \"result.csv\". Save the results in this file. ", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-086", "type": "Data Wrangling", "instruction": "Identify how many rented movies were returned late, early, and on time. Ensure the results are formatted strictly according to the provided format in \"result.csv\" and save them in this file. ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-087", "type": "Data Wrangling", "instruction": "Create a report listing cities with the number of active and inactive customers, with active customers defined as those where customer.active = 1. Ensure the data is sorted by the number of inactive customers in descending order and enter the results into \"result.csv\" according to the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-088", "type": "Data Wrangling", "instruction": "Determine the top 3 actors who have appeared most often in films from the \"Children\" category. Include all actors with the same number of appearances if there are ties. Enter the results into \"result.csv\" as per the given format.", "hardness": "Hard", "post_process": []}
{"id": "data-wrangling-089", "type": "Data Wrangling", "instruction": "Calculate each player's career span by determining the difference between their first and last game dates in the \"YYYY-MM-DD\" format. Save the results strictly according to the provided format in \"result.csv\"", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-090", "type": "Data Wrangling", "instruction": "Evaluate the performance of bowlers in the last overs (16 to 20) by calculating the total runs conceded total overs bowled, total wickets taken and the wicket rate for each over. Sort the results primarily by economy rate in ascending order and secondarily by wicket rate in descending order. Select only those bowlers who have bowled at least 50 overs. Ensure the results are formatted according to \"death_overs_specialist.csv\" and save them accordingly.\n", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-091", "type": "Data Wrangling", "instruction": "Query the number of different tracks each driver raced on in all 2009 races, and sort the results by driver name in ascending order. Fill the results in \"driver_circuits.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-092", "type": "Data Wrangling", "instruction": "Select the top 5 highest-rated and bottom 5 lowest-rated movies. Retrieve their names, ratings, and global box office revenue, then label them as \"Highest Rated\" or \"Lowest Rated.\" Sort the results by global box office revenue in descending order and save them in \"result.csv\" according to the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-093", "type": "Data Wrangling", "instruction": "Create a summary showing the total number of international students and their average depression scores. Ensure to fill in the results strictly according to the given format in the provided file, \"result.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-094", "type": "Data Wrangling", "instruction": "Prepare a summary showing the ten busiest start times for charging sessions, broken down by day and hour, specifically for shared users. Include the total session count for each period. Populate the data strictly according to the given format in the provided file, \"result.csv\".", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-095", "type": "Data Wrangling", "instruction": "Traverse multiple Olympic-related database tables to calculate the total medal count rankings for each country in each Olympic Games. Merge the data into a complete table and save the results strictly according to the provided format in \"olympics.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-096", "type": "Data Wrangling", "instruction": "Generate a report listing the highest-impact assignments in each region. For each assignment, include the assignment names, regions, impact scores, and the total number of donations received, but only for assignments that have received donations. Ensure to fill in the results strictly according to the given format in the provided file, \"result.csv\". ", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-097", "type": "Data Wrangling", "instruction": "Aggregate the total runs given up by each bowler in the matches, covering both normal and extra runs. Link these totals with the bowler's personal information, such as name, country, and bowling technique. Ensure the results are sorted by the total runs conceded from highest to lowest and formatted according to \"runs_concede_in_match.csv\". Save the results accordingly.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-098", "type": "Data Wrangling", "instruction": "Using data from 2019 to 2021, determine which industries had the highest average valuations. Identify the new unicorn companies within these industries. Make sure to fill your findings into the provided file, \"result.csv\", using the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-099", "type": "Data Wrangling", "instruction": "Calculate the average rating for each genre and rank them accordingly. Populate the results into \"genres_avg_rating.csv\" following the specified format.", "hardness": "Easy", "post_process": []}
{"id": "data-wrangling-100", "type": "Data Wrangling", "instruction": "Generate a list of industry groups showing the number of companies and their total carbon footprints for the latest year. Sort the list by footprint from highest to lowest. Save the results strictly according to the given format in the provided file, \"result.csv\".", "hardness": "Medium", "post_process": []}
{"id": "data-wrangling-101", "type": "Data Wrangling", "instruction": "Retrieve the name and revenue of the highest-grossing movie directed by each director, and sort the results by revenue in descending order. Fill the data into \"movies_all.csv\" according to the provided format.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-001", "type": "ML Classification", "instruction": "Use the dataset to make sentiment predictions on test.csv. Write the predicted labels (\"Negative\", \"Positive\") into result.csv as per the provided template.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-002", "type": "ML Classification", "instruction": "Predict user response for test.csv using the Health Insurance Cross Sell Prediction dataset. Write the results into `submission.csv` following the format of `sample_submission.csv.`", "hardness": "Hard", "post_process": []}
{"id": "ml-binary-003", "type": "ML Classification", "instruction": "Predict the sentiment categories for the texts in twitter_validation.csv. Save your predictions in prediction.csv using the provided template.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-004", "type": "ML Classification", "instruction": "Predict whether passengers in test.csv were transported to an alternate dimension based on the Spaceship Titanic dataset. Save your predictions in submission.csv using the template from sample_submission.csv.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-005", "type": "ML Classification", "instruction": "Predict the tumor diagnosis result (B or M) for the data in test.csv using the Breast Cancer Wisconsin (Diagnostic) Data Set. Save your predictions in label.csv with the column name 'result'.", "hardness": "Hard", "post_process": []}
{"id": "ml-binary-006", "type": "ML Classification", "instruction": "Predict the authenticity of the news in validation.csv using the dataset containing real and fake news. Save your predictions in result.csv with the column name 'result'.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-007", "type": "ML Classification", "instruction": "Predict whether customers will churn using the dataset in customer_churn_dataset-testing-master.csv. Save your predictions in result.csv with the column name 'result'.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-008", "type": "ML Classification", "instruction": "Predict the loan approval status (Y or N) for the data in test.csv using the loan approval dataset. Save your predictions in result.csv with the column name 'Loan_Status'.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-009", "type": "ML Classification", "instruction": "Predict passenger satisfaction in test.csv using the  dataset. Save the prediction results into result.csv with the column name 'satisfaction'.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-010", "type": "ML Classification", "instruction": "Predict the likelihood of a heart attack for individuals in test.csv using the heart attack dataset. Save your predictions in result.csv using the provided template.", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-011", "type": "ML Classification", "instruction": "Predict whether individuals in Test.csv have an income above the income limit using the income prediction dataset. Save your predictions in results.csv with columns \"ID\" (user id) and \"income_above_limit\" (your predicted value).\n", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-012", "type": "ML Classification", "instruction": "Predict the sentiment of the comments in test.csv. Save your prediction results into sentiment.csv, using the column name 'emotion' to represent the sentiment.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-013", "type": "ML Classification", "instruction": "Predict whether each turbine will fail based on the data provided in Test.csv. Save your predictions in target.csv with a single column labeled \"Target\", formatted according to the example in sample_target.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-014", "type": "ML Classification", "instruction": "Predict whether the users in test.csv will churn based on the customer churn prediction dataset. Save your predictions in churn.csv with the column name \"Churn\".", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-015", "type": "ML Classification", "instruction": "Predict the categories based on the Genetic Variant Classifications dataset using the information in test.csv. Save your predictions in class.csv with the column name \"CLASS\".", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-016", "type": "ML Classification", "instruction": "Predict whether credit card clients in test.csv will default based on the Credit Card Default dataset. Save your predictions in defaulter.csv with the column name \"IsDefaulter\".", "hardness": "Hard", "post_process": []}
{"id": "ml-binary-017", "type": "ML Classification", "instruction": "Predict whether it will rain the next day for each row in test.csv using the Rain in Australia dataset. Save your predictions in tomorrow.csv with the column name \"RainTomorrow\".", "hardness": "Hard", "post_process": []}
{"id": "ml-binary-018", "type": "ML Classification", "instruction": "Predict whether individuals in test.csv will have a stroke based on the Stroke Prediction dataset. Save your predictions in stroke.csv with the column name \"stroke\".", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-019", "type": "ML Classification", "instruction": "Predict whether mushrooms in test.csv are poisonous based on the Mushroom dataset. Save your predictions in class.csv with the column name \"class\".", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-020", "type": "ML Classification", "instruction": "Predict whether individuals in test.csv have heart disease based on the Cardiovascular Diseases Risk Prediction dataset. Save your predictions in disease.csv with the column name \"Heart_Disease\".", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-021", "type": "ML Classification", "instruction": "Predict whether individuals in test.csv have diabetes based on the Diabetes Prediction dataset. Save your predictions in diabetes.csv with the column name \"diabetes\".", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-022", "type": "ML Classification", "instruction": "Predict whether the company in test.csv will go bankrupt based on the Company Bankruptcy Prediction dataset. Save your predictions in bankrupt.csv with the column name \"Bankrupt?\".", "hardness": "Easy", "post_process": []}
{"id": "ml-binary-023", "type": "ML Classification", "instruction": "You are tasked with designing a method to predict the Response variable using the Health Insurance Cross Sell dataset. Predict the responses for entries in test.csv and save the results in submission.csv, ensuring they follow the format provided in sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-binary-024", "type": "ML Classification", "instruction": "You are tasked with designing a method to predict reservation cancellations using the Hotel Reservations dataset. Predict whether users will cancel their reservations based on test.csv data and save the results in result.csv, ensuring they follow the format in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-001", "type": "ML Classification", "instruction": "As a contestant in the Bank customer data churn prediction competition, you need to predict the data in test.csv according to the competition requirements. Write the results into submission.csv following the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-002", "type": "ML Classification", "instruction": "As a contestant in the health indicators and smoking status prediction competition, you need to design a method to predict the data in test.csv, incorporating the additional dataset provided, according to the competition requirements. Write the results into submission.csv following the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-003", "type": "ML Classification", "instruction": "As a contestant in the health features and medical condition prediction competition, you are tasked with predicting the data in test.csv using the provided datasets according to competition requirements. Please ensure your predictions are formatted and saved in submission.csv as specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-004", "type": "ML Classification", "instruction": "Predict defects in steel plates using the dataset provided. Your task is to design a method to predict test.csv based on competition guidelines. Save results in submission.csv as per sample_submission.csv format.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-005", "type": "ML Classification", "instruction": "Develop a prediction model for cirrhosis outcomes using the patient data dataset. Your task is to predict test.csv according to competition rules. Ensure your predictions are saved in submission.csv format, matching the structure of sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-006", "type": "ML Classification", "instruction": "Design a method to predict wine quality using the provided dataset. Your task is to generate predictions for test.csv based on competition guidelines. Save your results in submission.csv following the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-016", "type": "ML Classification", "instruction": "Predict machine failures in the dataset for Binary Classification competition. Your task is to develop a method to predict test.csv as per competition guidelines. Save your predictions in submission.csv in the format specified by sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-018", "type": "ML Classification", "instruction": "Your task is to develop a method to detect AI-generated text using the dataset provided. Generate predictions for test_essays.csv based on competition rules. Format your results in submission.csv following the structure of sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-019", "type": "ML Classification", "instruction": "Predict obesity risk using the provided dataset. Develop a method to predict test.csv according to competition guidelines. Save your predictions in submission.csv following the format specified in sample_submission.csv.", "hardness": "Easy", "post_process": []}
{"id": "ml-competition-020", "type": "ML Classification", "instruction": "Predict health outcomes of horses using the provided dataset. Develop a method to predict test.csv according to competition guidelines. Save your predictions in submission.csv following the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-multi-001", "type": "ML Classification", "instruction": "Classify human activities using the given dataset. Predict the activity categories for activity_test.csv. Write your predictions in activity.csv with the column name \"Activity\".", "hardness": "Easy", "post_process": []}
{"id": "ml-multi-002", "type": "ML Classification", "instruction": "Predict the status and type of cars using the provided dataset. Use test.csv to generate your predictions. Save your results in result.csv with the column name \"new&used\".", "hardness": "Easy", "post_process": []}
{"id": "ml-multi-003", "type": "ML Classification", "instruction": "Predict the experience level of job positions using the provided dataset. Use test.csv to generate your predictions. Save your results in result.csv with the column name \"formatted_experience_level\".", "hardness": "Hard", "post_process": []}
{"id": "ml-multi-004", "type": "ML Classification", "instruction": "Predict the adaptability level of students in online education using the provided dataset. Use test.csv to generate your predictions. Save your results in level.csv with the column name \"Adaptivity Level\".", "hardness": "Easy", "post_process": []}
{"id": "ml-multi-005", "type": "ML Classification", "instruction": "Classify emotions based on the provided NLP dataset. Predict the emotions for text data in test.txt. Write your predictions in emotions.csv with the column name \"emotion\".", "hardness": "Medium", "post_process": []}
{"id": "ml-multi-006", "type": "ML Classification", "instruction": "Classify credit scores based on the provided dataset. Predict the credit scores for data in test.csv. Write your predictions inscore.csv with the column name \"Credit_Score\".", "hardness": "Medium", "post_process": []}
{"id": "ml-multi-007", "type": "ML Classification", "instruction": "Predict fruit categories using the provided dataset. Use test.csv to generate your predictions. Save your results in class.csv with the column name \"Class\".", "hardness": "Easy", "post_process": []}
{"id": "ml-multi-008", "type": "ML Classification", "instruction": "Predict the categories based on the text information in test.csv for the News Aggregator Dataset competition. Save the predicted results into a file named category.csv, with the column name \"CATEGORY\".", "hardness": "Hard", "post_process": []}
{"id": "ml-multi-009", "type": "ML Classification", "instruction": "You are tasked with designing a method to predict body performance data using the Body Performance Data dataset. Predict the values for entries in test.csv and save the results in result.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-multi-010", "type": "ML Classification", "instruction": "Develop a model to predict body performance using the provided dataset. Generate predictions for test.csv. Save your results in class.csv with the column name \"class\".", "hardness": "Medium", "post_process": []}
{"id": "ml-multi-011", "type": "ML Classification", "instruction": "Classify the sentiment of tweets based on the provided dataset. Predict the sentiment for text data in Corona_NLP_test.csv. Write your predictions in sentiment.csv with the column name \"Sentiment\".", "hardness": "Hard", "post_process": []}
{"id": "ml-multi-012", "type": "ML Classification", "instruction": "Predict the damage status of buildings using the earthquake-affected dataset. Use incomplete.csv to generate your predictions. Save the results in prediction.csv with the columns \"building_id\" and \"damage_grade\".", "hardness": "Medium", "post_process": []}
{"id": "ml-cluster-001", "type": "ML Clustering", "instruction": "Perform clustering analysis on annual income and spending scores using the provided dataset. Save the clustering results in cluster.csv with columns \"Feature_i\" (the ith value of the processed feature vector) and \"Cluster\" (the clustering label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-002", "type": "ML Clustering", "instruction": "Perform clustering on the wine chemical components dataset stored in wine-clustering.csv. Save the clustering results into result.csv, with columns 'Feature_i' (representing the ith value of the processed feature vector) and 'Cluster' (displaying the clustering labels).", "hardness": "Medium", "post_process": []}
{"id": "ml-cluster-003", "type": "ML Clustering", "instruction": "Cluster the New York car data based on the provided dataset. Write the clustering results in cluster.csv with columns \"Feature_i\" (the ith value in the feature vector) and \"Cluster\" (the cluster label).\n\n", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-004", "type": "ML Clustering", "instruction": "Cluster Facebook interactions based on the provided dataset. Write the clustering results in cluster.csv with columns \"Feature_i\" (the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-005", "type": "ML Clustering", "instruction": "Perform clustering on the dataset blob_dataset.csv. Save the results in cluster_blob.csv with columns \"Feature_i\" (the ith value in the feature vector) and \"Cluster\" (the cluster label).\n", "hardness": "Medium", "post_process": []}
{"id": "ml-cluster-006", "type": "ML Clustering", "instruction": "Perform clustering on the Patient Dataset. Write the clustering results into cluster.csv. Ensure the column names are \"Feature_i\" (where i signifies the ith value in the feature vector) and \"Cluster\" (indicating the cluster label).", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-007", "type": "ML Clustering", "instruction": "Perform clustering on the dataset. Write the clustering results into cluster.csv with columns \"Feature_i\" (the ith value in the feature vector) and \"Cluster\" (the label for each cluster).", "hardness": "Medium", "post_process": []}
{"id": "ml-cluster-008", "type": "ML Clustering", "instruction": "Cluster user activity records from the real estate system dataset (property.csv) into 4 clusters. Save the clustering results in cluster.csv with columns \"Feature_i\" (i denotes the ith value in the feature vector) and \"Cluster\" (representing the cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-009", "type": "ML Clustering", "instruction": "Perform a clustering task on the global countries dataset, dividing the data into 3 clusters. Save the results in cluster.csv with the column names \"Feature_i\" (where i represents the ith value in the feature vector) and \"Cluster\" (the cluster label).", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-010", "type": "ML Clustering", "instruction": "Perform a clustering task on the Forest Cover Type Dataset, dividing the data into an appropriate number of clusters. Save the clustering results in cluster.csv with the columns named \"Feature_i\" (where i indicates the ith value of the feature vector) and \"Cluster\" (the label of the cluster).\n", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-011", "type": "ML Clustering", "instruction": "Perform clustering on the Online Retail dataset to identify distinct groups. Output the clustering results into cluster.csv with columns \"Feature_i\" (the ith value in the feature vector) and \"Cluster\" (representing the assigned cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-012", "type": "ML Clustering", "instruction": "Cluster the Customer Segmentation dataset to find distinct groups. Save the clustering results in cluster.csv with columns \"Feature_i\" (i indicating the ith value in the feature vector) and \"Cluster\" (showing the cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-013", "type": "ML Clustering", "instruction": "Perform a clustering task on the dataset of socioeconomic and health factors for various countries, dividing the data into an appropriate number of clusters. Save the clustering results in a file named cluster.csv with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster).", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-014", "type": "ML Clustering", "instruction": "Cluster the dataset of customers' personal and purchase data to discover distinct groups. Save the clustering outcomes in cluster.csv with columns \"Feature_i\" (where i denotes the ith value in the feature vector) and \"Cluster\" (representing the cluster label obtained from clustering).", "hardness": "Medium", "post_process": []}
{"id": "ml-cluster-015", "type": "ML Clustering", "instruction": "Analyze the Breast Cancer Proteomes dataset to perform a clustering task. Divide the data into suitable clusters and save the results in cluster.csv with columns \"Feature_i\" (where i denotes the ith value in the feature vector) and \"Cluster\"(indicating the assigned cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-016", "type": "ML Clustering", "instruction": "Perform a clustering task on the Online Retail II UCI dataset to segment the customers into an appropriate number of clusters. Save the clustering results in a file named cluster.csv with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster).", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-017", "type": "ML Clustering", "instruction": "Segment the bank customers in the provided dataset using clustering. Write the results to cluster.csv, including columns \"Feature_i\" (where i is the ith value in the feature vector) and \"Cluster\" (the resulting cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-018", "type": "ML Clustering", "instruction": "Use the Online Retail dataset to perform clustering and divide customers into appropriate clusters. Save the results in cluster.csv with columns named \"Feature_i\" (i indicates the ith value of the feature vector) and \"Cluster\" (the cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-019", "type": "ML Clustering", "instruction": "Cluster different types of customer groups from the Online Retail II Data Set dataset (using data from 2009-2010) into an appropriate number of clusters. Save the clustering results in a file named cluster.csv with the column names \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the label of the cluster).", "hardness": "Hard", "post_process": []}
{"id": "ml-cluster-020", "type": "ML Clustering", "instruction": "Perform clustering on the Iris Flower Dataset. Write the results to cluster.csv with columns \"Feature_i\" (where i represents the ith value of the feature vector) and \"Cluster\" (the cluster label).", "hardness": "Easy", "post_process": []}
{"id": "ml-cluster-021", "type": "ML Clustering", "instruction": "You are tasked with designing a clustering method using the Market Segmentation dataset. Cluster the users and write the results into cluster.csv, with the column for cluster labels named 'Cluster' and the feature vector columns named 'Feature_i'.", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-001", "type": "ML Regression", "instruction": "Based on the flight booking information dataset, predict the prices for the flights in test.csv. Save the predictions in result.csv with the column name 'price'.", "hardness": "Easy", "post_process": []}
{"id": "ml-regression-002", "type": "ML Regression", "instruction": "Predict electricity prices in test.csv based on your dataset on Spanish electricity and weather. Save your prediction results in result.csv, using the column name \"price actual\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-003", "type": "ML Regression", "instruction": "Predict the weekly sales using the Walmart sales dataset. Save the prediction results in submission.csv with the column name \"Weekly_Sales\".", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-004", "type": "ML Regression", "instruction": "Predict the popularity of the songs in test.csv using the pop music dataset. Save the prediction results in popularity.csv with the column name \"Popularity\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-005", "type": "ML Regression", "instruction": "Using the income prediction dataset, predict the incomes of individuals in test.csv. Save the results in price.csv with the column name \"price\".", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-006", "type": "ML Regression", "instruction": "Using the Lego sets and price dataset, predict the star ratings for the entries in test.csv. Save the predictions to result.csv with the column name \"Star rating\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-007", "type": "ML Regression", "instruction": "Based on the Eurostat dataset, predict the average job satisfaction for each entry in test.csv. Write the results to job_satisfaction.csv with the column name \"AVG_JOB_SATISFACTION\".", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-008", "type": "ML Regression", "instruction": "Predict the product sales based on the shoe product information in the test.csv. Write the prediction results into quantity.csv with the column name \"quantity_sold\" using the data from test.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-009", "type": "ML Regression", "instruction": "Based on the Airbnb listing dataset for Barcelona, predict the prices of the entries in test.csv. Write the predictions to price.csv with the column name \"price\".", "hardness": "Easy", "post_process": []}
{"id": "ml-regression-010", "type": "ML Regression", "instruction": "Utilize the comprehensive drug ratings dataset to predict the usefulness of drugs listed in drugsComTest_raw.csv. Write the predictions into \"Usefulness.csv\" with the column name \"usefulness\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-011", "type": "ML Regression", "instruction": "Utilize the biogas dataset to forecast the Biogas Generation Estimate (cu-ft/day) for entries in test.csv. Save the predictions as 'biogas_generation_estimate_cuftday' in result.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-012", "type": "ML Regression", "instruction": "Based on the dataset of vehicle information in New York, predict the Mileage for vehicles in test.csv. Write the results into result.csv under the column name \"Mileage\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-013", "type": "ML Regression", "instruction": "Predict the prices of vehicles in test.csv using the provided Vehicle dataset. Save the predictions in price.csv with the column name \"Selling_Price\".", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-014", "type": "ML Regression", "instruction": "Predict the car prices in test.csv using the Car Prices Poland dataset. Save the predicted prices in price.csv with the column name \"price\".\n", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-015", "type": "ML Regression", "instruction": "Predict the appliance energy consumption using the Appliances Energy Prediction dataset based on test.csv. Save the predictions in appliance.csv with the column name \"Appliances\".", "hardness": "Hard", "post_process": []}
{"id": "ml-regression-016", "type": "ML Regression", "instruction": "Using the Airfare ML dataset, forecast the flight fares for the entries in test.csv. Save your predictions in fare.csv under the column name \"Fare\".", "hardness": "Easy", "post_process": []}
{"id": "ml-regression-017", "type": "ML Regression", "instruction": "Based on the Steam Releases dataset, determine the ratings for the games in test.csv. Record your predictions in rating.csv, with the column named \"rating\".", "hardness": "Easy", "post_process": []}
{"id": "ml-regression-018", "type": "ML Regression", "instruction": "Predict food orders using the Food Demand Forecasting dataset based on the information in test.csv. Write the predictions to submission.csv following the format of sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-019", "type": "ML Regression", "instruction": "Based on the sensor data in test.csv from the Electric Motor Temperature dataset, determine the rotor temperature. Record your predictions in pm.csv with the column name \"pm\".", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-020", "type": "ML Regression", "instruction": "Predict the required delivery time using the Food Delivery Dataset based on the information in test.csv. Write the results into submission.csv following the format of sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-021", "type": "ML Regression", "instruction": "Using the Used Cars Price Prediction dataset, predict the prices of vehicles based on the information in test-data.csv. Write the predictions into price.csv with the column name \"price.\"", "hardness": "Easy", "post_process": []}
{"id": "ml-regression-022", "type": "ML Regression", "instruction": "You are tasked with designing a method to predict vehicle sales using the Vehicle Sales Data dataset. Predict the sales for entries in test.csv and fill  the results into result.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-regression-023", "type": "ML Regression", "instruction": "Design a method to predict the median house value using the California Housing Prices dataset. Predict the values for test.csv and fill the results in result.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-007", "type": "ML Regression", "instruction": "Perform house price predictions and record the results in submission.csv. Follow the format specified in sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-008", "type": "ML Regression", "instruction": "Participate in a flood prediction competition using the provided dataset. Your task is to develop a predictive model to forecast the likelihood of floods in test.csv. Ensure your predictions are saved in submission.csv, following the format outlined in the sample_submission.csv template.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-009", "type": "ML Regression", "instruction": "Design a solution for the abalone dataset competition described in README.md. Your task is to develop a predictive model and submit the prediction results into submission.csv, adhering to the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-010", "type": "ML Regression", "instruction": "Participate in the crab age prediction competition and leverage the additional dataset of crab ages to improve your method. Develop and refine your approach to achieve higher accuracy in predicting crab ages. Ensure your prediction results are saved in submission.csv, formatted according to 'sample_submission.csv'.\n", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-011", "type": "ML Regression", "instruction": "Participate in the wild blueberry yield prediction competition and leverage the additional dataset to improve your prediction method. Design and optimize your approach to achieve accurate yield predictions. Ensure your prediction results are saved in submission.csv, formatted according to \"sample_submission.csv\".", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-012", "type": "ML Regression", "instruction": "Design your method to predict CO2 emissions in the competition described in README.md. As a contestant, optimize your approach to accurately predict the data in test.csv. Submit your prediction results into submission.csv, following the format specified in sample_submission.csv.", "hardness": "Easy", "post_process": []}
{"id": "ml-competition-013", "type": "ML Regression", "instruction": "Participate in the sales volume prediction competition and formulate a strategy to predict the data provided in test.csv. Your goal as a contestant is to optimize your prediction method and output the results into submission.csv, adhering to the format outlined in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "ml-competition-014", "type": "ML Regression", "instruction": "Join the competition focusing on U.S. county-level monthly microbusiness data. Your task as a participant is to create a predictive model for the data in test.csv. Optimize your approach to achieve accurate predictions and format the results into submission.csv following the structure outlined in sample_submission.csv.", "hardness": "Medium", "post_process": []}
{"id": "ml-competition-015", "type": "ML Regression", "instruction": "Participate in the Mineral properties data for hardness prediction competition. Design and optimize a method to predict the data from test.csv and submit your results in submission.csv. Follow the specified format outlined in sample_submission.csv to ensure your predictions meet the competition's requirements.", "hardness": "Easy", "post_process": []}
{"id": "ml-competition-017", "type": "ML Regression", "instruction": "Generate predictions for the data in test.csv as part of the Automated Essay Scoring competition. Save your results in a submission file named submission.csv, following the format specified in sample_submission.csv.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-001", "type": "Statistical Analysis", "instruction": "Calculate the p-value and determine whether to reject or fail to reject the null hypothesis that the mean number of goals in women's and men's international soccer matches is the same. Use a 10% significance level. Save the result to 'result.csv'.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-002", "type": "Statistical Analysis", "instruction": "Follow the guidelines in tips.md to conduct a hypothesis test on the Business Case dataset, assessing if the weekday influences the number of electric bicycle rentals. Save the results in a file named ab_test_results.csv according to the provided format.\n", "hardness": "Easy", "post_process": []}
{"id": "data-sa-003", "type": "Statistical Analysis", "instruction": "Follow the guidelines in tips.md to conduct a hypothesis test on the Business Case dataset, assessing whether the number of rental bikes differs across seasons. Record the results in a CSV file named kruskal_wallis_results.csv according to the specified format.\n", "hardness": "Medium", "post_process": []}
{"id": "data-sa-004", "type": "Statistical Analysis", "instruction": "Could you analyze our Business Case dataset to perform a hypothesis test comparing bike rentals across different weather conditions? I need the results formatted and saved in a file named 'weather.csv'  following the requirement in tips.md.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-005", "type": "Statistical Analysis", "instruction": "Conduct a chi-square test on the student data to evaluate the association between gender and religion. Present the statistical results in the specified format:\n\n\"\"\"\n{\n    \"Chi-square value\": [...],\n    \"P-value\": [...],\n    \"Degrees of freedom\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "data-sa-006", "type": "Statistical Analysis", "instruction": "In this task, you will use the Quantitative Risk Management dataset to calculate and save covariance matrices for three distinct time periods: \"before,\" \"during,\" and \"after.\" Based on the `epochs_description.txt` file, perform the calculations and ensure that the results are saved according to the format in `template.csv`. Each file should be named `{epoch}_covariance.csv` to match the corresponding time period. Confirm that the matrices are calculated accurately and saved in the correct format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-007", "type": "Statistical Analysis", "instruction": "Compute two covariance matrices from the Quantitative Risk Management dataset: the sample covariance matrix from daily percentage price changes (annualized by 252), and the efficient covariance matrix using the Ledoit-Wolf method. Save them as `sample_covariance_matrix.csv` and `efficient_covariance_matrix.csv`, following the format of `template.csv`.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-008", "type": "Statistical Analysis", "instruction": "In this task, you will calculate several key risk metrics for the portfolio losses using the Quantitative Risk Management dataset provided. Specifically, you need to determine the mean, standard deviation, 95% Value at Risk (VaR), and Conditional Value at Risk (CVaR) for the worst 5% of cases. Ensure that daily portfolio losses are equally weighted. Please fill in the results in `result.csv`, following the format provided.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-009", "type": "Statistical Analysis", "instruction": "You are tasked with determining the 99% Conditional Value at Risk (CVaR) for our portfolio using historical data from the years 2008 and 2009. Use a Student's t-distribution to perform this analysis. Ensure that your findings are recorded in `result.csv`.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-010", "type": "Statistical Analysis", "instruction": "Determine the optimal portfolio weights to minimize Conditional Value at Risk (CVaR) at a 95% confidence level using historical stock price data. Save the optimal weights in `result.csv` according to the specified format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-011", "type": "Statistical Analysis", "instruction": "For this task, analyze IBM stock using the Black-Scholes model. Calculate the option prices based on the annualized volatility, and also compute the prices using double the historical volatility. Please enter the results into `result.csv`.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-012", "type": "Statistical Analysis", "instruction": "Please apply a delta hedging approach to our IBM portfolio using European put options. Make sure to compile your results in `result.csv` with the prescribed output format for review", "hardness": "Hard", "post_process": []}
{"id": "data-sa-013", "type": "Statistical Analysis", "instruction": "For this task, you need to estimate the 95% Value at Risk (VaR) for a stock portfolio across two periods: 2005-2006 and 2007-2009. Start by calculating daily returns, then compute the portfolio returns using equal weights, and find the 95th percentile of the losses. Enter the results into `result.csv` in the required format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-014", "type": "Statistical Analysis", "instruction": "Analyze whether there is a significant shift in the relationship between mortgage delinquencies and portfolio returns before and after the 2008 financial crisis using a Chow test. First, split the data at June 30, 2008, and perform OLS regressions for both sub-periods. Then, calculate the Chow test statistic to assess the structural break. Save the results, including the test statistic and p-value (if applicable), to result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-015", "type": "Statistical Analysis", "instruction": "Calculate and compare the 30-day rolling volatility for an equally-weighted portfolio of investment banks, both with and without Citibank. Compute daily returns, then calculate the volatilities, and save the results to result.csv using the provided template.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-016", "type": "Statistical Analysis", "instruction": "Estimate the Conditional Value at Risk (CVaR) at the 99% confidence level for GE stock. Calculate daily losses from historical data, determine the maximum weekly losses, fit a Generalized Extreme Value (GEV) distribution to these maxima, and compute the reserve needed to cover expected maximum weekly losses for a €1,000,000 GE stock holding in January 2010. Save the results in `result.csv` following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-017", "type": "Statistical Analysis", "instruction": "Compute the Pearson correlation coefficient between female illiteracy and fertility based on the tips provided in tips.md. After calculating the correlation coefficient, fill in the result in the provided result.csv template.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-018", "type": "Statistical Analysis", "instruction": "Run a linear regression using the Fama-French three-factor model to analyze the returns of a portfolio adjusted for risk (Portfolio_Excess) based on the instructions provided in tips.md. After running the regression, print the regression's adjusted R-squared value and write the results in the format specified in result.csv.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-019", "type": "Statistical Analysis", "instruction": "Calculate the skewness and fourth moment of daily stock returns following the instructions in tips.md. After computing these metrics, input the results into the result.csv template as specified.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-020", "type": "Statistical Analysis", "instruction": "Use a z-test to calculate the p-value for the hypothesis \"the conversion rate of the experimental group is greater than the conversion rate of the control group,\" and then fill in the results in the format specified in the result.csv file.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-021", "type": "Statistical Analysis", "instruction": "Perform an appropriate hypothesis test to determine the p-value and decide whether to reject or fail to reject the null hypothesis that the mean number of goals scored in women's international soccer matches is the same as men's, using a 10% significance level. Finally, fill in the results in the format specified in the result.csv file.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-022", "type": "Statistical Analysis", "instruction": "Perform a Mann-Whitney U test to compare the number of goals scored in FIFA Women's and Men's World Cup matches from 2002 onwards. The test will determine if the average number of goals in women's matches is significantly higher than in men's. If the p-value is less than 0.01, reject the null hypothesis (\"reject\"); otherwise, fail to reject it (\"fail to reject\"). Save the results and conclusion in result.csv following the given format.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-023", "type": "Statistical Analysis", "instruction": "Please use a Two-Sample Proportions Z-Test to compare the adverse effects between the Drug and Placebo groups, utilizing the `proportions_ztest` function. Ensure that the results are documented in `z_test_results.csv` as per the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-024", "type": "Statistical Analysis", "instruction": "Conduct a two-sided Mann-Whitney U test to assess whether there is a significant difference in the age distribution between the Drug and Placebo groups. Extract the p-value and store the result in a file named age_group_effects_p_value.csv", "hardness": "Medium", "post_process": []}
{"id": "data-sa-025", "type": "Statistical Analysis", "instruction": "Using the Interquartile Range (IQR) method, detect outliers in the 'Price' and 'No. of People rated' columns of the dataset. For each of these features, calculate the number of outliers, as well as the upper and lower bounds used to identify them. Record the results in a file named result.csv, following the format provided in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-026", "type": "Statistical Analysis", "instruction": "Calculate the Pearson correlation matrix for the columns 'Overall Rating', 'Seat Comfort', 'Staff Service', and 'Food & Beverages' in the Airline Reviews Dataset. Before computing, convert these columns to numeric data types and remove any rows with missing values in these columns. Save the resulting correlation matrix to a file named result.csv, ensuring it follows the same format as the provided sample_result.csv", "hardness": "Medium", "post_process": []}
{"id": "data-sa-027", "type": "Statistical Analysis", "instruction": "Calculate the correlation between other variables (excluding ‘id’) and the ‘outcome’ column. Find the variable with the highest correlation and fill in the corresponding column name into result.csv according to the format provided in result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-028", "type": "Statistical Analysis", "instruction": "It is observed that the beaks of G. scandens birds on Daphne Major appear to have become deeper. Assuming that the means of two groups of data are the same, you need to calculate the probability of observing the difference in means as per the instructions in tips.md. Under the null hypothesis that the means of the two groups are equal, calculate the probability (p-value) of observing the actual difference in means or a more extreme value.Write the calculated results in the format specified in sample_result.csv into the result.csv file.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-029", "type": "Statistical Analysis", "instruction": "Using the beak length and depth data of G. scandens birds from Daphne Major in 1975 and 2012, compute the beak length-to-depth ratios for each bird in both years. Calculate the mean ratio for each year and compute the 99% confidence intervals using bootstrap replicates (size = 10,000). Finally, write the results into result.csv, following the format specified in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-030", "type": "Statistical Analysis", "instruction": "Please estimate the 99% Conditional Value at Risk (CVaR) for a portfolio using two methods: T distribution and Gaussian Kernel Density Estimation (KDE). Ensure that the results are entered into `result.csv` as per the given format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-031", "type": "Statistical Analysis", "instruction": "Please perform a Bootstrap analysis to determine the 95% confidence interval for the reduction in deaths due to handwashing. Enter your findings into `result.csv`, following the provided format.\n", "hardness": "Medium", "post_process": []}
{"id": "data-sa-032", "type": "Statistical Analysis", "instruction": "Please calculate the annualized Sharpe ratio for Amazon and Facebook from the dataset \"Risk and Returns: The Sharpe Ratio.\" This ratio measures the additional return per unit of risk, with the annual factor being the square root of 252. Enter your results in `result.csv` as specified.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-033", "type": "Statistical Analysis", "instruction": "Analyze the returns of FAANG stocks from the dataset. Calculate the expected annual return and annualized Sharpe ratio for an equally weighted portfolio of these stocks. Save the results in result.csv according to the provided format", "hardness": "Medium", "post_process": []}
{"id": "data-sa-034", "type": "Statistical Analysis", "instruction": "Identify the geographic region and fuel source with the highest median CO₂ emissions. Then, within each geographical region, perform ANOVA tests to determine if there are significant differences in CO₂ emissions among the different fuel sources. If the ANOVA results are significant, conduct pairwise t-tests between fuel sources within each region, applying the Bonferroni correction for multiple comparisons. Record your analysis results in the file named `anova_results.csv`, following the provided format.", "hardness": "Hard", "post_process": []}
{"id": "data-sa-035", "type": "Statistical Analysis", "instruction": "Analyze CO2 emissions from various fuel sources across different geographical regions. Compute the Bonferroni corrected P-values and compare fuel sources. Save the results in `bonferroni_corrected_p_values.csv` using the provided template.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-036", "type": "Statistical Analysis", "instruction": "You need to compute the heritability for G. scandens and G. fortis. Then, compute a 95% confidence interval using pairs bootstrap. Heritability is defined as the ratio of the covariance of the trait in parents and offspring divided by the variance of the trait in the parents. Acquire 1000 bootstrap replicates of the heritability using pairs bootstrap for G. scandens and G. fortis (set the random seed to 42). Please save the results into the result.csv file following the provided format.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-037", "type": "Statistical Analysis", "instruction": "Calculate 10,000 bootstrap replicates to determine the variance in annual rainfall at the Sheffield Weather Station. Please save your findings in 'result.csv'.(Set Random seed as 42)", "hardness": "Hard", "post_process": []}
{"id": "data-sa-038", "type": "Statistical Analysis", "instruction": "The average strike force of Frog B was 0.71 Newtons (N), and that of Frog D was 0.42 N, for a difference of 0.29 N. It is possible that the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. Please use a permutation test with a test statistic of the difference of means to test this hypothesis and print the p-value. (seed=42). Save the results in the format of sample_result.csv to a file named result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-039", "type": "Statistical Analysis", "instruction": "Determine if Frog D and Frog E have similar impact forces. Compute the corresponding p-value and save the result in the format of sample_result.csv to a file named result.csv. (Set the random seed to 42)", "hardness": "Easy", "post_process": []}
{"id": "data-sa-040", "type": "Statistical Analysis", "instruction": "Perform a bootstrap hypothesis test to determine if Frog B and Frog D have the same mean impact force, without assuming their distributions are identical. Calculate the p-value and save the result to `result.csv`, following the format provided in `sample_result.csv`.", "hardness": "Easy", "post_process": []}
{"id": "data-sa-041", "type": "Statistical Analysis", "instruction": "Perform a permutation test to evaluate the hypothesis that a House member's party affiliation has no effect on their vote. Use the fraction of Democrats voting in favor as the test statistic. Calculate the p-value for observing a fraction of Democrats voting in favor less than or equal to the observed fraction (153/244). Set the random seed to 42. Save the p-value to a file named result.csv, following the format of sample_result.csv", "hardness": "Easy", "post_process": []}
{"id": "data-sa-042", "type": "Statistical Analysis", "instruction": "Using the beak depth data (bd_1975 and bd_2012) for G. scandens from 1975 and 2012, generate 10,000 bootstrap replicates of the mean for each year (set the random seed to 42). Calculate the difference in mean beak depth between the two years and compute a 95% confidence interval for this difference. Save the results to result.csv, following the format provided in sample_result.csv.", "hardness": "Medium", "post_process": []}
{"id": "data-sa-043", "type": "Statistical Analysis", "instruction": "Calculate the sum of squared residuals (SSR) from a regression analysis predicting quarterly minimum portfolio returns based on mortgage delinquencies using data from 2005 to 2010. Save the results in result.csv", "hardness": "Medium", "post_process": []}
{"id": "data-sa-044", "type": "Statistical Analysis", "instruction": "You are working on a project for a food industry analytics company. Your task is to analyze the data to determine the average ratings for restaurants offering pickup and delivery, along with the T-Statistic and P-Value. Report your findings with three significant digits and format your response as follows:\n\"\"\"\n{\n    \"Average rating for pickup restaurants\": [3.81],\n    \"Average rating for delivery restaurants\": [3.73],\n    \"T-Statistic\": [2.31],\n    \"P-Value\": [0.0207]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-045", "type": "Statistical Analysis", "instruction": "As a data scientist for a medical research institute, you need to examine the data to find the survival rate statistics, including the number of observations, mean, minimum, maximum, variance, skewness, and kurtosis. Your response should be formatted like this:\n\"\"\"\n{\n    \"Number of observations (Nobs)\": [5191],\n    \"Mean\": [...],\n    \"Minimum\": [...],\n    \"Maximum\": [...],\n    \"Variance\": [...],\n    \"Skewness\": [...],\n    \"Kurtosis\": [...]    \n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-046", "type": "Statistical Analysis", "instruction": "You are working on a project for an aviation industry analytics firm. Your task is to interpret the output of a two-sample t-test comparing the mean number of fatalities between aircraft manufactured by Boeing and Douglas. Provide the conclusion regarding the null hypothesis (\"reject\" or \"accept\") and the implications of the findings (\"statistically significant difference\" or \"no statistically significant difference\"). Format your response as follows:\n\"\"\"\n{\n    \"T-statistic\": [13.7262535],\n    \"P-value\": [3.196323892903482e-40],\n    \"Conclusion\": [\"reject\"/\"accept\"],\n    \"Implication\": [\"statistically significant difference\"/\"no statistically significant difference\"]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-047", "type": "Statistical Analysis", "instruction": "Calculate the Pearson correlation coefficient to evaluate the relationship between BMI and medical expenses to determine if there is a significant linear relationship. Provide your results in the following format:\n\n\"\"\"\n{\n    \"Pearson's correlation coefficient\": [...],\n    \"P-Value\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-048", "type": "Statistical Analysis", "instruction": "Perform a chi-square test to assess whether the app category significantly affects the rating distribution. Use a significance level of 0.05. Based on the p-value, select an appropriate conclusion between \"Fail to reject the null hypothesis: No significant effect of category on rating distribution.\" and \"Reject the null hypothesis: The category of an app affects its rating distribution.\" Provide your response in the following format:\n\n\"\"\"\n{\n    \"Chi2 Statistic\": [...],\n    \"P-value\": [...],\n    \"result\": [...]\n}\n'\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-049", "type": "Statistical Analysis", "instruction": "Conduct a t-test to compare the average ratings of free and paid apps to determine if there is a significant difference. Use a significance level of 0.05. Based on the p-value, choose the appropriate conclusion between \"Reject the null hypothesis: Free apps have a higher average rating than paid apps.\" and \"Fail to reject the null hypothesis: No significant difference in ratings between free and paid apps.\" Provide your answer in the following format:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-value\": [...],\n    \"result\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-050", "type": "Statistical Analysis", "instruction": "With a random seed of 42, randomly select 100 data points from the dataset and conduct a Z-test on Canada's CO2 emission data. Determine if the sample mean significantly differs from the population mean at a 0.05 significance level. Based on the Z-value, choose between \"Fail to reject null hypothesis\" and \"reject null hypothesis.\" Format your findings as follows:\n\"\"\"\n{\n    \"Z-value\": [...],\n    \"Sample Mean\": [...],\n    \"Sample Standard Deviation\": [...],\n    \"result\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-051", "type": "Statistical Analysis", "instruction": "Perform a t-test to assess whether there is a significant difference in average medical expenses between males and females. Apply a 0.05 significance level. Depending on the p-value, choose between \"Fail to reject null hypothesis\" and \"reject null hypothesis\" for the result. Format your answer as follows:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-Value\": [...],\n    \"result\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-052", "type": "Statistical Analysis", "instruction": "Perform a t-test to compare the average medical expenses between smokers and non-smokers to determine if smoking status has a significant impact on medical expenses. Use a significance level of 0.05. Based on the p-value, select either \"Fail to reject null hypothesis\" or \"reject null hypothesis\" as the result. Provide your results in the following format:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-Value\": [...],\n    \"result\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-053", "type": "Statistical Analysis", "instruction": "From the January 2018 dataset, select samples with a status of 'failed' and 'successful'. Apply a logarithmic transformation to the 'usd_goal_real' feature. Perform a Shapiro-Wilk normality test on the transformed data, calculating both the test statistic and p-value. Based on the p-value, conclude whether the data exhibits a \"Normal Distribution\" or a \"Non-Normal Distribution\". Present your findings as follows:\n\n\"\"\"\n{\n    \"Shapiro stat\": [...],\n    \"P-value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-054", "type": "Statistical Analysis", "instruction": "Use an independent two-sample t-test to determine if there is a significant difference between males and females in their reasons for eating comfort food. With a p-value threshold of 0.05, select the appropriate conclusion from \"There is a significant difference\" and \"There is no significant difference\". Provide your response in the following format:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-Value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "data-sa-055", "type": "Statistical Analysis", "instruction": "Analyze the data to calculate the mean and standard deviation of purchase amounts for two age groups: those over 65 years old and those under 25 years old. Perform an independent two-sample t-test to determine if there is a significant difference in purchase amounts between the two groups. Provide your response in the following format:\n\n\"\"\"\n{\n    \"Oldage Purchase Amount Mean\": [...],\n    \"Youngage Purchase Amount Mean\": [...],\n    \"Oldage Purchase Amount std\": [...],\n    \"Youngage Purchase Amount std\": [...],\n    \"T_Statistic\": [...],\n    \"P_Value\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-056", "type": "Statistical Analysis", "instruction": "Filter the data for each season by calculating the quartiles and outlier range, excluding any data outside this range. Next, perform an independent two-sample t-test to assess whether there is a significant difference in bike rental counts between weekdays and weekends. Use the resulting p-value to draw a conclusion, choosing from \"Accept the null hypothesis\" or \"Reject the null hypothesis\". Present your findings as follows:\n\n\"\"\"\n{\n    \"p_value\": [...],\n    \"conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-057", "type": "Statistical Analysis", "instruction": "For each season, filter the data by calculating the quartiles and outlier range, removing any data outside this range. Next, perform the Shapiro-Wilk test for normality on the filtered data for each season, and record the statistics and p-values. Ensure the results are saved in result.csv with the columns \"Season\", \"Stats\", and \"P-Value\".", "hardness": "Medium", "post_process": []}
{"id": "data-sa-058", "type": "Statistical Analysis", "instruction": "Calculate the quartiles and outlier range for each season's data, filtering out the outliers. Then, use the Levene test to compare the rental count variances across the four seasons. Based on the p-value obtained, decide between \"Variances are not equal\" and \"Variances are equal among different seasons\". Format your response like this:\n\n\"\"\"\n{\n    \"p-value\": [...],\n    \"conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-059", "type": "Statistical Analysis", "instruction": "Calculate the quartiles and outlier range for each season's data, filtering out the outliers. Then, use the Kruskal-Wallis test to determine if the number of bike rentals is significantly impacted by the season. Based on the p-value obtained, decide between \"Season does impact the count of cycles used\" and \"Season does not impact the count of cycles used\". Format your response like this:\n\n\"\"\"\n{\n    \"p-value\": [...],\n    \"conclusion\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "data-sa-060", "type": "Statistical Analysis", "instruction": "Filter the data for each season by calculating the quartiles and outlier range, excluding any data outside this range. Then, perform the Shapiro-Wilk normality test to calculate the normality of bike rental counts under different weather conditions (Clear, Misty, Rainy). Based on the p-values obtained, select the final conclusion from \"Data is not normally distributed\" and \"Data is normally distributed\". Provide your response in the following format:\n\n\"\"\"\n{\n    \"Clear weather P-value\": [...],\n    \"Misty weather P-value\": [...],\n    \"Rainy weather P-value\": [...],\n    \"conclusion\": [...]\n}\n\"\"\"", "hardness": "Hard", "post_process": []}
{"id": "data-sa-061", "type": "Statistical Analysis", "instruction": "First, filter the data for each season by calculating the quartiles and outlier range, removing any data outside this range. Then, apply the Levene test to assess whether the variances of bike rental counts differ under the three weather conditions (Clear, Misty, Rainy). Use the obtained p-values to draw a conclusion, choosing between \"Variances are not equal\" and \"Variances are equal\". Present your findings as follows:\n\n\"\"\"\n{\n    \"P-value\": [...],\n    \"conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-062", "type": "Statistical Analysis", "instruction": "Filter the data for each season by calculating the quartiles and outlier range, excluding any outliers. Next, use the Chi-square test to assess the correlation between season and weather conditions. Based on the p-values obtained, determine if there is an impact of season on weather. Format your response like this:\n\n\"\"\"\n{\n    \"P-value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-063", "type": "Statistical Analysis", "instruction": "Use one-way ANOVA to evaluate if there are significant differences in total rating counts (rating_count_tot) among different snapshot screen numbers (ipadSc_urls.num) at a 0.05 significance level. Based on the p-value, determine if \"There is a significant difference\" or \"There is no significant difference\". Format your response like this:\n\"\"\"\n{\n    \"One-way ANOVA P\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"\n", "hardness": "Medium", "post_process": []}
{"id": "data-sa-064", "type": "Statistical Analysis", "instruction": "Use the descriptions from tips.md to calculate the effect size for height and weight for males and females. Provide your response in the following format:\n\n\"\"\"\n{\n    \"height's effect size\": [...],\n    \"weight's effect size\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-065", "type": "Statistical Analysis", "instruction": "Perform a two-sample t-test to compare the net worth of self-made billionaires and inherited billionaires to determine if there is a significant difference. Provide your response in the following format:\n\n\"\"\"\n{\n    \"T-statistic\": [...],\n    \"P-value\": [...]\n}\n\"\"\"", "hardness": "Easy", "post_process": []}
{"id": "data-sa-066", "type": "Statistical Analysis", "instruction": "Calculate the mean, variance, and standard deviation for the win_by_wickets data according to the instructions in tips.md. Provide your response in the following format:\n\n\"\"\"\n{\n    \"Mean\": [...],\n    \"Variance\": [...],\n    \"Standard deviation\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-067", "type": "Statistical Analysis", "instruction": "Use a two-sample t-test to calculate the t-statistic and p-value for the technical abilities between left-footed and right-footed players using FIFA20 data. With a p-value threshold of 0.05, choose the appropriate conclusion from \"There is a significant difference\" and \"There is no significant difference\". Provide your response in the following format:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-Value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-068", "type": "Statistical Analysis", "instruction": "Perform a two-sample t-test to assess the shot power of left-footed and right-footed players using FIFA20 data. Calculate the t-statistic and p-value. With a threshold of 0.05, determine if \"There is a significant difference\" or \"There is no significant difference\". Format your response like this:\n\n\"\"\"\n{\n    \"T-Statistic\": [...],\n    \"P-Value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
{"id": "data-sa-069", "type": "Statistical Analysis", "instruction": "Compute the mean, standard deviation, sample size, and standard error for the natural log of total medals for Olympic countries when they are hosting and when they are not hosting. Calculate the standard error between the means by comparing the difference in means to the sum of standard errors. Present your findings as follows:\n\n\"\"\"\n{\n    \"Mean Natural Log of Total Medals when Not Hosting\": [...],\n    \"Mean Natural Log of Total Medals when Hosting\": [...],\n    \"Standard Error of Natural Log of Total Medals when Not Hosting\": [...],\n    \"Standard Error of Natural Log of Total Medals when Hosting\": [...],\n    \"Standard Errors between means\": [...]\n}\n\"\"\"", "hardness": "Hard", "post_process": []}
{"id": "data-sa-070", "type": "Statistical Analysis", "instruction": "Conduct a two-sample t-test to evaluate whether there is a significant difference in the natural log of total medals between countries that host the Olympics and those that do not. Based on a p-value threshold of 0.05, determine the conclusion: \"Same distributions (fail to reject H0)\" or \"Different distributions (reject H0)\". Present your findings as follows:\n\n\"\"\"\n{\n    \"T-Statistics\": [...],\n    \"P-Value\": [...],\n    \"Conclusion\": [...]\n}\n\"\"\"", "hardness": "Medium", "post_process": []}
