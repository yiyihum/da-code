[
    {
        "id": "dbt001",
        "source": "https://github.com/dbt-labs/jaffle_shop",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/dbt001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "This is a dbt project for modeling jaffle_shop data. The full data working flow is shown in the figure. I have encountered some bugs now, Encountered an error: Compilation Error Documentation for 'model.jaffle_shop.orders' depends on doc 'orders_status'  which was not found. Please help me improve this project and supplement these codes. The goal is that “dbt run” can be executed correctly.",
        "hardness": "level4",
        "actions": [
        ],
        "eval_type": "dbt_check",
        "eval_args": {
        },
        "output": "",
        "gold": "benchmark/source/dbt001/eval.sh",
        "evaluator": {
            "func": "check_include_exclude",
            "result": {
                "type": "docker_script_output",
                "url": "https://drive.usercontent.google.com/download?id=1U7npRKhoe1ms3Nkj7OVT6x-w9K9KzZtL&export=download&authuser=0&confirm=t",
                "dest": "/workspace/eval.sh"
            },
            "expected": {
                "type": "rule",
                "rules": {
                    "include": ["succeed"],
                    "exclude": ["failed"]
                }
            }
        }
    },
    {
        "id": "text2sql001",
        "source": "https://app.datacamp.com/learn/projects/worlds_oldest_businesses",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/text2sql001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "The data exists in CSV files. A postgres database is provided locally. Please use Python or SQL to analyze this data and answer the question: which are the most common categories for the oldest businesses on each continent? Show the continent, category and the number (greater than 5). The data exists in CSV file 'result.csv'",
        "hardness": "level1",
        "actions": [
        ],
        "eval_type": "csv_file_comparison",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/result/text2sql001/result.csv",
        "gold": "benchmark/source/text2sql001/result.csv",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1JAsxzgtQhkUUo5eRsAzSSVZTHGqyB96Q&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    },
    {
        "id": "text2sql002",
        "source": "https://app.datacamp.com/learn/projects/1531",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/text2sql002",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "Using a PostgreSQL database you'll be able to analyze information about unicorn companies. The data exists in these CSV files. A postgres database is provided locally. Please use Python or SQL to analyze these data and answer the question: Determine which industries have the highest average valuations from 2019 to 2021, and tell me what new unicorns are in these industries?', the columns of answer are 'industry', 'year', 'num_unicorns', 'average_valuation_billions'.",
        "hardness": "level1",
        "actions": [
        ],
        "eval_type": "csv_file_comparison",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/result/text2sql002/result.csv",
        "gold": "benchmark/source/text2sql002/result.csv",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1hSKQ3f8As5wrP6HRg4md3tFTxw3o_o_4&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    },
    {
        "id": "load001",
        "source": "https://github.com/avkaz/bike-rental-data-engineering-project",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/load001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "You need to create a relational database with analytics-ready views connecting Citi Bike and weather datasets. The designed database ER diagram is shown in the figure. Please write code to process raw data and load it into the local postgres database.",
        "hardness": "level5",
        "actions": [
        ],
        "eval_type": "check_postgres",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/mnt/load1/cache",
        "gold": "benchmark/result/load1/results",
        "evaluator": {
            "func": "compare_csv_files",
            "result": {
                "type": "postgre_files",
                "schema": "bike_rental",
                "dest": "/workspace/cache"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1cM5zB54cgrJXhRFV-SEEZZV_0BaIRlS4&export=download&authuser=0&confirm=t",
                "dest": "/workspace/results.zip"
            }
        }
    },
    {
        "id": "clean001",
        "source": "https://app.datacamp.com/learn/courses/cleaning-data-in-postgresql-databases",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/clean001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "NYC_Open_Data_Parking_Violations is stored in the local postgres database. Now, we need to use the data from it for data analysis. There is a local document that stores data standards. Please follow the data standards for data cleaning.",
        "hardness": "level3",
        "actions": [
        ],
        "eval_type": "check_postgres",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/mnt/clean001/cache",
        "gold": "benchmark/result/clean001/results",
        "evaluator": {
            "func": "compare_csv_files",
            "result": {
                "type": "postgre_files",
                "schema": "public",
                "dest": "/workspace/cache"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1j58gM0OlWrNoYHqGRZc8fSHM4GfPk3MF&export=download&authuser=0&confirm=t",
                "dest": "/workspace/results.zip"
            }
        }
    },
    {
        "id": "clean002",
        "source": "https://app.datacamp.com/learn/courses/cleaning-data-in-postgresql-databases",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/clean002",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "NYC_Open_Data_Parking_Violations is stored in the local postgres database. There have been a number of complaints indicating that some New York residents have been receiving multiple parking tickets for a single violation. This is resulting in the affected residents having to incur additional legal fees for a single incident. You have been tasked with identifying records that reflect this duplication of violations. A decision has been made to use the minimum fee to resolve the ambiguity created by these duplicates.",
        "hardness": "level3",
        "actions": [
        ],
        "eval_type": "check_postgres",
        "eval_args": {
        },
        "output": "benchmark/mnt/clean002/cache",
        "gold": "benchmark/result/clean002/results",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=14nstv4m6BmmAb_DTmNX28lqqMZHY0ziG&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    },
    {
        "id": "snowflake001",
        "source": "https://app.snowflake.com/marketplace/listing/GZSOZ1LLD8/weather-source-llc-global-weather-climate-data-for-bi",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/snowflake001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "When it snows in excess of six inches per day, my company experiences delivery delays. Please help to query the snowflake market data GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI and HISTORY_DAY view and anwers that: How many of my deliveries were impacted during the third week of January for 2021?",
        "hardness": "level2",
        "actions": [
        ],
        "eval_type": "csv_file_comparison",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/result/snowflake001/result.csv",
        "gold": "benchmark/source/snowflake001/result.csv",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1gRDw86AtDuoQuo0wvL6fCw4NMkk_Bf99&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    },
    {
        "id": "bigquery001",
        "source": "https://app.snowflake.com/marketplace/listing/GZSOZ1LLD8/weather-source-llc-global-weather-climate-data-for-bi",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/bigquery001",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "I'm in the process of choosing between living in Beijing or Shanghai, and air quality is a significant factor for me. I've heard about OpenAQ, a public dataset available on BigQuery that provides information about air quality. Could you provide the average AQI for PM2.5 of the aforementioned cities, along with detailed longitude and latitude positions, from 2017 to 2021? The output file should be named as 'data.csv'.",
        "hardness": "level2",
        "actions": [
        ],
        "eval_type": "csv_file_comparison",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/result/bigquery001/result.csv",
        "gold": "benchmark/source/bigquery001/result.csv",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=1eHJvczxFHjTifMOkGsGH2K50nJRqgV6Z&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    },
    {
        "id": "bigquery002",
        "source": "https://stackoverflow.com/questions/76833664/how-to-fetch-data-from-openaq-api-with-python-filter-by-country-city-and-spec",
        "env_name": "bash_new",
        "image_name": "bash-py-postgres",
        "data_path": "benchmark/data/bigquery002",
        "init_args": {
            "work_dir": "/workspace",
            "ports": {
                "5432": "12001"
            }
        },
        "task_input": "I am working on a project where I need to forecast PM2.5 concentrations in mining areas. To achieve this, I require air quality data with specific parameters from cities with gold mining activities and their environs. Parameters: PM2.5 PM10, Cities: United States: Sacramento, South Africa: Johannesburg, Australia: Bendigo From 2018 to 2022. I am seeking assistance in writing a Python script to fetch this air quality data from the Bigquery API. I intend to use this data for my forecasting model to analyze PM2.5 concentrations in relation to the given parameters in mining areas. Please help to continue the my python scripts and save this data with 5 columns on city, country, pollutant, value, timestamp as 'result.csv'.",
        "hardness": "level3",
        "actions": [
        ],
        "eval_type": "csv_file_comparison",
        "eval_args": {
            "ignore_case": false
        },
        "output": "benchmark/result/bigquery002/result.csv",
        "gold": "benchmark/source/bigquery002/result.csv",
        "evaluator": {
            "func": "compare_csv",
            "result": {
                "type": "docker_file",
                "path": "/workspace/result.csv"
            },
            "expected": {
                "type": "cloud_file",
                "path": "https://drive.usercontent.google.com/download?id=17YvsCsSjTQbSlMJ7ZIzv62eD_KLqlDgP&export=download&authuser=0&confirm=t",
                "dest": "/workspace/gold.csv"
            }
        }
    }
]

